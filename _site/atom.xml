<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Jingye Wang</title>
 <link href="/atom.xml" rel="self"/>
 <link href="/"/>
 <updated>2021-03-15T23:17:40+08:00</updated>
 <id></id>
 <author>
   <name>Jingye Wang</name>
   <email>wangjy5@shanghaitech.edu.cn</email>
 </author>

 
 <entry>
   <title>Unity3D - Astroxygen</title>
   <link href="/2021/03/14/Astroxygen/"/>
   <updated>2021-03-14T00:00:00+08:00</updated>
   <id>/2021/03/14/Astroxygen</id>
   <content type="html">&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;../../../../assets/images/astroxygen_bg.png&quot; alt=&quot;&quot; width=&quot;700&quot; /&gt;
&lt;/div&gt;

&lt;center&gt;
  &lt;p style=&quot;font-size:80%;&quot;&gt;
Figure 1. Astroxygen.
  &lt;/p&gt;
&lt;/center&gt;

&lt;h1 id=&quot;0-background&quot;&gt;0. Background&lt;/h1&gt;

&lt;p&gt;Finally this blog has something related to video games. This, &lt;a href=&quot;https://github.com/2ez4ai/Astroxygen&quot;&gt;Astroxygen&lt;/a&gt;, is my first Unity3D project after attending like three courses of Unity3D. This game is generally based on the &lt;em&gt;perfect match&lt;/em&gt; map of &lt;strong&gt;Fall Guys: Ultimate Knockout&lt;/strong&gt;, as I find the models of that are friendly given my poor modeling skills. Also, it innately allows me to personalize, to some extent, my game by using a different figure set rather than fruits. Unlike &lt;em&gt;perfect match&lt;/em&gt; that has fixed rounds, I set the goal for the player as collecting enough oxygen cylinder.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;../../../../assets/images/fallguys_pm.png&quot; alt=&quot;&quot; width=&quot;450&quot; /&gt;
&lt;/div&gt;

&lt;center&gt;
  &lt;p style=&quot;font-size:80%;&quot;&gt;
Figure 2. The perfect match map of FullGuys.
  &lt;/p&gt;
&lt;/center&gt;

&lt;h1 id=&quot;1-components&quot;&gt;1. Components&lt;/h1&gt;

&lt;p&gt;The scene includes two main components:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;a screen to display the target figure and other information;&lt;/li&gt;
  &lt;li&gt;a platform consisted of 16 tiles allowing players to stand on.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the players, there is only one controllable and others are actually scripts.&lt;/p&gt;

&lt;p&gt;To make the falling reasonable, I use a picture of space as the background. For the choice of items of CSGO, that was my very original idea since they are fascinating and highly distinguishable.&lt;/p&gt;

&lt;h1 id=&quot;2-mechanism&quot;&gt;2. Mechanism&lt;/h1&gt;

&lt;p&gt;Actually, it occurred to me quite late that listing a time table for all scripts can relieve me a lot when doing such a periodical game (at the start of the project, I dived into each logic independently, and very often got stuck with their inconsistent logic). In each round, the game proceeds as follows:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;-1.5s ~ 0.0s, ready for start;&lt;/li&gt;
  &lt;li&gt;0.0s ~ 5.0s, uncover tiles randomly for memorisation;&lt;/li&gt;
  &lt;li&gt;5.0s ~ 10.0s, uncover tiles randomly for memorisation; start counting down;&lt;/li&gt;
  &lt;li&gt;10.s ~ 15.0s, cover tiles; create oxygen tank randomly;&lt;/li&gt;
  &lt;li&gt;15.0s ~ 17.5s, disable moving ability of all players; uncover tiles to show the corresponding figures;&lt;/li&gt;
  &lt;li&gt;17.5s ~ 20.0s, disable tiles that showing wrong figures;&lt;/li&gt;
  &lt;li&gt;20.0s ~ , reset timer, tiles;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now I would like to summarize the detailed mechanism of each object.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;FallGuys&lt;/strong&gt;: it is controlled by player via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CharacterController&lt;/code&gt;. To avoid others jump over head that stacks fallguys,  I actually add a invisiable high cylinder for it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Computers&lt;/strong&gt;: to make it of great diversity, I created some materials for skins and outfits from which it choices randomly at the start of the game, and its action are actually done by a lot of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Random.RandomRange()&lt;/code&gt;. Somehow it does look like smart XD. The move is done by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;move()&lt;/code&gt; of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CharacterController&lt;/code&gt;, where I set the horizontal/vertical input manually according to their position. I intended to implement the move by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NavMeshAgent&lt;/code&gt;, but failed to make it due to the platform I implemented is not static.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;../../../../assets/images/astroxygen_computers.png&quot; alt=&quot;&quot; width=&quot;1000&quot; /&gt;
&lt;/div&gt;

&lt;center&gt;
  &lt;p style=&quot;font-size:80%;&quot;&gt;
Figure 3. The computers can arrive right tiles, as well as the wrong ones (the green computer standing on the tile with a glove figure, which is different from the one in the screen).
  &lt;/p&gt;
&lt;/center&gt;

&lt;p&gt;&lt;strong&gt;Oxygen&lt;/strong&gt;: they are created randomly in the 10th seconds of each round. The main logic is done by in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OnTriggerEnter()&lt;/code&gt;. I also add floating effect for them by changing the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;transform.position.y&lt;/code&gt;, which is also applied to the blue indicator sign.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;../../../../assets/images/astroxygen_oxygen.png&quot; alt=&quot;&quot; width=&quot;1000&quot; /&gt;
&lt;/div&gt;

&lt;center&gt;
  &lt;p style=&quot;font-size:80%;&quot;&gt;
Figure 4. The player is trying to get access to the oxygen cylinder. The screen shows that it still needs to collect 4 more oxygen cylinders.
  &lt;/p&gt;
&lt;/center&gt;

&lt;p&gt;&lt;strong&gt;Platform&lt;/strong&gt;: the removing of tiles makes the platform can not be static (there must be a lot of ways to make it, but I am ok with this so far XD). The showing and covering were all done by changing the corresponding material. It is quite easy actually.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Screen&lt;/strong&gt;: like platform, its function mostly is done by changing materials.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SphereBlock&lt;/strong&gt;: though they look gigantic, they are generally harmless, as they are floating quite high above players. The only reason I added this is that the project asks us to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NavMeshAgent&lt;/code&gt; XD. And yes, this is done with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NavMeshAgent&lt;/code&gt;.&lt;/p&gt;

&lt;h1 id=&quot;3-conclusion&quot;&gt;3. Conclusion&lt;/h1&gt;

&lt;p&gt;This project reminds me of the days when I enjoyed scenario editor of &lt;em&gt;Age of Empires II&lt;/em&gt; and &lt;em&gt;Warcraft 3&lt;/em&gt;. But at that time I was like 15 years old, and actually did not make a thing. As for my first project and given such a short time, I think this time the project is of a high degree of completeness. The main drawback, for me, is the code construction is messy due to my lacking in the game logic in the early stages.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Test comments</title>
   <link href="/2021/01/11/test/"/>
   <updated>2021-01-11T00:00:00+08:00</updated>
   <id>/2021/01/11/test</id>
   <content type="html">&lt;p&gt;test post&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Paper - Reinforcement Learning with Deep Energy-Based Policies</title>
   <link href="/2021/01/11/soft_q_learning/"/>
   <updated>2021-01-11T00:00:00+08:00</updated>
   <id>/2021/01/11/soft_q_learning</id>
   <content type="html">&lt;p&gt;&lt;em&gt;This is a brief summary of paper &lt;a href=&quot;https://arxiv.org/pdf/1702.08165.pdf&quot;&gt;Reinforcement Learning with Deep Energy-Based Policies&lt;/a&gt; for my personal interest.&lt;/em&gt;&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#0-introduction&quot; id=&quot;markdown-toc-0-introduction&quot;&gt;0. Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#1-motivation&quot; id=&quot;markdown-toc-1-motivation&quot;&gt;1. Motivation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-soft-definition&quot; id=&quot;markdown-toc-2-soft-definition&quot;&gt;2. Soft Definition&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#21-objective-function&quot; id=&quot;markdown-toc-21-objective-function&quot;&gt;2.1. Objective Function&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#22-soft-function&quot; id=&quot;markdown-toc-22-soft-function&quot;&gt;2.2. Soft Function&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3-theorem-analyses&quot; id=&quot;markdown-toc-3-theorem-analyses&quot;&gt;3. Theorem Analyses&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#31-policy-improvement&quot; id=&quot;markdown-toc-31-policy-improvement&quot;&gt;3.1. Policy Improvement&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#32-policy-iteration&quot; id=&quot;markdown-toc-32-policy-iteration&quot;&gt;3.2. Policy Iteration&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#33-soft-bellman-equation&quot; id=&quot;markdown-toc-33-soft-bellman-equation&quot;&gt;3.3. Soft Bellman Equation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#34-soft-value-iteration&quot; id=&quot;markdown-toc-34-soft-value-iteration&quot;&gt;3.4. Soft Value Iteration&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#4-algorithm&quot; id=&quot;markdown-toc-4-algorithm&quot;&gt;4. Algorithm&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#4-conclusion&quot; id=&quot;markdown-toc-4-conclusion&quot;&gt;4. Conclusion&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#5-references&quot; id=&quot;markdown-toc-5-references&quot;&gt;5. References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;0-introduction&quot;&gt;0. Introduction&lt;/h1&gt;

&lt;p&gt;After publishing the paper &lt;a href=&quot;https://arxiv.org/pdf/1702.08165.pdf&quot;&gt;soft Q learning&lt;/a&gt; in Jul 2017, the author proposed the influential algorithm &lt;a href=&quot;https://arxiv.org/pdf/1801.01290.pdf&quot;&gt;SAC&lt;/a&gt; in the next year. While SAC has received tremendous publicity, the discussion, if any, about this precedent work generally falls into the rut of &lt;a href=&quot;https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/&quot;&gt;the excellent blog&lt;/a&gt;. Therefore I decided to summarize some of thoughts and problems I had during the reading. Though this post is mainly for my personal interest, any advice will be appreciated.&lt;/p&gt;

&lt;h1 id=&quot;1-motivation&quot;&gt;1. Motivation&lt;/h1&gt;

&lt;p&gt;In reinforcement learning (RL) problem, given an agent and an environment, the objective of the agent is to learn a policy that maximizes the rewards the agent can gain. However, faced with an unknown environment, there is a tradeoff between exploitation and exploration. Before maximum entropy RL, the exploration is generally ensured by external mechanisms, such as $\varepsilon-$greedy in DQN and adding exploratory noise to the actions in DDPG. Un like those heuristic and inefficient methods, maximum entropy encourages the agent to conduct exploration by itself based on both reward and entropy. Specifically, in maximum entropy RL, the optimal policy is redefined as&lt;/p&gt;

\[\pi^\ast_\text{MaxEnt}=\arg\max_\pi\mathbb{E}_{\pi}\left[r(s_t,a_t)+\mathcal{H}(\pi(\cdot\vert s_t))\right],\tag{1}\]

&lt;p&gt;which adds a regularization term to the standard definition. Many paper adds a parameter $\alpha$ to entropy, which we will ignore in the following discussion as it does not affect the related conclusion. Based on this redefinition, the motivation of this paper can be summarized as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Generalize it to continuous cases:&lt;/p&gt;

    &lt;p&gt;Before this work, entropy maximization was mainly utilized in discrete cases. Theoretical analyses were needed for applying into continuous cases.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Take trajectory-wise entropy into consideration:&lt;/p&gt;

    &lt;p&gt;The term $\mathcal{H}(\pi(\cdot\vert s_t))$ only considers the entropy of the policy at state $s_t$. Traditional methods tend to act greedily based on the entropy at the next state. In this work, the author considers the long term entropy reward instead that of the next state.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Policy formulation:&lt;/p&gt;

    &lt;p&gt;Even though we have an objective function given the definition, it still needs a probabilistic definition from which we can make sampling. Instead of using conditional Gaussian distribution like many other works, the author borrows the idea of Boltzmann distribution.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;2-soft-definition&quot;&gt;2. Soft Definition&lt;/h1&gt;

&lt;p&gt;We now discuss the related definition in this paper.&lt;/p&gt;

&lt;h2 id=&quot;21-objective-function&quot;&gt;2.1. Objective Function&lt;/h2&gt;

&lt;p&gt;The optimal policy in this paper is defined as&lt;/p&gt;

\[\pi^\ast=\arg\max_\pi\sum_t\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}\left[r(s_t,a_t)+\mathcal{H}(\pi(\cdot\vert s_t))\right],\tag{2}\]

&lt;p&gt;which differs from the one defined in $(1)$ as it aims to reach states where they may have high entropy in the future. Specifically, a detailed version is given by&lt;/p&gt;

\[\pi^\ast=\arg\max_\pi\sum_t\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}\left[\sum_{l=t}^\infty \gamma^{l-t}\mathbb{E}_{(s_l,a_l)\sim\rho_\pi}\left[r(s_l,a_l)+\mathcal{H}(\pi(\cdot\vert s_l))\right]\bigg\vert s_t,a_t\right],\tag{3}\]

&lt;p&gt;where the first expectation \(\sum_t\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}\) is over all the pairs \((s_t, a_t)\) at any time step, and the second expectation \(\sum_{l=t}^\infty \gamma^{l-t}\mathbb{E}_{(s_l,a_l)\sim\rho_\pi}\) is over all the trajectories originating from $(s_t,a_t)$.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In the original paper, the detailed version is actually given by&lt;/p&gt;

\[\pi^\ast_\text{MaxEnt}=\arg\max_\pi\sum_t\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}\left[\sum_{l=t}^\infty \gamma^{l-t}\mathbb{E}_{(s_l,a_l)}\left[r(s_t,a_t)+\mathcal{H}(\pi(\cdot\vert s_t))\right]\bigg\vert s_t,a_t\right].\]

  &lt;p&gt;Problem: The subscripts (shown in red below) in the second expectation does confuse me a lot. Why it is over $t$ rather than $l$?&lt;/p&gt;

\[r(s_{\color{red}t},a_{\color{red}t})+\mathcal{H}(\pi(\cdot\vert s_{\color{red}t}))?\]
&lt;/blockquote&gt;

&lt;h2 id=&quot;22-soft-function&quot;&gt;2.2. Soft Function&lt;/h2&gt;

&lt;p&gt;The corresponding $Q$-function in this paper is defined as&lt;/p&gt;

\[Q_\text{soft}^\pi(s_t,a_t)\triangleq r(s_t,a_t)+\sum_{l=t+1}^\infty \gamma^{l-t}\mathbb{E}_{(s_{l},a_{l})\sim\rho_\pi}\left[r(s_{l},a_{l})+\mathcal{H}(\pi(\cdot\vert s_{l}))\right].\tag{4}\]

&lt;p&gt;Notice that the entropy at state $s_t$ is omitted in the definition.&lt;/p&gt;

&lt;p&gt;The corresponding value function is given by&lt;/p&gt;

\[V_\text{soft}^\pi(s_t)\triangleq \log \int_{\mathcal{A}}\exp\left(Q_\text{soft}^\pi(s_t,a)\right)da,\tag{5}\]

&lt;p&gt;which is actually in the form of log sum exponential that approximates maximum.&lt;/p&gt;

&lt;p&gt;Given the two definitions, the soft policy is then given by&lt;/p&gt;

\[\pi(a_t\vert s_t)=\exp\left(Q^\pi_\text{soft}(s_t,a_t)-V^\pi_\text{soft}(s_t)\right).\tag{6}\]

&lt;p&gt;As $V^\pi_\text{soft}$ only depends on $Q^\pi_\text{soft}$, the soft policy is actually the Boltzmann distribution based on the value of $Q^\pi_\text{soft}$. The comparison is shown in Figure 1. It can be found that Boltzmann distribution assigns a reasonable likelihood for all actions (rather than just the optimal one).&lt;/p&gt;

&lt;figure&gt;
    &lt;div style=&quot;display:flex&quot;&gt;
            &lt;figure&gt;
&lt;img src=&quot;http://bair.berkeley.edu/static/blog/softq/figure_3a_unimodal-policy.png&quot; /&gt;
                &lt;figcaption&gt;&lt;center&gt;(a)&lt;/center&gt;&lt;/figcaption&gt;
            &lt;/figure&gt;
            &lt;figure&gt;
&lt;img src=&quot;http://bair.berkeley.edu/static/blog/softq/figure_3b_multimodal_policy.png&quot; /&gt;
                &lt;figcaption&gt;&lt;center&gt;(b)&lt;/center&gt;&lt;/figcaption&gt;
            &lt;/figure&gt;
    &lt;/div&gt;
&lt;/figure&gt;
&lt;center&gt;
&lt;p style=&quot;font-size:80%;&quot;&gt;
Figure 1. Policies based on the value of Q function. (a) Unimodal policy. (b) Multimodal policy.
  &lt;/p&gt;
&lt;/center&gt;
&lt;p&gt;With those definitions, we have proposed the solutions to the problems mentioned in the motivation: continuous function for continuous states and actions space; the trajectory-wise optimization defined by the objective function $(3)$; and Boltzmann distribution to represent the optimal policy. To ensure things work, we need theoretical analyses and feasible update rules.&lt;/p&gt;

&lt;h1 id=&quot;3-theorem-analyses&quot;&gt;3. Theorem Analyses&lt;/h1&gt;

&lt;p&gt;We now discuss the related theoretical guarantee. The first theorem shows us the optimality:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 1&lt;/strong&gt;: The optimal policy for equation (3) is given by&lt;/p&gt;

\[\pi^\ast(a_t\vert s_t)=\exp\left(Q^\ast_\text{soft}(s_t,a_t)-V^\ast_\text{soft}(s_t)\right).\]

&lt;p&gt;The proof sketch follows two steps: policy improvement and policy iteration.&lt;/p&gt;

&lt;h2 id=&quot;31-policy-improvement&quot;&gt;3.1. Policy Improvement&lt;/h2&gt;

&lt;p&gt;We first show that given any policy, we can improve it by ‘softmizing’ it. Specifically,&lt;/p&gt;

\[\forall\pi,\text{let }\tilde\pi(\cdot\vert s)\propto\exp\left(Q^\pi_\text{soft}(s,\cdot)\right),\text{then }Q^\pi_\text{soft}(s_t,a_t)\le Q^{\tilde\pi}_\text{soft}(s_t,a_t).\]

&lt;p&gt;To show that, we rewrite the second part of $Q$ function (defined in (4)) as&lt;/p&gt;

\[\begin{aligned}&amp;amp;\sum_{l=t+1}^\infty \gamma^{l-t}\mathbb{E}_{(s_{l},a_{l})\sim\rho_\pi}\left[r(s_{l},a_{l})+\mathcal{H}(\pi(\cdot\vert s_{l}))\right]\\=&amp;amp;\mathbb{E}_{(s_{t+1},a_{t+1})\sim\rho_\pi}[[\gamma\mathcal{H}(\pi(\cdot\vert s_{l}))+\gamma r(s_{t+1},a_{t+1})\\&amp;amp;+\sum_{l=t+2}^\infty \gamma^{l-t}\mathbb{E}_{(s_{l},a_{l})\sim\rho_\pi}[r(s_{l},a_{l})+\mathcal{H}(\pi(\cdot\vert s_{l}))]]\\=&amp;amp;\mathbb{E}_{(s_{t+1},a_{t+1})\sim\rho_\pi}[\gamma\mathcal{H}(\pi(\cdot\vert s_{l}))+\gamma (r(s_{t+1},a_{t+1})\\&amp;amp;+ \sum_{l=t+2}^\infty \gamma^{l-t-1}\mathbb{E}_{(s_{l},a_{l})\sim\rho_\pi}[r(s_{l},a_{l})+\mathcal{H}(\pi(\cdot\vert s_{l}))])]\\=&amp;amp;\mathbb{E}_{(s_{t+1},a_{t+1})\sim\rho_\pi}[\gamma\mathcal{H}(\pi(\cdot\vert s_{t+1}))+\gamma Q^\pi_\text{soft}(s_{t+1},a_{t+1})].\end{aligned}\]

&lt;p&gt;As the entropy term is independent of $a_{t+1}$, we then have the following equation&lt;/p&gt;

\[Q^\pi_\text{soft}(s_t,a_t)=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}[\mathcal{H}(\pi(\cdot\vert s_{t+1}))+\mathbb{E}_{a_{t+1}\sim\pi}\left[Q^\pi_\text{soft}(s_{t+1},a_{t+1})]\right].\tag{7}\]

&lt;p&gt;We then provide an inequality:&lt;/p&gt;

\[\mathcal{H}(\pi(\cdot\vert s_{t}))+\mathbb{E}_{a_{t}\sim\pi}\left[Q^\pi_\text{soft}(s_{t},a_{t})\right]\le \mathcal{H}(\tilde\pi(\cdot\vert s_{t}))+\mathbb{E}_{a_{t}\sim\tilde\pi}\left[Q^\pi_\text{soft}(s_{t},a_{t})\right].\tag{8}\]

&lt;p&gt;&lt;em&gt;Proof of (8):&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We rewrite the left hand side of the inequality&lt;/p&gt;

\[\begin{aligned}&amp;amp;\mathcal{H}(\pi(\cdot\vert s_{t}))+\mathbb{E}_{a_{t}\sim\pi}\left[Q^\pi_\text{soft}(s_{t},a_{t})\right]\\=&amp;amp;\mathbb{E}_{a_t\sim \pi}\left[{\color{red}{-\log\pi(a_t\vert s_t)}}+Q^\pi_\text{soft}(s_t,a_t)\right]\\=&amp;amp;\mathbb{E}_{a_t\sim \pi}\left[-\log\pi(a_t\vert s_t){\color{red}{+\log\tilde{\pi}(a_t\vert s_t)-\log\tilde{\pi}(a_t\vert s_t)}}+Q^\pi_\text{soft}(s_t,a_t)\right]\\=&amp;amp;-\mathbb{E}_{a_t\sim \pi}\left[\log\pi(a_t\vert s_t)-\log\tilde{\pi}(a_t\vert s_t)\right]+\mathbb{E}_{a_t\sim \pi}\left[Q^\pi_\text{soft}(s_t,a_t)-\log\tilde{\pi}(a_t\vert s_t)\right]\\=&amp;amp;-D_\text{KL}(\pi\vert\vert\tilde{\pi})+\mathbb{E}_{a_t\sim\pi}\left[Q^\pi_\text{soft}(s_t,a_t)-Q^\pi_\text{soft}(s_t,a_t)+\log\int_\mathcal{A}\exp(Q^\pi_\text{soft}(s_t,a'))da'\right]\\=&amp;amp;-D_\text{KL}(\pi\vert\vert\tilde{\pi})+\log\int_\mathcal{A}\exp(Q^\pi_\text{soft}(s_t,a'))da’.\end{aligned}\]

&lt;p&gt;For the right hand side of the inequality, we have&lt;/p&gt;

\[\begin{aligned}&amp;amp;\mathcal{H}(\tilde\pi(\cdot\vert s_{t}))+\mathbb{E}_{a_{t}\sim\tilde\pi}\left[Q^\pi_\text{soft}(s_{t},a_{t})\right]\\=&amp;amp;\mathbb{E}_{a_t\sim \tilde\pi}\left[{\color{red}{-\log\tilde\pi(a_t\vert s_t)}}+Q^\pi_\text{soft}(s_t,a_t)\right]\\=&amp;amp;\mathbb{E}_{a_t\sim \tilde\pi}\left[-\log\frac{\exp\left(Q^\pi_\text{soft}(s_t,a_t)\right)}{\int_\mathcal{A}\exp\left(Q^\pi_\text{soft}(s_t,a')\right)da'}+Q^\pi_\text{soft}(s_t,a_t)\right]\\=&amp;amp;\mathbb{E}_{a_t\sim \tilde\pi}\left[-Q^\pi_\text{soft}(s_t,a_t)+\log\int_\mathcal{A}\exp\left(Q^\pi_\text{soft}(s_t,a')\right)da'+Q^\pi_\text{soft}(s_t,a_t)\right]\\=&amp;amp;\log\int_\mathcal{A}\exp\left(Q^\pi_\text{soft}(s_t,a')\right)da’.\end{aligned}\]

&lt;p&gt;Since $D_\text{KL}\ge 0$, we have&lt;/p&gt;

\[\mathcal{H}(\pi(\cdot\vert s_{t}))+\mathbb{E}_{a_{t}\sim\pi}\left[Q^\pi_\text{soft}(s_{t},a_{t})\right]\le \mathcal{H}(\tilde\pi(\cdot\vert s_{t}))+\mathbb{E}_{a_{t}\sim\tilde\pi}\left[Q^\pi_\text{soft}(s_{t},a_{t})\right].\tag*{$\blacksquare$}\]

&lt;p&gt;With (7) and (8), we now ready to show policy improvement. The idea is simple: we use inequality (8) to contract the right hand side of equality (7) to complete the proof.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof of policy improvement:&lt;/em&gt;&lt;/p&gt;

\[\begin{aligned}Q_\text{soft}^\pi(s_t,a_t)=&amp;amp; r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}\left[\mathcal{H}(\pi(\cdot\vert s_{t+1}))+\mathbb{E}_{a_{t+1}\sim\pi}[Q_\text{soft}^\pi(s_{t+1},a_{t+1})]\right]\\\le&amp;amp; r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}\left[\mathcal{H}({\color{red}{\tilde{\pi}}}(\cdot\vert s_{t+1}))+ \mathbb{E}_{a_{t+1}\sim{\color{red}{\tilde{\pi}}}}[Q^\pi_\text{soft}(s_{t+1},a_{t+1})]\right]\\=&amp;amp;r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}[\mathcal{H}({\color{red}{\tilde{\pi}}}(\cdot\vert s_{t+1}))\\&amp;amp;+ \mathbb{E}_{a_{t+1}\sim{\color{red}{\tilde{\pi}}}}[r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+2}\sim\mathcal{P}}[\mathcal{H}(\pi(\cdot\vert s_{t+2}))+\mathbb{E}_{a_{t+2}\sim\pi}[Q_\text{soft}^\pi(s_{t+2},a_{t+2})]]]\\\le &amp;amp;r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}[\mathcal{H}({\color{red}{\tilde{\pi}}}(\cdot\vert s_{t+1}))\\&amp;amp;+ \mathbb{E}_{a_{t+1}\sim{\color{red}{\tilde{\pi}}}}[r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+2}\sim\mathcal{P}}[\mathcal{H}({\color{red}{\tilde{\pi}}}(\cdot\vert s_{t+2}))+\mathbb{E}_{a_{t+2}\sim{\color{red}{\tilde{\pi}}}}[Q_\text{soft}^\pi(s_{t+2},a_{t+2})]]]\\ \vdots \\ \le &amp;amp; r(s_t,a_t)+\sum_{l=t+1}^\infty \mathbb{E}_{(s_l,a_l)\sim\rho_{\tilde\pi}}\left[r(s_l,a_l)+\mathcal{H}(\tilde\pi(\cdot\vert s_l))\right]\\=&amp;amp; Q^{\tilde\pi}_\text{soft}(s_t,a_t).\end{aligned}\]

&lt;p&gt;Therefore we complete the proof. \(\tag*{$\blacksquare$}\)&lt;/p&gt;

&lt;h2 id=&quot;32-policy-iteration&quot;&gt;3.2. Policy Iteration&lt;/h2&gt;

&lt;p&gt;With &lt;em&gt;policy improvement&lt;/em&gt; theorem, we can improve any arbitrary policy. Therefore the policy can be naturally updated by&lt;/p&gt;

\[\pi_{i+1}(\cdot \vert s_t)\propto \exp\left(Q^{\pi_i}_\text{soft}(s_t,\cdot)\right).\]

&lt;p&gt;Since any policy can be improved in this way, the optimal policy must satisfy this form, and the proof of &lt;em&gt;Theorem 1&lt;/em&gt; is completed. \(\tag*{$\blacksquare$}\)&lt;/p&gt;

&lt;h2 id=&quot;33-soft-bellman-equation&quot;&gt;3.3. Soft Bellman Equation&lt;/h2&gt;

&lt;p&gt;Though we have that the optimal policy can be obtained by policy iteration, it would be exhausting to conduct the iteration exactly in that way (just think about the integral we omit with the help of $\propto$)! Therefore, a more feasible way is to find the optimal $Q$ function (which is why they call the algorithm &lt;em&gt;soft Q learning&lt;/em&gt;, I guess) as&lt;/p&gt;

\[\pi^\ast(a_t\vert s_t)\propto\exp\left(Q^\ast_\text{soft}(s_t,a_t)\right).\]

&lt;p&gt;We now show the soft Bellman optimality equation which connects the two optimal function.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 2.&lt;/strong&gt; The soft $Q$ function defined in (4) satisfies the soft Bellman equation&lt;/p&gt;

\[Q^\ast_\text{soft}(s_t,a_t)=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}\left[V^\ast_\text{soft}(s_{t+1})\right].\]

&lt;p&gt;&lt;em&gt;Proof of Theorem 2&lt;/em&gt;:&lt;/p&gt;

&lt;p&gt;The proof is pretty straightforward. Notice that&lt;/p&gt;

\[\begin{aligned}&amp;amp;\mathcal{H}(\pi(\cdot\vert s_{t+1}))+\mathbb{E}_{a_{t+1}\sim\pi}[Q_\text{soft}^\pi(s_{t+1},a_{t+1})]\\
=&amp;amp;\mathbb{E}_{a_{t+1}\sim\pi}[-\log \pi(a_{t+1}\vert s_{t+1})+Q_\text{soft}^\pi(s_{t+1},a_{t+1})]\\
=&amp;amp;\mathbb{E}_{a_{t+1}\sim\pi}[-\log \exp(Q_\text{soft}^\pi(s_{t+1},a_{t+1})-V_\text{soft}^\pi(s_{t+1}))+Q_\text{soft}^\pi(s_{t+1},a_{t+1})]\\
=&amp;amp;\mathbb{E}_{a_{t+1}\sim\pi}[V_\text{soft}^\pi(s_{t+1})]\\
=&amp;amp;V_\text{soft}^\pi(s_{t+1}).\end{aligned}\]

&lt;p&gt;Therefore the soft $Q$ function defined in (4) is equivalent to&lt;/p&gt;

\[\begin{aligned}Q_\text{soft}^\pi(s_t,a_t)&amp;amp;= r(s_t,a_t)+\sum_{l=t+1}^\infty \gamma^{l-t}\mathbb{E}_{(s_{l},a_{l})\sim\rho_\pi}\left[r(s_{l},a_{l})+\mathcal{H}(\pi(\cdot\vert s_{l}))\right]\\&amp;amp;=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}\left[V^\pi_\text{soft}(s_{t+1})\right].\end{aligned}\]

\[\tag*{$\blacksquare$}\]

&lt;p&gt;Theorem 2 actually sheds light on how we update our $Q$ function, which will be introduced in next section.&lt;/p&gt;

&lt;h2 id=&quot;34-soft-value-iteration&quot;&gt;3.4. Soft Value Iteration&lt;/h2&gt;

&lt;p&gt;So far we have shown the optimality of soft policy (&lt;em&gt;Theorem 1&lt;/em&gt;) and soft $Q$ function &lt;em&gt;(Theorem 2)&lt;/em&gt;. However, we still need a rule to learn the function. Specifically, we mainly focus on the update rule of $Q$ function as the policy and value function both are defined by $Q$ function. To this end, the author provides the following theorem.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 3.&lt;/strong&gt; The iteration&lt;/p&gt;

\[Q^\pi_\text{soft}(s_t,a_t)\gets r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}\left[V^\pi_\text{soft}(s_{t+1})\right],\]

\[V_\text{soft}^\pi(s_t)\gets \log \int_{\mathcal{A}}\exp\left(Q_\text{soft}^\pi(s_t,a)\right)da,\]

&lt;p&gt;converges to \(Q^\ast_\text{soft}\) and \(V^\ast_\text{soft}\), respectively.&lt;/p&gt;

&lt;p&gt;The proof is quite similar to the general case in RL. For the detailed proof one can refer to the paper directly. Notice that the update of $Q$ function does not involve the policy, therefore it is an off-policy RL.&lt;/p&gt;

&lt;h1 id=&quot;4-algorithm&quot;&gt;4. Algorithm&lt;/h1&gt;

&lt;p&gt;Given the above analyses, there are two key issues in designing a truly practical algorithm:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The intractable integral for computing the value of $V^\pi_\text{soft}$;&lt;/li&gt;
  &lt;li&gt;The intractable sampling from Boltzmann distribution.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To deal with the integral issue, this paper leverages &lt;em&gt;importance sampling&lt;/em&gt;, which has been widely used in many previous works. For the second issue, generally speaking, the author uses a neural network to approximate the Boltzmann distribution of the policy (rather than the policy itself, and that differs from actor-critic, claimed by the author), and the loss function is defined as&lt;/p&gt;

\[J_\pi(\phi;s_t)=D_\text{KL}\left(\pi^\phi(a_t\vert s_t)\bigg\vert\bigg\vert\exp\left(Q^\theta_\text{soft}(s_t,a_t)-V^\theta_\text{soft}(s_t)\right)\right).\]

&lt;p&gt;The gradient of the loss function is given by &lt;em&gt;Stein Variational Gradient Descent&lt;/em&gt; (SVGD). In my view, the use of SVGD is mainly for the analysis of the resemblance between the proposed algorithm, soft Q learning (SQL), and actor-critic, as the succeeding works seem to use no SVGD anymore.&lt;/p&gt;

&lt;p&gt;The author provides the implementation in &lt;a href=&quot;https://github.com/haarnoja/softqlearning&quot;&gt;github-softqlearning&lt;/a&gt;. However, the latest version is faced with dependencies issue. Luckily, the older version (committed on Oct 30, 2017) works well. Other feasible implementation can be hardly found. For the performance, I tested it on Multigoal environment and the results are consistent with that of the original paper. Besides, I conducted experiments with varying values of $\alpha$, which is shown in Figure 2.&lt;/p&gt;

&lt;figure&gt;
    &lt;div style=&quot;display:flex&quot;&gt;
            &lt;figure&gt;
&lt;img src=&quot;../../../../assets/images/sql_alpha0_500.png&quot; /&gt;
                &lt;figcaption&gt;&lt;center&gt;$\alpha=0$&lt;/center&gt;&lt;/figcaption&gt;
            &lt;/figure&gt;
            &lt;figure&gt;
&lt;img src=&quot;../../../../assets/images/sql_alpha05_500.png&quot; /&gt;
                &lt;figcaption&gt;&lt;center&gt;$\alpha=0.5$&lt;/center&gt;&lt;/figcaption&gt;
            &lt;/figure&gt;
            &lt;figure&gt;
&lt;img src=&quot;../../../../assets/images/sql_alpha1_500.png&quot; /&gt;
                &lt;figcaption&gt;&lt;center&gt;$\alpha=1$&lt;/center&gt;&lt;/figcaption&gt;
            &lt;/figure&gt;
    &lt;/div&gt;
&lt;/figure&gt;
&lt;center&gt;
&lt;p style=&quot;font-size:80%;&quot;&gt;
Figure 2. After 500 steps, the performance of SQL in Multigoal with differnt values of $\alpha$.
  &lt;/p&gt;
&lt;/center&gt;

&lt;p&gt;Generally speaking, when $\alpha\ne 0$, SQL tends to have a high variance, which can also be viewed as the cost for exploration.&lt;/p&gt;

&lt;h1 id=&quot;4-conclusion&quot;&gt;4. Conclusion&lt;/h1&gt;

&lt;p&gt;It was proven that bringing in the entropy term does work for the end of better exploring. The author successfully extended the entropy RL framework to contiunous case in this paper. However, the high variance makes it challenging to be used for performing complicated tasks, and that may be one of the reasons why little (relatively) ink has been spilled on SQL. A wise choice could be to use SQL as an initializer rather than a trainer.&lt;/p&gt;

&lt;h1 id=&quot;5-references&quot;&gt;5. References&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1702.08165.pdf&quot;&gt;Reinforcement Learning with Deep Energy-Based Policies&lt;/a&gt; - Tuomas Haarnoja et al.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1801.01290.pdf&quot;&gt;Soft Actor-Critic&lt;/a&gt; - Tuomas Haarnoja et al.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/&quot;&gt;Learning Diverse Skills via Maximum Entropy Deep Reinforcement Learning&lt;/a&gt; - BAIR&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://julien-vitay.net/deeprl/EntropyRL.html&quot;&gt;Deep Reinforcement Learning&lt;/a&gt; - Julien Vitay&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.slideshare.net/DongMinLee32/maximum-entropy-reinforcement-learning-stochastic-control&quot;&gt;Maximum Entropy Reinforcement Learning (Stochastic Control)&lt;/a&gt; - Dongmin Lee&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1608.04471.pdf&quot;&gt;Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm&lt;/a&gt; - Qiang Liu and Dilin Wang&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Reinforcement Learning - Soft Q Learning</title>
   <link href="/2021/01/09/soft_q_lear/"/>
   <updated>2021-01-09T00:00:00+08:00</updated>
   <id>/2021/01/09/soft_q_lear</id>
   <content type="html">&lt;p&gt;&lt;em&gt;This is a summary of paper &lt;a href=&quot;https://arxiv.org/pdf/1702.08165.pdf&quot;&gt;Reinforcement Learning with Deep Energy-Based Policies&lt;/a&gt; for my personal interest.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;0-introduction&quot;&gt;0. Introduction&lt;/h1&gt;

&lt;p&gt;This paper was published in Jul 2017. After that the author proposed the influential algorithm &lt;a href=&quot;https://arxiv.org/pdf/1801.01290.pdf&quot;&gt;SAC&lt;/a&gt; in the next year. Though SAC has received tremendous publicity, this precedent work&lt;/p&gt;

&lt;h1 id=&quot;1-variable-elimination&quot;&gt;1. Variable Elimination&lt;/h1&gt;

&lt;p&gt;We first consider the marginal inference in a &lt;em&gt;chain&lt;/em&gt; Bayesian network which has the following joint distribution,&lt;/p&gt;

\[P(x_1,x_2,\dots,x_n)=P(x_1)\prod_{i=2}^nP(x_i\vert x_{i-1}).\]

&lt;p&gt;Suppose we want to infer the marginal distribution $P(x_n)$. A naive way to do that is marginalizing $x_1,x_2,\dots,x_{n-1}$:&lt;/p&gt;

\[P(x_n)=\sum_{x_1}\sum_{x_2}\dots\sum_{x_{n-1}}P(x_1,x_2,\dots,x_{n-1}).\]

&lt;p&gt;However, as there are $n-1$ variables each with $k$ states, the computation needs to sum the probability over $k^{n-1}$ values and would scale exponentially with the length of the chain. To simplify the computation, we can leverage &lt;em&gt;variable elimination&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The key of &lt;em&gt;variable elimination&lt;/em&gt; is in &lt;em&gt;rearranging the order&lt;/em&gt; of the summations and the multiplications. Specifically, the marginal distribution follows that&lt;/p&gt;

\[\begin{aligned}P(x_n)&amp;amp;=\sum_{x_1}\sum_{x_2}\dots\sum_{x_{n-1}}P(x_1)\prod_{i=2}^nP(x_i\vert x_{i-1})\\&amp;amp;=\sum_{x_{n-1}}P(x_n\vert x_{n-1})\cdot \sum_{x_{n-2}}P(x_{n-1}\vert x_{n-2})\cdot\dots\cdot \sum_{x_{1}}P(x_{2}\vert x_{1})P(x_1).\end{aligned}\]

&lt;p&gt;Such a rearrangement works because multiplication is distributive over addition,&lt;/p&gt;

\[ab+ac=a(b+c),\]

&lt;p&gt;where the number of arithmetic operations are reduced from three (the left-hand side) to two (the right-hand side). Before we move on, we generalize the new expression to a Markov network since every Bayesian network can be transformed into a Markov network. For the chain Bayesian network we considered, we can remove all the arrows of the graph to obtain a Markov network. The obtained Markov network would have maximum cliques \(\{x_1,x_2\}\), …, \(\{x_{n-2},x_{n-1}\}\) and \(\{x_{n-1},x_{n}\}\) and the corresponding potentials are&lt;/p&gt;

\[\begin{aligned}\psi_{1,2}(x_1,x_2)&amp;amp;=P(x_2\vert x_1)P(x_1),\\&amp;amp;\vdots\\\psi_{x_{n-2},x_{n-1}}(x_{n-2},x_{n-1})&amp;amp;=P(x_{n-2}\vert x_{n-1}),\\\psi_{n-1,n}(x_{n-1},x_n)&amp;amp;=P(x_{n-1}\vert x_n).\end{aligned}\]

&lt;p&gt;The rearrangement for the Markov network then follows that&lt;/p&gt;

\[P(x_n)=\sum_{x_{n-1}}\psi_{n-1,n}(x_{n-1}, x_{n})\cdot \sum_{x_{n-2}}\psi_{n-2,n-1}(x_{n-2}, x_{n-1})\cdot\dots\cdot \sum_{x_{1}}\psi_{1,2}(x_1,x_2).\]

&lt;p&gt;Now the computation composes of $n-1$ summations. More importantly, unlike the previous one sums over $k^{n-1}$ values, this expression allows each term only need to sum over $k\times k$ values. Specifically, the sum&lt;/p&gt;

\[\sum_{x_i}\psi_{x_i,x_{i+1}}(x_{i},x_{i+1})\]

&lt;p&gt;only involves two variables and thus the summation is over $k\times k$ values. Then the overall computation is of $O(nk^2)$ complexity, which is much better than the naive $O(k^n)$ method.&lt;/p&gt;

&lt;p&gt;However, the variable elimination (VE) method requires an ordering over the variables. In fact, the running time of VE on different orderings would vary greatly, while to find the best ordering is still an NP-hard problem. Moreover, VE method for $P(x_n)$ can be hard to be generalized to other marginal distribution as it does not store the intermediate results.&lt;/p&gt;

&lt;h1 id=&quot;2-belief-propagation&quot;&gt;2. Belief Propagation&lt;/h1&gt;

&lt;p&gt;For convenience, we consider undirected graphs with tree structure, where the optimal variable elimination ordering for node $x_i$ is the post-order iteration of the subtree rooted at $x_i$. The relationship between any two directly connected nodes is decided by which node the tree is rooted at and how far the two nodes are away from the root: the close one is the parent of the farther one.&lt;/p&gt;

&lt;h2 id=&quot;21-message&quot;&gt;2.1. Message&lt;/h2&gt;

&lt;p&gt;For a tree graph, its maximum cliques contains only two nodes. By VE algorithm, to compute the marginal $P(x_i)$, we need to eliminate all nodes that are in the subtree of $x_i$. For node $x_j$, the elimination involves computing $\sum_{x_j}\psi_{x_j,x_k}(x_j,x_k)m_{j,k}$ where $x_k$ is the parent of $x_j$ in the tree. The term $m_{j,k}$ can be thought of a &lt;em&gt;message&lt;/em&gt; that $x_j$ sends to $x_k$ about the subtree rooted at $x_j$. Similarly, the computing result can be viewed as&lt;/p&gt;

\[m_{k,l}=\sum_{x_j}\psi_{x_j,x_k}(x_j,x_k)m_{j,k}\]

&lt;p&gt;that contains the information for $x_l$, the parent of $x_k$, about the subtree rooted at $x_k$. By doing so, at the end of VE, $x_i$ would receive messages from all of its immediate children and then marginalize them out to yield the final marginal.&lt;/p&gt;

&lt;p&gt;Suppose that after computing $P(x_i)$, we are interested in computing $P(x_k)$ as well. If we use VE algorithm again, we can find that the computation also involves the messages $m_{j,k}$ as node $x_k$ is still the parent of node $x_j$. Moreover, such a message is exactly the same as the one used in computing $P(x_i)$ since the graph structure does not change. Therefore, it is easy to find that if we store the intermediary messages of VE, we can obtain other marginals quickly.&lt;/p&gt;

&lt;h2 id=&quot;22-sum-product&quot;&gt;2.2. Sum-Product&lt;/h2&gt;

&lt;p&gt;Belief propagation can be viewed as a combination of VE and &lt;em&gt;caching&lt;/em&gt;. For each edge between $x_i$ and $x_j$, the messages passing on it are $m_{i,j}$ and $m_{j,i}$, which depends on the marginal we want to determine. After computing all these messages, one can compute any marginals with these messages.&lt;/p&gt;

&lt;p&gt;Belief propagation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Set a node, for example, node $x_i$, as the root;&lt;/li&gt;
  &lt;li&gt;For each $x_j$ in $N(x_i)$, &lt;em&gt;i.e.,&lt;/em&gt; the neighborhood of $x_i$, collect the messages sent to $x_i$:&lt;/li&gt;
&lt;/ul&gt;

\[m_{j,i}=\sum_{x_j}\psi_{x_i,x_j}(x_i,x_j)\psi_{x_j}(x_j)\prod_{k\in N(x_j)\setminus i}m_{k,j};\]

&lt;ul&gt;
  &lt;li&gt;For each $x_j$ in $N(x_i)$, collect the messages sent from $x_i$:&lt;/li&gt;
&lt;/ul&gt;

\[m_{i,j}=\sum_{x_i}\psi_{x_j,x_i}(x_j,x_i)\psi_{x_i}(x_i)\prod_{k\in N(x_i)\setminus j}m_{k,i}.\]

&lt;p&gt;By doing so, we can obtain all the messages with \(2\vert E\vert\) steps, where $E$ is the set of edges. Then for any marginal we have&lt;/p&gt;

\[P(x_i)=\psi_i(x_i)\prod_{k\in N(x_i)}m_{k,i}.\]

&lt;h2 id=&quot;23-max-product&quot;&gt;2.3 Max-Product&lt;/h2&gt;

&lt;p&gt;We now consider a problem of finding the set of values that have the largest probability so that&lt;/p&gt;

\[\hat{\text{x}}=\arg\max_{\text{x}} P(\text{x}).\]

&lt;p&gt;Notice that&lt;/p&gt;

\[\max_{\text{x}} P(\text{x})=\max_{\text{x}_1}\dots \max_{\text{x}_n}P(\text{x}).\]

&lt;p&gt;By &lt;em&gt;sum-product&lt;/em&gt;, we have&lt;/p&gt;

\[\begin{aligned}\max_{\text{x}} P(\text{x})&amp;amp;=\max_{x_1}\dots \max_{x_n}\psi_i(x_i)\prod_{k\in N(x_i)}m_{k,i}\\&amp;amp;=\max_{x_n}\max_{x_n-1{}}\psi_{x_n,x_{n-1}}(x_n,x_{n-1})\max_{x_{n-2}}\psi_{x_{n-1},x_{n-2}}(x_{n-1},x_{n-2})\\&amp;amp;\quad\ \dots\max_{x_1}\psi_{x_2,x_1}(x_2,x_1)\psi_{x_1}(x_1).\end{aligned}\]

&lt;p&gt;Such a method for maximizing &lt;em&gt;max-product&lt;/em&gt; is known as &lt;em&gt;max-product&lt;/em&gt; algorithm.&lt;/p&gt;

&lt;h1 id=&quot;3-conclusion&quot;&gt;3. Conclusion&lt;/h1&gt;

&lt;p&gt;In this post, we briefly introduced two algorithms for &lt;em&gt;exact inference&lt;/em&gt; in graphical models. Given a proper order of nodes, &lt;em&gt;variable elimination&lt;/em&gt; algorithm is efficient. However, the finding of the proper order is an NP-hard problem. Besides, each query of marginals needs running the algorithm, during which the computation can be highly redundant. To improve computing efficiency, &lt;em&gt;belief propagation&lt;/em&gt; stores the intermediate results as messages. After that, one can get any marginal by the messages. Moreover, we can also exploit those messages to determine the values of random variables with the largest probability, which is known as &lt;em&gt;max-product&lt;/em&gt;.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Paper - Reinforcement Learning with Deep Energy-Based Policies</title>
   <link href="/2021/01/09/TOC/"/>
   <updated>2021-01-09T00:00:00+08:00</updated>
   <id>/2021/01/09/TOC</id>
   <content type="html">&lt;p&gt;&lt;em&gt;This is a brief summary of paper &lt;a href=&quot;https://arxiv.org/pdf/1702.08165.pdf&quot;&gt;Reinforcement Learning with Deep Energy-Based Policies&lt;/a&gt; for my personal interest.&lt;/em&gt;&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#sec-1&quot; id=&quot;markdown-toc-sec-1&quot;&gt;Sec 1&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#sec-11&quot; id=&quot;markdown-toc-sec-11&quot;&gt;Sec 1.1&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#sec-12&quot; id=&quot;markdown-toc-sec-12&quot;&gt;Sec 1.2&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#sec-2&quot; id=&quot;markdown-toc-sec-2&quot;&gt;Sec 2&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#sec-21&quot; id=&quot;markdown-toc-sec-21&quot;&gt;Sec 2.1&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#sec-211&quot; id=&quot;markdown-toc-sec-211&quot;&gt;Sec 2.1.1&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;sec-1&quot;&gt;Sec 1&lt;/h1&gt;

&lt;h2 id=&quot;sec-11&quot;&gt;Sec 1.1&lt;/h2&gt;

&lt;h2 id=&quot;sec-12&quot;&gt;Sec 1.2&lt;/h2&gt;

&lt;h1 id=&quot;sec-2&quot;&gt;Sec 2&lt;/h1&gt;

&lt;h2 id=&quot;sec-21&quot;&gt;Sec 2.1&lt;/h2&gt;

&lt;h3 id=&quot;sec-211&quot;&gt;Sec 2.1.1&lt;/h3&gt;

</content>
 </entry>
 
 <entry>
   <title>Machine Learning - 09 Exact Inference of Graphical Models</title>
   <link href="/2020/12/05/inference-ml09/"/>
   <updated>2020-12-05T00:00:00+08:00</updated>
   <id>/2020/12/05/inference-ml09</id>
   <content type="html">&lt;p&gt;&lt;em&gt;The notes are based on the &lt;a href=&quot;https://github.com/shuhuai007/Machine-Learning-Session&quot;&gt;session&lt;/a&gt;, &lt;a href=&quot;https://ermongroup.github.io/cs228-notes/&quot;&gt;CS228-notes&lt;/a&gt; and &lt;a href=&quot;https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf&quot;&gt;PRML&lt;/a&gt;. For the fundamental of probability, one can refer to &lt;a href=&quot;https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view&quot;&gt;Introduction to Probability&lt;/a&gt;. Many thanks to these great works.&lt;/em&gt;&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#0-introduction&quot; id=&quot;markdown-toc-0-introduction&quot;&gt;0. Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#1-variable-elimination&quot; id=&quot;markdown-toc-1-variable-elimination&quot;&gt;1. Variable Elimination&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-belief-propagation&quot; id=&quot;markdown-toc-2-belief-propagation&quot;&gt;2. Belief Propagation&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#21-message&quot; id=&quot;markdown-toc-21-message&quot;&gt;2.1. Message&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#22-sum-product&quot; id=&quot;markdown-toc-22-sum-product&quot;&gt;2.2. Sum-Product&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#23-max-product&quot; id=&quot;markdown-toc-23-max-product&quot;&gt;2.3 Max-Product&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3-conclusion&quot; id=&quot;markdown-toc-3-conclusion&quot;&gt;3. Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;0-introduction&quot;&gt;0. Introduction&lt;/h1&gt;

&lt;p&gt;In the last &lt;a href=&quot;https://2ez4ai.github.io/2020/11/29/probabilistic_graphical_models-ml08/&quot;&gt;post&lt;/a&gt;, we introduce graphical models which is capable of representing random variables and the conditional independences among them. We now consider the problem of inference in graphical models. Particularly, we wish to compute posterior distributions of one or more nodes conditioned on some other known (observed) nodes, and the techniques we shall talk in this post are for &lt;em&gt;exact inference&lt;/em&gt;.&lt;/p&gt;

&lt;h1 id=&quot;1-variable-elimination&quot;&gt;1. Variable Elimination&lt;/h1&gt;

&lt;p&gt;We first consider the marginal inference in a &lt;em&gt;chain&lt;/em&gt; Bayesian network which has the following joint distribution,&lt;/p&gt;

\[P(x_1,x_2,\dots,x_n)=P(x_1)\prod_{i=2}^nP(x_i\vert x_{i-1}).\]

&lt;p&gt;Suppose we want to infer the marginal distribution $P(x_n)$. A naive way to do that is marginalizing $x_1,x_2,\dots,x_{n-1}$:&lt;/p&gt;

\[P(x_n)=\sum_{x_1}\sum_{x_2}\dots\sum_{x_{n-1}}P(x_1,x_2,\dots,x_{n-1}).\]

&lt;p&gt;However, as there are $n-1$ variables each with $k$ states, the computation needs to sum the probability over $k^{n-1}$ values and would scale exponentially with the length of the chain. To simplify the computation, we can leverage &lt;em&gt;variable elimination&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The key of &lt;em&gt;variable elimination&lt;/em&gt; is in &lt;em&gt;rearranging the order&lt;/em&gt; of the summations and the multiplications. Specifically, the marginal distribution follows that&lt;/p&gt;

\[\begin{aligned}P(x_n)&amp;amp;=\sum_{x_1}\sum_{x_2}\dots\sum_{x_{n-1}}P(x_1)\prod_{i=2}^nP(x_i\vert x_{i-1})\\&amp;amp;=\sum_{x_{n-1}}P(x_n\vert x_{n-1})\cdot \sum_{x_{n-2}}P(x_{n-1}\vert x_{n-2})\cdot\dots\cdot \sum_{x_{1}}P(x_{2}\vert x_{1})P(x_1).\end{aligned}\]

&lt;p&gt;Such a rearrangement works because multiplication is distributive over addition,&lt;/p&gt;

\[ab+ac=a(b+c),\]

&lt;p&gt;where the number of arithmetic operations are reduced from three (the left-hand side) to two (the right-hand side). Before we move on, we generalize the new expression to a Markov network since every Bayesian network can be transformed into a Markov network. For the chain Bayesian network we considered, we can remove all the arrows of the graph to obtain a Markov network. The obtained Markov network would have maximum cliques \(\{x_1,x_2\}\), …, \(\{x_{n-2},x_{n-1}\}\) and \(\{x_{n-1},x_{n}\}\) and the corresponding potentials are&lt;/p&gt;

\[\begin{aligned}\psi_{1,2}(x_1,x_2)&amp;amp;=P(x_2\vert x_1)P(x_1),\\&amp;amp;\vdots\\\psi_{x_{n-2},x_{n-1}}(x_{n-2},x_{n-1})&amp;amp;=P(x_{n-2}\vert x_{n-1}),\\\psi_{n-1,n}(x_{n-1},x_n)&amp;amp;=P(x_{n-1}\vert x_n).\end{aligned}\]

&lt;p&gt;The rearrangement for the Markov network then follows that&lt;/p&gt;

\[P(x_n)=\sum_{x_{n-1}}\psi_{n-1,n}(x_{n-1}, x_{n})\cdot \sum_{x_{n-2}}\psi_{n-2,n-1}(x_{n-2}, x_{n-1})\cdot\dots\cdot \sum_{x_{1}}\psi_{1,2}(x_1,x_2).\]

&lt;p&gt;Now the computation composes of $n-1$ summations. More importantly, unlike the previous one sums over $k^{n-1}$ values, this expression allows each term only need to sum over $k\times k$ values. Specifically, the sum&lt;/p&gt;

\[\sum_{x_i}\psi_{x_i,x_{i+1}}(x_{i},x_{i+1})\]

&lt;p&gt;only involves two variables and thus the summation is over $k\times k$ values. Then the overall computation is of $O(nk^2)$ complexity, which is much better than the naive $O(k^n)$ method.&lt;/p&gt;

&lt;p&gt;However, the variable elimination (VE) method requires an ordering over the variables. In fact, the running time of VE on different orderings would vary greatly, while to find the best ordering is still an NP-hard problem. Moreover, VE method for $P(x_n)$ can be hard to be generalized to other marginal distribution as it does not store the intermediate results.&lt;/p&gt;

&lt;h1 id=&quot;2-belief-propagation&quot;&gt;2. Belief Propagation&lt;/h1&gt;

&lt;p&gt;For convenience, we consider undirected graphs with tree structure, where the optimal variable elimination ordering for node $x_i$ is the post-order iteration of the subtree rooted at $x_i$. The relationship between any two directly connected nodes is decided by which node the tree is rooted at and how far the two nodes are away from the root: the close one is the parent of the farther one.&lt;/p&gt;

&lt;h2 id=&quot;21-message&quot;&gt;2.1. Message&lt;/h2&gt;

&lt;p&gt;For a tree graph, its maximum cliques contains only two nodes. By VE algorithm, to compute the marginal $P(x_i)$, we need to eliminate all nodes that are in the subtree of $x_i$. For node $x_j$, the elimination involves computing $\sum_{x_j}\psi_{x_j,x_k}(x_j,x_k)m_{j,k}$ where $x_k$ is the parent of $x_j$ in the tree. The term $m_{j,k}$ can be thought of a &lt;em&gt;message&lt;/em&gt; that $x_j$ sends to $x_k$ about the subtree rooted at $x_j$. Similarly, the computing result can be viewed as&lt;/p&gt;

\[m_{k,l}=\sum_{x_j}\psi_{x_j,x_k}(x_j,x_k)m_{j,k}\]

&lt;p&gt;that contains the information for $x_l$, the parent of $x_k$, about the subtree rooted at $x_k$. By doing so, at the end of VE, $x_i$ would receive messages from all of its immediate children and then marginalize them out to yield the final marginal.&lt;/p&gt;

&lt;p&gt;Suppose that after computing $P(x_i)$, we are interested in computing $P(x_k)$ as well. If we use VE algorithm again, we can find that the computation also involves the messages $m_{j,k}$ as node $x_k$ is still the parent of node $x_j$. Moreover, such a message is exactly the same as the one used in computing $P(x_i)$ since the graph structure does not change. Therefore, it is easy to find that if we store the intermediary messages of VE, we can obtain other marginals quickly.&lt;/p&gt;

&lt;h2 id=&quot;22-sum-product&quot;&gt;2.2. Sum-Product&lt;/h2&gt;

&lt;p&gt;Belief propagation can be viewed as a combination of VE and &lt;em&gt;caching&lt;/em&gt;. For each edge between $x_i$ and $x_j$, the messages passing on it are $m_{i,j}$ and $m_{j,i}$, which depends on the marginal we want to determine. After computing all these messages, one can compute any marginals with these messages.&lt;/p&gt;

&lt;p&gt;Belief propagation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Set a node, for example, node $x_i$, as the root;&lt;/li&gt;
  &lt;li&gt;For each $x_j$ in $N(x_i)$, &lt;em&gt;i.e.,&lt;/em&gt; the neighborhood of $x_i$, collect the messages sent to $x_i$:&lt;/li&gt;
&lt;/ul&gt;

\[m_{j,i}=\sum_{x_j}\psi_{x_i,x_j}(x_i,x_j)\psi_{x_j}(x_j)\prod_{k\in N(x_j)\setminus i}m_{k,j};\]

&lt;ul&gt;
  &lt;li&gt;For each $x_j$ in $N(x_i)$, collect the messages sent from $x_i$:&lt;/li&gt;
&lt;/ul&gt;

\[m_{i,j}=\sum_{x_i}\psi_{x_j,x_i}(x_j,x_i)\psi_{x_i}(x_i)\prod_{k\in N(x_i)\setminus j}m_{k,i}.\]

&lt;p&gt;By doing so, we can obtain all the messages with \(2\vert E\vert\) steps, where $E$ is the set of edges. Then for any marginal we have&lt;/p&gt;

\[P(x_i)=\psi_i(x_i)\prod_{k\in N(x_i)}m_{k,i}.\]

&lt;h2 id=&quot;23-max-product&quot;&gt;2.3 Max-Product&lt;/h2&gt;

&lt;p&gt;We now consider a problem of finding the set of values that have the largest probability so that&lt;/p&gt;

\[\hat{\text{x}}=\arg\max_{\text{x}} P(\text{x}).\]

&lt;p&gt;Notice that&lt;/p&gt;

\[\max_{\text{x}} P(\text{x})=\max_{\text{x}_1}\dots \max_{\text{x}_n}P(\text{x}).\]

&lt;p&gt;By &lt;em&gt;sum-product&lt;/em&gt;, we have&lt;/p&gt;

\[\begin{aligned}\max_{\text{x}} P(\text{x})&amp;amp;=\max_{x_1}\dots \max_{x_n}\psi_i(x_i)\prod_{k\in N(x_i)}m_{k,i}\\&amp;amp;=\max_{x_n}\max_{x_n-1{}}\psi_{x_n,x_{n-1}}(x_n,x_{n-1})\max_{x_{n-2}}\psi_{x_{n-1},x_{n-2}}(x_{n-1},x_{n-2})\\&amp;amp;\quad\ \dots\max_{x_1}\psi_{x_2,x_1}(x_2,x_1)\psi_{x_1}(x_1).\end{aligned}\]

&lt;p&gt;Such a method for maximizing &lt;em&gt;max-product&lt;/em&gt; is known as &lt;em&gt;max-product&lt;/em&gt; algorithm.&lt;/p&gt;

&lt;h1 id=&quot;3-conclusion&quot;&gt;3. Conclusion&lt;/h1&gt;

&lt;p&gt;In this post, we briefly introduced two algorithms for &lt;em&gt;exact inference&lt;/em&gt; in graphical models. Given a proper order of nodes, &lt;em&gt;variable elimination&lt;/em&gt; algorithm is efficient. However, the finding of the proper order is an NP-hard problem. Besides, each query of marginals needs running the algorithm, during which the computation can be highly redundant. To improve computing efficiency, &lt;em&gt;belief propagation&lt;/em&gt; stores the intermediate results as messages. After that, one can get any marginal by the messages. Moreover, we can also exploit those messages to determine the values of random variables with the largest probability, which is known as &lt;em&gt;max-product&lt;/em&gt;.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Machine Learning - 08 Probabilistic Graphical Models</title>
   <link href="/2020/11/29/probabilistic_graphical_models-ml08/"/>
   <updated>2020-11-29T00:00:00+08:00</updated>
   <id>/2020/11/29/probabilistic_graphical_models-ml08</id>
   <content type="html">&lt;p&gt;&lt;em&gt;The notes are based on the &lt;a href=&quot;https://github.com/shuhuai007/Machine-Learning-Session&quot;&gt;session&lt;/a&gt; and &lt;a href=&quot;https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf&quot;&gt;PRML&lt;/a&gt;. For the fundamental of probability, one can refer to &lt;a href=&quot;https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view&quot;&gt;Introduction to Probability&lt;/a&gt;. Many thanks to these great works.&lt;/em&gt;&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#0-introduction&quot; id=&quot;markdown-toc-0-introduction&quot;&gt;0. Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#1-bayesian-networks&quot; id=&quot;markdown-toc-1-bayesian-networks&quot;&gt;1. Bayesian Networks&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#11-conditional-independence&quot; id=&quot;markdown-toc-11-conditional-independence&quot;&gt;1.1. Conditional Independence&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#111-tail-to-tail&quot; id=&quot;markdown-toc-111-tail-to-tail&quot;&gt;1.1.1. Tail-to-tail&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#112-head-to-tail&quot; id=&quot;markdown-toc-112-head-to-tail&quot;&gt;1.1.2. Head-to-tail&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#113-head-to-head&quot; id=&quot;markdown-toc-113-head-to-head&quot;&gt;1.1.3. Head-to-head&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#114-d-separation&quot; id=&quot;markdown-toc-114-d-separation&quot;&gt;1.1.4. D-separation&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#12-markov-blanket&quot; id=&quot;markdown-toc-12-markov-blanket&quot;&gt;1.2. Markov Blanket&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-markov-network&quot; id=&quot;markdown-toc-2-markov-network&quot;&gt;2. Markov Network&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#21-conditional-independence&quot; id=&quot;markdown-toc-21-conditional-independence&quot;&gt;2.1. Conditional Independence&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#22-maximum-clique&quot; id=&quot;markdown-toc-22-maximum-clique&quot;&gt;2.2. Maximum Clique&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#23-moralization&quot; id=&quot;markdown-toc-23-moralization&quot;&gt;2.3. Moralization&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3-factor-graph&quot; id=&quot;markdown-toc-3-factor-graph&quot;&gt;3. Factor Graph&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#4-conclusion&quot; id=&quot;markdown-toc-4-conclusion&quot;&gt;4. Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;0-introduction&quot;&gt;0. Introduction&lt;/h1&gt;

&lt;p&gt;For a vector featured with multiple random variables like \(X=[x_1,x_2,\dots,x_n]\), what we care most are  &lt;em&gt;marginal probability&lt;/em&gt; \(P(x_i)\), &lt;em&gt;joint distribution&lt;/em&gt; \(P(X)\) and &lt;em&gt;conditional probability&lt;/em&gt; \(P(x_i\vert x_j)\):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Marginal distribution:&lt;/p&gt;

\[P(x_i)=\sum_{x_n}\dots\sum_{x_{i+1}}\sum_{x_{i-1}}\dots\sum_{x_1}P(X);\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Joint distribution:&lt;/p&gt;

\[P(X)=P(x_1)\cdot\prod_{i=2}^{n}P(x_i\vert x_1,\dots,x_{i-1});\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Conditional probability (&lt;em&gt;Bayesian rule&lt;/em&gt;):&lt;/p&gt;

\[P(x_i\vert x_j)=\frac{P(x_i,x_j)}{P(x_j)}.\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Obviously when $n$ is large, all of the above three can be of high computation complexity. Now we consider simplifying the computation of joint distribution, and to achieve that most methods have been proposed by&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;assuming all the features are totally &lt;em&gt;independent&lt;/em&gt;:&lt;/p&gt;

\[P(X)=\prod_{i=1}^nP(x_i);\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;or,  assuming that the features are &lt;em&gt;conditional independent&lt;/em&gt;, which is used in &lt;strong&gt;naive Bayes classifier&lt;/strong&gt; as class conditional independence:&lt;/p&gt;

\[P(X\vert Y)=\prod_{i=1}^n P(x_i\vert Y)\implies P(X)=\int_y \prod_{i=1}^nP(x_i\vert y)P(y)\text{d}y;\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;or, based on &lt;em&gt;conditional independent&lt;/em&gt;, assuming that the features process the &lt;em&gt;Markov Property&lt;/em&gt;, which is used in &lt;strong&gt;hidden Markov models&lt;/strong&gt;:&lt;/p&gt;

\[P(x_j\vert x_1,x_2,\dots,x_{j-1})=P(x_j\vert x_{j-1})\implies P(X)=P(x_1)\cdot\prod_{i=2}^nP(x_i\vert x_{i-1}).\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Among them, &lt;em&gt;graphical probabilistic models&lt;/em&gt; (PGM) are generally based on the &lt;em&gt;conditional independence assumption&lt;/em&gt;, ‘&lt;em&gt;capturing the way in which the joint distribution over all of the random variables can be decomposed into a product of factors each depending only on a subset of the variables&lt;/em&gt;’. (&lt;a href=&quot;https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf&quot;&gt;PRML&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;By leveraging the manipulations and properties of graph, many probabilistic insights can be obtained, motivating the design of new models.&lt;/p&gt;

&lt;h1 id=&quot;1-bayesian-networks&quot;&gt;1. Bayesian Networks&lt;/h1&gt;

&lt;p&gt;We now consider general graphical probabilistic models &lt;em&gt;Bayesian networks&lt;/em&gt; which are defined by &lt;em&gt;directed acyclic graphs&lt;/em&gt; (DAGs). Given a DAG, the &lt;em&gt;nodes&lt;/em&gt; in a Bayesian network represent random variables and edges represent conditional dependencies among those variables.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;../../../../assets/images/Figure8.2.png&quot; alt=&quot;Figure8.2 in PRML&quot; width=&quot;250&quot; /&gt;
&lt;/div&gt;

&lt;center&gt;
  &lt;p style=&quot;font-size:80%;&quot;&gt;
Figure 1. Example of a directed acyclic graph (Figure 8.2 of PRML).
  &lt;/p&gt;
&lt;/center&gt;

&lt;p&gt;A Bayesian network actually represents a joint distribution over all the random variables represented by its nodes. As it is shown in Figure 1 (from PRML), there are 7 random variables. The edge going from a node $x_i$ to a node $x_j$ indicates that $x_j$ is dependent to $x_i$. The joint distribution of the model in Figure 1 is then given by&lt;/p&gt;

\[P(x_1,x_2,\dots,x_7)=P(x_1)P(x_2)P(x_3)P(x_4\vert x_1,x_2,x_3)P(x_5\vert x_1,x_3)P(x_6\vert x_4)P(x_7\vert x_4,x_5).\]

&lt;p&gt;More generally, for a graph with $K$ nodes, the corresponding joint distribution is given by&lt;/p&gt;

\[P(X)=\prod_{k=1}^KP(x_k\vert \mathbb{Pa}_k),\]

&lt;p&gt;where $\mathbb{Pa}_k$ denotes the set of parents of $x_k$ and $X=(x_1,x_2,\dots,x_k)$. Such a key equation expresses the &lt;em&gt;factorization&lt;/em&gt; properties of the joint distribution for a directed graph. Compared with the joint distribution expression mentioned in Section 0, the expression of that of Figure 1 we show above definitely is simplified a lot. Such simplification is actually introduced by the &lt;em&gt;absence&lt;/em&gt; of edges in the graph, which conveys the &lt;em&gt;conditional independence&lt;/em&gt; information of those variables.&lt;/p&gt;

&lt;h2 id=&quot;11-conditional-independence&quot;&gt;1.1. Conditional Independence&lt;/h2&gt;

&lt;p&gt;Conditional independence sometimes can greatly simplify the computations needed to perform inference and learning under probabilistic models. By using graphical models, we can find the conditional independence properties directly without effort to any analytical manipulations. We start the discussion by considering three simple examples each involving graphs having just three nodes.&lt;/p&gt;

&lt;h3 id=&quot;111-tail-to-tail&quot;&gt;1.1.1. Tail-to-tail&lt;/h3&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;../../../../assets/images/Figure8.15.png&quot; alt=&quot;Figure8.15 in PRML&quot; width=&quot;250&quot; /&gt;
&lt;/div&gt;
&lt;center&gt;
  &lt;p style=&quot;font-size:80%;&quot;&gt;
Figure 2. Example of tail-to-tail for conditional independence (Figure 8.15 of PRML).
  &lt;/p&gt;
&lt;/center&gt;

&lt;p&gt;Intuitively, we can consider the path from node &lt;em&gt;a&lt;/em&gt; to node &lt;em&gt;b&lt;/em&gt; via &lt;em&gt;c&lt;/em&gt; in Figure 2. The node &lt;em&gt;c&lt;/em&gt; is said to be &lt;em&gt;tail-to-tail&lt;/em&gt; as the node is connected to the tails of the two arrows. Given such a path, we have $a$ and $b$ dependent. However, if we condition on node &lt;em&gt;c&lt;/em&gt;, then the conditioned node &lt;em&gt;c&lt;/em&gt; will &lt;em&gt;block&lt;/em&gt; the path and cause $a$ and $b$ to become conditionally independent. A naive proof is as follows.&lt;/p&gt;

&lt;p&gt;By the definition given in section 1, the joint distribution of the graph in Figure 2 is&lt;/p&gt;

\[P(a,b,c)=P(c)P(a\vert c)P(b\vert c).\]

&lt;p&gt;By the general definition of the marginal distribution, we have&lt;/p&gt;

\[P(a,b)=\sum_c P(c)P(a\vert c)P(b\vert c),\]

&lt;p&gt;which generally dose not factorize into the product \(P(a)\cdot P(b)\), therefore $a$ and $b$ are dependent, denoted as&lt;/p&gt;

\[a\not\perp b\ \ \ \ \ \ \text{or, equivalenly,}\ \ \ \ \ \ a\not\!\perp\!\!\!\perp b\vert \emptyset.\]

&lt;p&gt;By the general definition of the joint distribution, we have&lt;/p&gt;

\[P(a,b,c)=P(c)P(a\vert c)P(b\vert a,c).\]

&lt;p&gt;Thus it follows that&lt;/p&gt;

\[\begin{alignat*}{3}&amp;amp;&amp;amp;P(c)P(a\vert c)P(b\vert a,c)&amp;amp;=P(c)P(a\vert c)P(b\vert c)\\\implies&amp;amp;&amp;amp;P(b\vert a,c)&amp;amp;=P(b\vert c)\\\implies&amp;amp;&amp;amp;\frac{P(a,b\vert c)}{P(a\vert c)}&amp;amp;=P(b\vert c)\\\implies&amp;amp;&amp;amp;P(a,b\vert c)&amp;amp;=P(a\vert c)P(b\vert c),\end{alignat*}\]

&lt;p&gt;which means&lt;/p&gt;

\[a\perp\!\!\!\perp b\vert c.\]

&lt;h3 id=&quot;112-head-to-tail&quot;&gt;1.1.2. Head-to-tail&lt;/h3&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;../../../../assets/images/Figure8.17.png&quot; alt=&quot;Figure8.17 in PRML&quot; width=&quot;250&quot; /&gt;
&lt;/div&gt;
&lt;center&gt;
  &lt;p style=&quot;font-size:80%;&quot;&gt;
Figure 3. Example of head-to-tail for conditional independence (Figure 8.17 of PRML).
  &lt;/p&gt;
&lt;/center&gt;

&lt;p&gt;The example of head-to-tail is shown in Figure 3. Similarly, the node &lt;em&gt;c&lt;/em&gt; is said to be &lt;em&gt;head-to-tail&lt;/em&gt; with respect to the path from node &lt;em&gt;a&lt;/em&gt; to node &lt;em&gt;b&lt;/em&gt;. Given such a path, we have $a$ and $b$ dependent. When node &lt;em&gt;c&lt;/em&gt; is conditioned, the case would be the same as &lt;em&gt;tail-to-tail&lt;/em&gt;: the conditioned node &lt;em&gt;c&lt;/em&gt; would &lt;em&gt;block&lt;/em&gt; the path and render $a$ and $b$ conditional independent. The naive proof of this is as follows.&lt;/p&gt;

&lt;p&gt;The joint distribution of the model in Figure 3 is&lt;/p&gt;

\[P(a,b,c)=P(a)P(c\vert a)P(b\vert c).\]

&lt;p&gt;Marginalizing the distribution over $c$, we have&lt;/p&gt;

\[\begin{aligned}P(a,b)&amp;amp;=\sum_c P(a)P(c\vert a)P(b\vert c)\\&amp;amp;=P(a)\sum_c P(b,c\vert a)\\&amp;amp;=P(a)P(b\vert a),\end{aligned}\]

&lt;p&gt;which means \(a\not\perp b\).&lt;/p&gt;

&lt;p&gt;By rewriting the general joint distribution expression, we have&lt;/p&gt;

\[P(a,b,c)=P(a)P(b\vert a)P(c\vert a,b).\]

&lt;p&gt;Then we have&lt;/p&gt;

\[\begin{alignat*}{3}&amp;amp;&amp;amp;P(a)P(b\vert a)P(c\vert a,b)&amp;amp;=P(a)P(c\vert a)P(b\vert c)\\\implies&amp;amp;&amp;amp; P(b\vert a)\cdot\frac{P(a,b\vert c)P(c)}{P(b\vert a)P(a)}&amp;amp;=\frac{P(a\vert c)P(c)}{P(a)}\cdot P(b\vert c)\\\implies&amp;amp;&amp;amp;P(a,b\vert c)&amp;amp;=P(a\vert c)P(b\vert c)\\\implies&amp;amp;&amp;amp;a&amp;amp;\perp\!\!\!\perp b\vert c.\end{alignat*}\]

&lt;h3 id=&quot;113-head-to-head&quot;&gt;1.1.3. Head-to-head&lt;/h3&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;../../../../assets/images/Figure8.19.png&quot; alt=&quot;Figure8.19 in PRML&quot; width=&quot;250&quot; /&gt;
&lt;/div&gt;
&lt;center&gt;
  &lt;p style=&quot;font-size:80%;&quot;&gt;
Figure 4. Example of head-to-head for conditional independence (Figure 8.19 of PRML).
  &lt;/p&gt;
&lt;/center&gt;

&lt;p&gt;The third example given in Figure 4 is opposite to the previous two cases. The node &lt;em&gt;c&lt;/em&gt; in Figure 4 is &lt;em&gt;head-to-head&lt;/em&gt; with respect to the path from &lt;em&gt;a&lt;/em&gt; to &lt;em&gt;b&lt;/em&gt; as it connects to the heads of the two arrows. In this case, node &lt;em&gt;c&lt;/em&gt; would &lt;em&gt;block&lt;/em&gt; the path while conditioning on node &lt;em&gt;c&lt;/em&gt; would &lt;em&gt;unblock&lt;/em&gt; the path and render $a$ and $b$ dependent. The proof is much similar to the previous cases.&lt;/p&gt;

&lt;p&gt;The joint distribution of the model in Figure 4 is&lt;/p&gt;

\[P(a,b,c)=P(a)P(b)P(c\vert a,b).\]

&lt;p&gt;Marginalizing the distribution over $c$, we have&lt;/p&gt;

\[\begin{aligned}P(a,b)&amp;amp;=\sum_c P(a)P(b)P(c\vert a,b)\\&amp;amp;=P(a)P(b)\sum_c P(c\vert a)\\&amp;amp;=P(a)P(b)\end{aligned}\]

&lt;p&gt;which means \(a\perp b\). In particular, the third equation is given by the fact ‘&lt;em&gt;conditional probabilities are probabilities&lt;/em&gt;’ (&lt;a href=&quot;https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view&quot;&gt;Introduction to Probability&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;By rewriting the general joint distribution expression, we have&lt;/p&gt;

\[P(a,b,c)=P(a,b\vert c)P(c).\]

&lt;p&gt;Then we have&lt;/p&gt;

\[\begin{alignat*}{3}&amp;amp;&amp;amp;P(a,b\vert c)P(c)&amp;amp;=P(a)P(b)P(c\vert a,b)\\\implies&amp;amp;&amp;amp;P(a,b\vert c)&amp;amp;=\frac{P(a)P(b)P(c\vert a,b)}{P(c)},\end{alignat*}\]

&lt;p&gt;where the last term in general does not factorize into the product $P(a\vert c)P(b\vert c)$, hence we have&lt;/p&gt;

\[a\not\!\perp\!\!\!\perp b\vert c.\]

&lt;h3 id=&quot;114-d-separation&quot;&gt;1.1.4. D-separation&lt;/h3&gt;

&lt;p&gt;With the three examples, we now introduce &lt;em&gt;D-separation&lt;/em&gt; which is used to determine the independence of those random variables in a graph. Specifically, given three arbitrary nonintersecting sets $\mathcal{A}$, $\mathcal{B}$ and $\mathcal{C}$ of the nodes of the graph, by D-separation we can determine whether the statement \(\mathcal{A}\perp\!\!\!\perp\mathcal{B}\vert \mathcal{C}\) is true under the graph.&lt;/p&gt;

&lt;p&gt;The method proceeds as follows. (I am sure this great &lt;a href=&quot;https://www.youtube.com/watch?v=yDs_q6jKHb0&quot;&gt;video&lt;/a&gt; can help you get into &lt;em&gt;D-separation&lt;/em&gt; easily.)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Find out all possible &lt;em&gt;undirected&lt;/em&gt; paths between any node in $\mathcal{A}$ and any node in $\mathcal{B}$;&lt;/li&gt;
  &lt;li&gt;Check whether those paths are blocked: for each path,
    &lt;ul&gt;
      &lt;li&gt;splitting it into continuous triples;&lt;/li&gt;
      &lt;li&gt;for each triple, its structure (with directionality concerns) must belong to one of the three examples we mentioned before, and we just need to determine whether it is blocked when conditioning on $\mathcal{C}$;&lt;/li&gt;
      &lt;li&gt;if there is at least one triple blocked, the path is said to be blocked, otherwise, the path is unblocked;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;If all the paths are blocked, the statement \(\mathcal{A}\perp\!\!\!\perp\mathcal{B}\vert \mathcal{C}\) is true and $\mathcal{A}$ is said to be d-separated from $\mathcal{B}$ by $\mathcal{C}$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;12-markov-blanket&quot;&gt;1.2. Markov Blanket&lt;/h2&gt;

&lt;p&gt;We now introduce the concept of a &lt;em&gt;Markov blanket&lt;/em&gt; or &lt;em&gt;Markov boundary&lt;/em&gt;. Consider a joint distribution $P(x_1,x_2,\dots,x_N)$ represented by a directed graph having $N$ nodes. In particular, we want to determine the conditional distribution&lt;/p&gt;

\[P(x_i\vert x_{\{j\ne i\}})=\frac{\prod_k P(x_k\vert \mathbb{Pa}_k)}{\int\prod_k P(x_k\vert \mathbb{Pa}_k)\text{d}x_i}.\]

&lt;p&gt;Notice that if the term \(p(x_k\vert \mathbb{Pa}_k)\) does not involve $x_i$, that is to say $k\ne i$ and/or $x_i\notin\mathbb{Pa}_k$, we then can remove the term from both numerator and denominator. The remaining terms in the conditional distribution then must be&lt;/p&gt;

\[P(x_k\vert \mathbb{Pa}_k), k\in\{i\}\cup\{k\vert x_i\in\mathbb{Pa}_k\},\]

&lt;p&gt;and the conditional distribution \(P(x_i\vert x_{\{j\ne i\}})\) depends only on those terms. We now discuss which nodes those terms are related to. Obviously, when \(k\in\{i\}\), the term \(P(x_i\vert \mathbb{Pa}_i)\) would only involve $x_i$ and &lt;strong&gt;the parents&lt;/strong&gt; of it. When \(k\in\{j\vert x_i\in\mathbb{Pa}_j\}\) and $k\ne i$, the term $P(x_k\vert \mathbb{Pa}_k)$ are related to two parts. The first part $x_k$ is &lt;strong&gt;the child&lt;/strong&gt; of $x_i$ as $x_i\in\mathbb{Pa}_k$, while the second part $\mathbb{Pa}_k$ is &lt;strong&gt;the co-parents&lt;/strong&gt; of the child $x_k$.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;../../../../assets/images/Figure8.26.png&quot; alt=&quot;Figure8.26 in PRML&quot; width=&quot;250&quot; /&gt;
&lt;/div&gt;
&lt;center&gt;
  &lt;p style=&quot;font-size:80%;&quot;&gt;
Figure 5. The Markov blanket of $x_i$ is denoted by colored nodes. (Figure 8.26 of PRML).
  &lt;/p&gt;
&lt;/center&gt;

&lt;p&gt;As shown in Figure 5, we say that a &lt;em&gt;Markov blanket&lt;/em&gt; $\mathcal{M}_i$ of a node $x_i$ comprises the set of its &lt;em&gt;parents&lt;/em&gt;, &lt;em&gt;child&lt;/em&gt; and &lt;em&gt;co-parents&lt;/em&gt;. Given the Markov blanket, the conditional distribution \(P(x_i\vert x_{\{j\ne i\}})\) can be rewritten as \(P(x_i\vert \mathcal{M}_i)\).&lt;/p&gt;

&lt;h1 id=&quot;2-markov-network&quot;&gt;2. Markov Network&lt;/h1&gt;

&lt;p&gt;The graphs we talked in the previous sections are directed. When it comes to undirected graphs, some concepts of directed graphs still play important roles while others do not. The graphical probabilistic models defined by &lt;em&gt;undirected graphs&lt;/em&gt; is called &lt;em&gt;Markov networks&lt;/em&gt;, also known as &lt;em&gt;Markov random fields&lt;/em&gt;. Similar to Bayesian networks, the nodes in a Markov network represent random variables. However, as edges carry no arrows in undirected graphs, the function of edges changes a lot.&lt;/p&gt;

&lt;h2 id=&quot;21-conditional-independence&quot;&gt;2.1. Conditional Independence&lt;/h2&gt;

&lt;p&gt;The conditional independence of an undirected graph is given by the &lt;em&gt;absence&lt;/em&gt; of edges. Specifically, for a graph with nodes representing random variable $x_1,x_2,\dots, x_N$, we have:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pairwise Markov Property&lt;/strong&gt;: the &lt;em&gt;absence&lt;/em&gt; of an edge between two nodes $x_i$ and $x_j$ means the corresponding random variables of the two nodes are conditionally independent given all the other random variables, which is&lt;/p&gt;

\[x_i\text{ and }x_j\text{ are not adjacent}\implies x_i\perp\!\!\!\perp x_j\vert X_{\setminus \{i,j\}},\]

&lt;p&gt;where \(X_{\setminus\{i,j\}}\) denotes the set of all the variables with $x_i$ and $x_j$ removed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Local Markov Property&lt;/strong&gt;: a random variable $x_i$ is conditionally independent of all other random variables given its neighbors, which is&lt;/p&gt;

\[x_i\perp\!\!\!\perp X_{\setminus \mathbb{Ne}_i}\vert \mathbb{Ne}_{i},\]

&lt;p&gt;where $\mathbb{Ne}_i$ is the set of neighbors of $x_i$, &lt;em&gt;i.e.,&lt;/em&gt; every node directly connected with $x_i$ is in $\mathbb{Ne}_i$. Recalling the definition of the &lt;em&gt;Markov blanket&lt;/em&gt;, we can find that \(\mathbb{Ne}_i\) is the &lt;em&gt;Markov blanket&lt;/em&gt; in the undirected graph.&lt;/p&gt;

&lt;p&gt;The property below is to Markov networks as &lt;em&gt;D-separation&lt;/em&gt; is to Bayesian networks&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Global Markov Property&lt;/strong&gt;: for any three nonintersecting sets $\mathcal{A}$, $\mathcal{B}$ and $\mathcal{C}$ of the nodes of the graph, we can determine whether&lt;/p&gt;

\[\mathcal{A}\perp\!\!\!\perp \mathcal{B}\vert\mathcal{C}\]

&lt;p&gt;by the following steps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Find out all possible paths between any node in $\mathcal{A}$ and any node in $\mathcal{B}$;&lt;/li&gt;
  &lt;li&gt;Check whether every path from $\mathcal{A}$ to $\mathcal{B}$ passes through at least one node in $\mathcal{C}$;&lt;/li&gt;
  &lt;li&gt;If so, the statement is true.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Notice that compared with Bayesian networks, the way we check a statement of the conditional independence in Markov networks actually does not entail the concept ‘&lt;em&gt;block&lt;/em&gt;’. Testing for conditional independence in undirected graphs is therefore simpler than in directed graphs.&lt;/p&gt;

&lt;p&gt;It can be shown that the three properties above are equivalent.&lt;/p&gt;

&lt;h2 id=&quot;22-maximum-clique&quot;&gt;2.2. Maximum Clique&lt;/h2&gt;

&lt;p&gt;As a Bayesian network can represent a joint distribution over finite random variables, there also exists a probability density function for each Markov network that is consistent with the three properties we mentioned above. Before moving on, we introduce a concept for a Markov network called a &lt;em&gt;clique&lt;/em&gt;, which is defined as a subset of fully connected notes of the undirected graph. Obviously, there may be many different cliques for a Markov network. Among them, we particularly focus on the cliques each of which allows no other nodes to be added without it ceasing to be a clique, and we call such a clique a &lt;em&gt;maximal clique&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Denote the maximal cliques set of a Markov network with \(\{x_1,x_2,\dots,x_N\}\) by \(C_m=\{C\vert C\text{ is a maximal clique}\}\), and the nodes in maximal clique $C$ by \(\text{x}_C=\{x_i\vert x_i\in C\}\). Then the joint distribution represented by the Markov network can be written as&lt;/p&gt;

\[P(x_1,x_2,\dots,x_N)=\frac{1}{Z}\prod_{C}^{C_m}\psi_C(\text{x}_C),\]

&lt;p&gt;where $\psi_C(\cdot)$ are positive functions called &lt;em&gt;potential functions&lt;/em&gt;, and the quantity $Z$ is called &lt;em&gt;partition function&lt;/em&gt; that validates the distribution, &lt;em&gt;i.e.&lt;/em&gt;,&lt;/p&gt;

\[Z=\sum_x\prod_C^{C_m}\psi_C(\text{x}_C).\]

&lt;p&gt;Given a Markov network, the equivalence of the joint distribution and the conditional independence can be shown by &lt;em&gt;Hammesley-Clifford theorem&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;We now consider the choice of potential functions. Given the existence of partition function, we have great flexibilities in choosing potential functions. However, it naturally raises the question of how to motivate a choice of potential function for a particular application. Since it requires the potential functions to be positive, a widely used function is &lt;em&gt;exponential function&lt;/em&gt;:&lt;/p&gt;

\[\psi_C(\text{x}_C)=\exp\{-E(\text{x}_C)\},\]

&lt;p&gt;where $E(\text{x}_C)$ is called &lt;em&gt;energy function&lt;/em&gt;. The joint distribution then is&lt;/p&gt;

\[\begin{aligned}P(x_1,x_2,\dots,x_N)&amp;amp;=\frac{1}{Z}\prod_{C}^{C_m}\psi_C(\text{x}_C)\\&amp;amp;=\frac{1}{Z}\exp\left\{-\sum_{C}^{C_m}E(\text{x}_C)\right\},\end{aligned}\]

&lt;p&gt;which is known as &lt;em&gt;Boltzmann distribution&lt;/em&gt; (or, &lt;em&gt;Gibbs distribution&lt;/em&gt;). Moreover, we can see that the distribution is consistent with the definition of &lt;a href=&quot;https://2ez4ai.github.io/2020/11/12/exponential_family-ml07/&quot;&gt;exponential families&lt;/a&gt;. The joint distribution of any Markov network in which every potential has the form of exponentials is in exponential families.&lt;/p&gt;

&lt;h2 id=&quot;23-moralization&quot;&gt;2.3. Moralization&lt;/h2&gt;

&lt;p&gt;We now consider the relation between the two graphical models. Particularly, we consider a problem of how to converting a Bayesian network to a Markov network. We start the discussion from the three examples of Bayesian networks mentioned in section 1.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Tail-to-tail: Given the tail-to-tail case shown in Figure 2, we have the joint distribution&lt;/p&gt;

\[P(a,b,c)=P(c)P(a\vert c)P(b\vert c).\]

    &lt;p&gt;A factorization can be easily obtained by identifying&lt;/p&gt;

\[\begin{aligned}P(a,b,c)&amp;amp;=\psi(a,c)\psi(b,c),\\\psi(a,c)&amp;amp;=P(c)P(a\vert c),\\\psi(b,c)&amp;amp;=P(b\vert c),\end{aligned}\]

    &lt;p&gt;which is actually the joint distribution represented by the Markov network whose maximum cliques are \(\{a,c\}\) and \(\{b,c\}\) and potential functions are \(\psi(a,c)\) and \(\psi(b,c)\). Obviously, such a Markov network is the same as the Bayesian network in Figure 2 with removing its arrows.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Head-to-tail: For the graph in Figure 3, we have&lt;/p&gt;

\[P(a,b,c)=P(a)P(c\vert a)P(b\vert c).\]

    &lt;p&gt;Similarly, by identifying&lt;/p&gt;

\[\begin{aligned}\psi(a,c)&amp;amp;=P(a)P(c\vert a),\\\psi(b,c)&amp;amp;=P(b\vert c).\end{aligned}\]

    &lt;p&gt;We have the corresponding Markov network with maximum cliques \(\{a,c\}\) and \(\{b,c\}\), which also can be obtained by removing the arrows of the graph.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Head-to-head: The case shown in Figure 4 is a little tricky. Given the joint distribution&lt;/p&gt;

\[P(a,b,c)=P(a)P(b)P(c\vert a,b),\]

    &lt;p&gt;we can find that the term \(P(c\vert a,b)\) leads to a factor that depends on three nodes. Therefore the corresponding Markov network must have a maximum clique consists of \(\{a,b,c\}\). To this end, we need to not only remove the arrows of the graph but also add an edge between $a$ and $b$. Then we have the corresponding Markov network with potential function&lt;/p&gt;

\[\psi(a,b,c)=P(a)P(b)P(c\vert a,b).\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Given the above discussion, to convert a directed graph into an undirected graph, the general steps are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Remove all the arrows in the directed graph;&lt;/li&gt;
  &lt;li&gt;Add additional edges between all pairs of parents for each node;&lt;/li&gt;
  &lt;li&gt;Initialize all the potential functions to 1;&lt;/li&gt;
  &lt;li&gt;Multiply each conditional distribution factor into the potential function whose corresponding clique contains all the variables of the factor.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The step &lt;em&gt;adding additional edges&lt;/em&gt; is known as &lt;em&gt;moralization&lt;/em&gt;. And the resulting undirected graph after &lt;em&gt;removing arrows&lt;/em&gt; is called &lt;em&gt;moral graph&lt;/em&gt;.&lt;/p&gt;

&lt;h1 id=&quot;3-factor-graph&quot;&gt;3. Factor Graph&lt;/h1&gt;

&lt;p&gt;Notice that in moralization, we may invite loops in the moral graph, which can be tricky in some cases. To avoid the issues incurred by the loops, we can leverage &lt;em&gt;factor graphs&lt;/em&gt;. Given a joint distribution of a moral graph, we can construct a factor graph by&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Remove all the edges in the graph;&lt;/li&gt;
  &lt;li&gt;Rewrite the joint distribution as a multiplication of multiple functions where the functions can depend on an arbitrary set of the nodes;&lt;/li&gt;
  &lt;li&gt;Add new nodes for each function of the new expression;&lt;/li&gt;
  &lt;li&gt;Add edges between each function and the nodes it depends on.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;div style=&quot;display:flex&quot;&gt;
            &lt;figure&gt;
&lt;img src=&quot;../../../../assets/images/Figure8.42a.png&quot; alt=&quot;Figure8.42 (a) in PRML&quot; /&gt;
                &lt;figcaption&gt;&lt;center&gt;(a)&lt;/center&gt;&lt;/figcaption&gt;
            &lt;/figure&gt;
            &lt;figure&gt;
&lt;img src=&quot;../../../../assets/images/Figure8.41a.png&quot; alt=&quot;Figure8.41 (a) in PRML&quot; /&gt;
                &lt;figcaption&gt;&lt;center&gt;(b)&lt;/center&gt;&lt;/figcaption&gt;
            &lt;/figure&gt;
            &lt;figure&gt;
&lt;img src=&quot;../../../../assets/images/Figure8.42c.png&quot; alt=&quot;Figure8.42 (c) in PRML&quot; /&gt;
                &lt;figcaption&gt;&lt;center&gt;(c)&lt;/center&gt;&lt;/figcaption&gt;
            &lt;/figure&gt;
    &lt;/div&gt;
&lt;/figure&gt;
&lt;center&gt;
&lt;p style=&quot;font-size:80%;&quot;&gt;
Figure 6. (a) A directed graph. (b) The corresponding moral graph of the directed graph. (c) A factor graph of the graph where the factors are depicted by small solid squares. (Figure 8.41 and Figure 8.42 of PRML).
  &lt;/p&gt;
&lt;/center&gt;
&lt;p&gt;An example is shown in Figure 6. The directed graph represents the joint distribution&lt;/p&gt;

\[P(x_1,x_2,x_3)=P(x_1)P(x_2)P(x_3\vert x_1,x_2).\]

&lt;p&gt;The moralization of the directed graph incurs a loop among $x_1,x_2,x_3$ as shown in Figure 6 (b). Defining $f_a(x_1)=P(x_1)$, $f_b(x_2)=P(x_2)$ and \(f_c(x_1,x_2,x_3)=P(x_3\vert x_1,x_2)\), we have&lt;/p&gt;

\[P(x_1,x_2,x_3)=f_a(x_1)f_b(x_2)f_c(x_1,x_2,x_3).\]

&lt;p&gt;The factor graph corresponding to such a factorization is shown in Figure 6 (c). Moreover, all factor graphs are &lt;em&gt;bipartite&lt;/em&gt; as they consist of two distinct kinds of nodes. With factor graphs, we can conduct the related computation based on the factor nodes rather than variable nodes so that the loop can be avoided.&lt;/p&gt;

&lt;h1 id=&quot;4-conclusion&quot;&gt;4. Conclusion&lt;/h1&gt;

&lt;p&gt;In this post, we first introduced two kinds of probabilistic graphical models. One is &lt;em&gt;Bayesian networks&lt;/em&gt; that is based on directed acyclic graph. The other is &lt;em&gt;Markov network&lt;/em&gt; that is based on undirected graph. Both two models can be used to represent the joint distribution and reflect the conditional independences over a set of random variables. Then we discussed how to convert a Bayesian network into a Markov network. The loops in the &lt;em&gt;moral graph&lt;/em&gt; incurred by the conversion can be avoided by transforming the graph into a &lt;em&gt;factor graph&lt;/em&gt;.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Machine Learning - 07 Exponential Family</title>
   <link href="/2020/11/12/exponential_family-ml07/"/>
   <updated>2020-11-12T00:00:00+08:00</updated>
   <id>/2020/11/12/exponential_family-ml07</id>
   <content type="html">&lt;p&gt;&lt;em&gt;The notes are based on the &lt;a href=&quot;https://github.com/shuhuai007/Machine-Learning-Session&quot;&gt;session&lt;/a&gt; and the &lt;a href=&quot;https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter8.pdf&quot;&gt;material&lt;/a&gt;. For the fundamental of linear algebra, one can always refer to &lt;a href=&quot;http://math.mit.edu/~gs/linearalgebra/&quot;&gt;Introduction to Linear Algebra&lt;/a&gt; and &lt;a href=&quot;https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf&quot;&gt;The Matrix Cookbook&lt;/a&gt; for more details. Many thanks to these great works.&lt;/em&gt;&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#0-introduction&quot; id=&quot;markdown-toc-0-introduction&quot;&gt;0. Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#1-exponential-family&quot; id=&quot;markdown-toc-1-exponential-family&quot;&gt;1. Exponential Family&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-sufficient-statistic&quot; id=&quot;markdown-toc-2-sufficient-statistic&quot;&gt;2. Sufficient Statistic&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3-log-partition-function&quot; id=&quot;markdown-toc-3-log-partition-function&quot;&gt;3. Log-partition Function&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#4-maximum-entropy&quot; id=&quot;markdown-toc-4-maximum-entropy&quot;&gt;4. Maximum Entropy&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#5-gaussian-distribution&quot; id=&quot;markdown-toc-5-gaussian-distribution&quot;&gt;5. Gaussian Distribution&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#6-conclusion&quot; id=&quot;markdown-toc-6-conclusion&quot;&gt;6. Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;0-introduction&quot;&gt;0. Introduction&lt;/h1&gt;

&lt;p&gt;An exponential family is a family of distributions which share some properties in common. The real message of this note is the simplicity and elegance of the exponential family. Once the ideas are mastered, it is often easier to work within the general exponential family framework than with specific instances.&lt;/p&gt;

&lt;h1 id=&quot;1-exponential-family&quot;&gt;1. Exponential Family&lt;/h1&gt;

&lt;p&gt;Given one real-vector parameter \(\mathbf{\theta}=[\theta_1,\theta_2,\dots,\theta_d]^T\), we define an &lt;em&gt;exponential family&lt;/em&gt; of probability distributions as those distributions whose density have the following general form:&lt;/p&gt;

\[f_X(x\vert\eta)=h(x)\exp\left(\eta^T T(x)-A(\eta)\right).\]

&lt;p&gt;&lt;strong&gt;Canonical parameter&lt;/strong&gt;: $\eta$ is called &lt;em&gt;canonical&lt;/em&gt;, or &lt;em&gt;natural parameter&lt;/em&gt; (function), which can be viewed as a transformation of $\mathcal{\theta}$. The set of values of $\eta$ is always convex.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sufficient statistic&lt;/strong&gt;: Given a data set sampled from \(f_X(x\vert\eta)\), the sufficient statistic $T(x)$ is a function of the data that holds all information the data set provides with regard to the unknown parameter $\mathbf{\theta}$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Log-partition function&lt;/strong&gt;: $A(\eta)$ is the &lt;em&gt;log-partition function&lt;/em&gt; to normalize \(f_X(x\vert \eta)\) to be a probability distribution,&lt;/p&gt;

\[A(\eta)=\log\left(\int_{X}h(x)\exp(\eta^T T(x))\text{d}x\right).\]

&lt;p&gt;In the following sections, we will discuss them detailedly.&lt;/p&gt;

&lt;h1 id=&quot;2-sufficient-statistic&quot;&gt;2. Sufficient Statistic&lt;/h1&gt;

&lt;p&gt;Consider the problem of estimating the unknown parameters by &lt;em&gt;maximum likelihood estimation&lt;/em&gt; (MLE) in exponential family cases. Specifically, for an &lt;em&gt;i.i.d.&lt;/em&gt; data set \(\mathcal{D}=\{x_1,x_2,\dots,x_N\}\), we have the log likelihood&lt;/p&gt;

\[\mathcal{L}(\eta\vert\mathcal{D})=\log\left(\prod_{i=1}^Nh(x_i)\right)+\eta^T\left(\sum_{i=1}^NT(x_i)\right)-NA(\eta).\]

&lt;p&gt;By &lt;em&gt;MLE&lt;/em&gt;, we have the estimation $\hat\eta$ when its gradient with respect to $\eta$ is zero:&lt;/p&gt;

\[\mathcal{L}’(\eta\vert\mathcal{D})=\sum_{i=1}^NT(x_i)-NA’(\eta)=0.\]

&lt;p&gt;Solving the equation, we have&lt;/p&gt;

\[A’(\hat\eta)=\frac{1}{N}\sum_{i=1}^NT(x_i),\]

&lt;p&gt;which is the general formula of MLE for the parameters in the exponential family. Further, notice that our formula involves the data only via the sufficient statistic $T(x_i)$. This gives the operational meaning to &lt;em&gt;sufficiency&lt;/em&gt;—for the purpose of estimating parameters we retain only the sufficient statistic.&lt;/p&gt;

&lt;h1 id=&quot;3-log-partition-function&quot;&gt;3. Log-partition Function&lt;/h1&gt;

&lt;p&gt;As we mentioned in section 1, $A(\eta)$ can be viewed as a normalization factor. In fact, $A(\eta)$ is not a degree of freedom in the specification of an exponential family density; it is determined once $T(x)$ and $h(x)$ are determined. The relation between $A(\eta)$ and $T(x)$ can be further characterized by&lt;/p&gt;

\[\begin{aligned}A’(\eta)&amp;amp;=\frac{\text{d}\log\left(\int_{X}h(x)\exp(\eta^T T(x))\text{d}x\right)}{\text{d}\eta}\\&amp;amp;=\frac{\int_{X}h(x)\exp(\eta^T T(x))\cdot  T(x)\text{d}x}{\int_{X}h(x)\exp(\eta^T T(x))\text{d}x}\\&amp;amp;=\frac{\int_{X}h(x)\exp(\eta^T T(x))\cdot T(x)\text{d}x}{\exp({A(\mathbf{\theta})})}\\&amp;amp;=\int_{X}\underbrace{h(x)\exp(\eta^T T(x)-A(\eta))}_{f_X(x\vert \eta)}\cdot T(x)\text{d}x\\&amp;amp;=\mathbb{E}_{f_X(x\vert\eta)}[T(x)].\end{aligned}\]

&lt;p&gt;Further, we have&lt;/p&gt;

\[\begin{aligned}A’’(\eta)&amp;amp;=\int_{X}f_X(x\vert \eta)\cdot(T(x)-A’(\eta)) T(x)\text{d}x\\&amp;amp;=\int_{X}f_X(x\vert \eta)\cdot(T(x))^2\text{d}x-A’(\eta)\int_{X}f_X(x\vert \mathbf{\eta})\cdot T(x)\text{d}x\\&amp;amp;=\mathbb{E}_{f_X(x\vert\eta)}[(T(x))^2]-\left(\mathbb{E}_{f_X(x\vert\eta)}[T(x)]\right)^2\\&amp;amp;=var[T(x)],\end{aligned}\]

&lt;p&gt;which also shows that $A(\eta)$ is convex as $var[T(x)]\ge 0$.&lt;/p&gt;

&lt;h1 id=&quot;4-maximum-entropy&quot;&gt;4. Maximum Entropy&lt;/h1&gt;

&lt;p&gt;The entropy of $P$ with distribution $p(x)$ supported on $X$ is&lt;/p&gt;

\[H(P)=\mathbb{E}_{P}[-\log p(x)].\]

&lt;p&gt;The &lt;em&gt;maximum entropy&lt;/em&gt; principle is that: given some constraints (prior information) about the distribution $P$, we consider all probability distributions satisfying said constraints such that the constraints are being utilized as &lt;em&gt;objective&lt;/em&gt; as possible, &lt;em&gt;i.e.,&lt;/em&gt; be as uncertain as possible.&lt;/p&gt;

&lt;p&gt;For example, consider the case where the very constraint is $\sum_Xp(x)=1$, which formulates&lt;/p&gt;

\[\begin{aligned}\max&amp;amp;\quad H(P)\\\text{s.t.}&amp;amp;\quad \sum_{X}p(x)=1.\end{aligned}\]

&lt;p&gt;By the definition we have&lt;/p&gt;

\[H(P)=-\sum_{i=1}^{\vert X\vert}p(x_i)\log p(x_i).\]

&lt;p&gt;Then the &lt;em&gt;Lagrangian&lt;/em&gt; for the optimization problem is&lt;/p&gt;

\[L(P,\lambda)=\sum_{i=1}^{\vert X\vert}p(x_i)\log p(x_i)+\lambda\left(1-\sum_{i=1}^{\vert X\vert}p(x_i)\right).\]

&lt;p&gt;Setting the first derivation of the Lagrangian to be zero yields&lt;/p&gt;

\[\frac{\partial L}{\partial p(x_i)}=0\implies \hat{p}(x_i)=\exp(\lambda-1),\]

&lt;p&gt;which gives that&lt;/p&gt;

\[\hat{p}(x_1)=\hat{p}(x_2)=\dots=\hat{p}(x_{\vert X\vert})=\frac{1}{\vert X\vert},\]

&lt;p&gt;&lt;em&gt;i.e.,&lt;/em&gt; the distribution with maximum entropy is &lt;em&gt;uniform distribution&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;We now consider a general case where $p(x)$ is continuous with a general constraint $\mathbb{E}_P[\Phi(x)]=\alpha$, where $\Phi(x)=[\phi_1(x),\phi_2(x),\dots,\phi_d(x)]\in\mathbb{R}^d$ and $\alpha=[\alpha_1,\alpha_2,\dots,\alpha_d]\in\mathbb{R}^d$, which formulates&lt;/p&gt;

\[\begin{aligned}\max&amp;amp;\quad H(P)\\\text{s.t.}&amp;amp;\quad \mathbb{E}_P[\Phi(x)]=\alpha\\\implies\min&amp;amp;\quad \int_Xp(x)\log p(x)\text{d}x\\\text{s.t.}&amp;amp;\quad \int_X p(x)\phi_i(x)\text{d}x=\alpha_i,\ i=1,2,\dots, d,\\&amp;amp;\quad \int_X p(x)\text{d}x=1.\end{aligned}\]

&lt;p&gt;Similarly, we obtain the Lagrangian as&lt;/p&gt;

\[L(P,\theta,\lambda)=\int_X p(x)\log p(x)\text{d}x+\sum_{i=1}^d\theta_i\left(\alpha_i-\int_X p(x)\phi_i(x)\text{d}x\right)+\lambda\left(\int_X p(x)\text{d}x-1\right).\]

&lt;p&gt;By treating the density $P=[p(x)]_{x\in X}$ as a finite vector such that $\int_X p(x)\text{d}x$ is similar to $\sum_X p(x)$, we have&lt;/p&gt;

\[\begin{aligned}\frac{\partial L}{\partial p(x)}&amp;amp;=\frac{\partial }{\partial p(x)}\left(\sum_X p(x)\log p(x)-\sum_{i=1}^d\theta_i\sum_X p(x)\phi_i(x)+\lambda\sum_X p(x)\right)\\&amp;amp;=1+\log p(x)-\sum_{i=1}^d\theta_i\phi_i(x)+\lambda\\&amp;amp;=1+\log p(x)-\theta^T\Phi(x)+\lambda.\end{aligned}\]

&lt;p&gt;Setting the derivation to be zero for all $x$, we have&lt;/p&gt;

\[p(x)=\exp\left\{\theta^T\Phi(x)-(\lambda+1)\right\},\]

&lt;p&gt;which is in the exponential family form with&lt;/p&gt;

\[\begin{aligned}\eta&amp;amp;=\theta,\\T(x)&amp;amp;=\Phi(x),\\A(\eta)&amp;amp;=\lambda+1,\\h(x)&amp;amp;=1.\end{aligned}\]

&lt;h1 id=&quot;5-gaussian-distribution&quot;&gt;5. Gaussian Distribution&lt;/h1&gt;

&lt;p&gt;In this section, we consider an example, Gaussian distribution, which is of the exponential family and exemplifies the properties we mentioned above.&lt;/p&gt;

&lt;p&gt;We first rewritten the PDF of one-dimension Gaussian distribution to show it is in the exponential family .&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; Given unknown parameter \(\mathbf{\theta}=[\mu,\sigma^2]\), the Gaussian density can be written as follows,&lt;/p&gt;

\[\begin{aligned}f_X(x\vert \mathbf{\theta})&amp;amp;=\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{-\frac{1}{2\sigma^2}(x-\mu)^2\right\}\\&amp;amp;=\frac{1}{\sqrt{2\pi}}\exp\left\{\frac{\mu}{\sigma^2}x-\frac{1}{2\sigma^2}x^2-\frac{1}{2\sigma^2}\mu^2-\log\sigma\right\}\\&amp;amp;=\frac{1}{\sqrt{2\pi}}\exp\left\{\begin{bmatrix}\frac{\mu}{\sigma^2}&amp;amp;-\frac{1}{2}\sigma^2\end{bmatrix}\begin{bmatrix}x\\x^2\end{bmatrix}-\left(\frac{\mu^2}{2\sigma^2}+\log\sigma\right)\right\},\end{aligned}\]

&lt;p&gt;which is in the exponential family form with&lt;/p&gt;

\[\begin{aligned}\eta&amp;amp;=\begin{bmatrix}\frac{\mu}{\sigma^2}&amp;amp;-\frac{1}{2\sigma^2}\end{bmatrix}^T,\\T(x)&amp;amp;=\begin{bmatrix}x&amp;amp;x^2\end{bmatrix}^T,\\A(\eta)&amp;amp;=\frac{\mu^2}{2\sigma^2}+\log\sigma=-\frac{\eta_1^2}{4\eta_2}-\frac{1}{2}\log(-2\eta_2),\\h(x)&amp;amp;=\frac{1}{\sqrt{2\pi}}.\end{aligned}\]

\[\tag*{$\blacksquare$}\]

&lt;p&gt;Then we verify the relation between the sufficient statistic and MLE method.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; Given a data set \(\mathcal{D}=\{x_1,x_2,\dots,x_N\}\), as we mentioned in section 2, we can derive the parameters via the sufficient statistic as follows,&lt;/p&gt;

\[\begin{cases}A’(\eta)=\frac{1}{N}\sum_{i=1}^NT(x)\implies\begin{cases}A’(\hat\eta_1)=\frac{1}{N}\sum_{i=1}^N x_i\\A’(\hat\eta_2)=\frac{1}{N}\sum_{i=1}^Nx_i^2\end{cases}\\A(\eta)=-\frac{\eta_1^2}{4\eta_2}-\frac{1}{2}\log(-2\eta_2)\implies\begin{cases}A’(\hat\eta_1)=-\frac{\eta_1}{2\eta_2}=\hat\mu\\A’(\hat\eta_2)=\frac{\eta_1^2}{4\eta_2^2}-\frac{1}{2\eta_2}=\hat\sigma^2+\hat\mu^2\end{cases}\end{cases}\]

&lt;p&gt;Solving the equations we have&lt;/p&gt;

\[\hat\mu=\frac{1}{N}\sum_{i=1}^Nx_i,\quad \hat\sigma=\frac{1}{N}\sum_{i=1}^Nx_i^2-\hat\mu^2,\]

&lt;p&gt;which is consistent with the result in the &lt;a href=&quot;https://2ez4ai.github.io/2020/09/28/intro-ml01/&quot;&gt;post&lt;/a&gt;. $\tag*{$\blacksquare$}$&lt;/p&gt;

&lt;p&gt;Now we show that&lt;/p&gt;

\[A’’(\hat\eta)=var[T(x)].\]

&lt;p&gt;&lt;em&gt;Proof&lt;/em&gt;: Firstly, we have&lt;/p&gt;

\[\begin{cases}A’’(\hat\eta_1)=-\frac{1}{2\eta_2}=\sigma^2\\A’’(\hat\eta_2)=-\frac{\eta_1^2}{2\eta_2^3}+\frac{1}{2\eta_2^2}=4\sigma^2\mu^2+2\sigma^4\end{cases}\]

&lt;p&gt;For \(T(x)=\begin{bmatrix}x&amp;amp;x^2\end{bmatrix}^T\), we have&lt;/p&gt;

\[var[x]=\sigma^2, \text{ as }x\sim\mathcal{N}(\mu,\sigma^2),\]

&lt;p&gt;and&lt;/p&gt;

\[var[x^2]=\mathbb{E}[x^4]-\left(\mathbb{E}[x^2]\right)^2.\]

&lt;p&gt;For $\mathbb{E}[x^2]$, it follows that&lt;/p&gt;

\[\mathbb{E}[x^2]=var[x]+(\mathbb{E}[x])^2=\sigma^2+\mu^2.\]

&lt;p&gt;For $\mathbb{E}[x^4]$, to compute it we leverage &lt;em&gt;moment generating functions&lt;/em&gt; which follows that&lt;/p&gt;

\[M_X(t)=e^{\mu t+\frac{1}{2}\sigma^2t^2},\quad \mathbb{E}[x^4]=M^{(4)}_X(0).\]

&lt;p&gt;After a laborious computing, we have&lt;/p&gt;

\[\begin{aligned}var[x^2]&amp;amp;=\mathbb{E}[x^4]-\left(\mathbb{E}[x^2]\right)^2\\&amp;amp;=3\sigma^4+6\sigma^2\mu^2+\mu^4-\sigma^4-2\sigma^2-\mu^4\\&amp;amp;=4\sigma^2\mu^2+2\sigma^4.\end{aligned}\]

&lt;p&gt;Therefore, we have&lt;/p&gt;

\[A’’(\hat\eta)=\begin{bmatrix}var[x]\\var[x^2]\end{bmatrix}.\tag*{$\blacksquare$}\]

&lt;p&gt;Finally, we show that $X\sim\mathcal{N}(\mu,\sigma^2)$ is the distribution that maximizes the entropy over all distributions $P$ satisfying&lt;/p&gt;

\[\mathbb{E}_P\left[\left(\frac{X-\mu}{\sigma}\right)^2\right]=1.\]

&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; Consider the expression we formulated in section 4,&lt;/p&gt;

\[p(x)=\exp\left\{\theta^T\Phi(x)-(\lambda+1)\right\},\]

&lt;p&gt;which maximizes the entropy while satisfying $\mathbb{E}_P[\Phi(x)]=\alpha$. Now letting&lt;/p&gt;

\[\begin{aligned}\alpha&amp;amp;=1,\\\Phi(x)&amp;amp;=\frac{(x-\mu)^2}{\sigma^2},\\\theta&amp;amp;=-\frac{1}{2},\\\exp\{-\lambda-1\}&amp;amp;=\frac{1}{\sqrt{2\pi}\sigma}.\end{aligned}\]

&lt;p&gt;Therefore we have&lt;/p&gt;

\[p(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{-\frac{1}{2\sigma^2}(x-\mu)^2\right\}.\tag*{$\blacksquare$}\]

&lt;h1 id=&quot;6-conclusion&quot;&gt;6. Conclusion&lt;/h1&gt;

&lt;p&gt;In this post, we briefly introduced the basic form of the exponential family. Then we discussed its properties from three perspectives: sufficient statistic, log-partition function and maximum entropy. Moreover, with one-dimension Gaussian distribution, we exemplified the properties.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Machine Learning - 06 Kernel Method</title>
   <link href="/2020/11/05/kernel_method-ml06/"/>
   <updated>2020-11-05T00:00:00+08:00</updated>
   <id>/2020/11/05/kernel_method-ml06</id>
   <content type="html">&lt;p&gt;&lt;em&gt;The notes are based on the &lt;a href=&quot;https://github.com/shuhuai007/Machine-Learning-Session&quot;&gt;session&lt;/a&gt;. For the fundamental of linear algebra, one can always refer to &lt;a href=&quot;http://math.mit.edu/~gs/linearalgebra/&quot;&gt;Introduction to Linear Algebra&lt;/a&gt; and &lt;a href=&quot;https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf&quot;&gt;The Matrix Cookbook&lt;/a&gt; for more details. Many thanks to these great works.&lt;/em&gt;&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#0-introduction&quot; id=&quot;markdown-toc-0-introduction&quot;&gt;0. Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#1-kernel-method&quot; id=&quot;markdown-toc-1-kernel-method&quot;&gt;1. Kernel method&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-kernel-function&quot; id=&quot;markdown-toc-2-kernel-function&quot;&gt;2. Kernel function&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3-conclusion&quot; id=&quot;markdown-toc-3-conclusion&quot;&gt;3. Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;0-introduction&quot;&gt;0. Introduction&lt;/h1&gt;

&lt;p&gt;Kernel methods are a class of algorithms for pattern analysis. The name of kernel methods comes from the use of &lt;em&gt;kernel function&lt;/em&gt;, which enable operations in a high-dimensional and implicit space. Specifically, by &lt;a href=&quot;https://en.wikipedia.org/wiki/Cover%27s_theorem&quot;&gt;Cover’s theorem&lt;/a&gt;, given a set of training data that is not &lt;em&gt;linearly separable&lt;/em&gt;, one can with high probability transform it into a training set that is linearly separable by projecting it into a higher-dimensional space via some &lt;em&gt;non-linear transformation&lt;/em&gt;. With the help of kernel function, the operation, &lt;em&gt;i.e.,&lt;/em&gt; inner product, it involves after transforming can be often computationally cheaper than the explicit computation. Such an approach is called the &lt;em&gt;kernel trick&lt;/em&gt;. In this post, we will focus on the application of kernel method to SVM.&lt;/p&gt;

&lt;h1 id=&quot;1-kernel-method&quot;&gt;1. Kernel method&lt;/h1&gt;

&lt;p&gt;Define the data set as \(\mathcal{D}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}, X=\{x_1,x_2,\dots,x_N\}\) and \(Y=\{y_1,y_2,\dots,y_N\}\) where $x_i\in\mathbb{R}^{d\times 1}$ and \(y_i\in\{-1,1\}\). We further assume that the data set is non-linearly separable. Kernel method supposes that there is a non-linear transformation $\phi(x):\mathbb{R}^{d\times 1}\to\mathbb{R}^{p\times 1},d&amp;lt;p,$  such that \(\mathcal{D}_p=\{(\phi(x_1),y_1),(\phi(x_2),y_2),\dots,(\phi(x_N),y_N)\}\) are linearly separable. For such a linearly separable data set, recalling the problem we formulated in section 1.3 of &lt;a href=&quot;https://2ez4ai.github.io/2020/10/28/support_vector_machine-ml05/&quot;&gt;SVM&lt;/a&gt;, we have the duality problem&lt;/p&gt;

\[\begin{aligned}\min_\lambda&amp;amp;\quad \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^N\left(\lambda_i\lambda_jy_iy_j\phi^T(x_i)\phi(x_j)\right)-\sum_{i=1}^N\lambda_i\\\text{s.t.}&amp;amp;\quad \lambda_i\ge0,i=1,2,\dots,N\\&amp;amp;\quad \sum_{i=1}^N\lambda_iy_i=0.\end{aligned}\]

&lt;p&gt;However, after transforming, the inner product $\phi(x_i)^T\phi(x_j)=\langle\phi(x_i),\phi(x_j)\rangle$ could be hard to obtain (consider the case that $\phi(\cdot)$ has infinite dimensions), which requires the aid of &lt;em&gt;kernel function&lt;/em&gt;.&lt;/p&gt;

&lt;h1 id=&quot;2-kernel-function&quot;&gt;2. Kernel function&lt;/h1&gt;

&lt;p&gt;A kernel function is defined as $K:\mathbb{R}^{d\times 1}\times\mathbb{R}^{d\times 1}\to\mathbb{R}$. Specifically, for non-linear transformation $\phi(\cdot)\in\mathcal{H}\text{ (Hilbert space) }:\mathbb{R}^{d\times 1}\to\mathbb{R}^{p\times 1}$ and any $x_i,x_j\in\mathbb{R}^{d\times 1}$, we call $K(x_i,x_j)=\langle\phi(x_i),\phi(x_j)\rangle$ a kernel function. Such a kernel function is regarded &lt;em&gt;positive definite&lt;/em&gt;, which satisfies&lt;/p&gt;

\[K(x_i,x_j)=K(x_j,x_i),\]

&lt;p&gt;and for \(x_{1},x_{2},\dots,x_{N}\in\mathbb{R}^{d\times 1},\)&lt;/p&gt;

\[\mathcal{K}=[K(x_{i},x_{j})]_{N\times N}\text{ is a positive semi-definite (PSD) matrix},\]

&lt;p&gt;where \(\mathcal{K}\) is called &lt;em&gt;Gram matrix&lt;/em&gt; of $K$ over set \(\{x_{1},x_{2},\dots,x_{N}\}\). When the explicit expression of $\phi(\cdot)$ is hard to be determined, it quite often to show the positive definiteness of a kernel function via its corresponding Gram matrix.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(Properties)&lt;/strong&gt; We now show two &lt;em&gt;properties&lt;/em&gt; of kernel functions.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Let $K$ be kernel function such that \(K:\mathbb{R}^{d\times 1}\times\mathbb{R}^{d\times 1}\to\mathbb{R}\), then we define its Gram matrix $\mathcal{K}\in\mathbb{R}^{N\times N}$ over \(\{x_1,x_2,\dots,x_N\}\) where $x_i\in\mathbb{R}^{d\times 1}$. Considering the mapping function $\phi(\cdot):\mathbb{R}^{d\times 1}\to\mathbb{R}^{p\times 1}$, we have&lt;/li&gt;
&lt;/ul&gt;

\[K(x_i,x_j)=\langle\phi(x_i),\phi(x_j)\rangle, \phi(\cdot)\in\mathcal{H}\implies \begin{cases}K(x_i,x_j)=K(x_j,x_i)\\ \mathcal{K}\text{ is a PSD matrix}\end{cases}.\]

&lt;p&gt;&lt;em&gt;Proof&lt;/em&gt;:&lt;/p&gt;

&lt;p&gt;By the definition of $K(x_i,x_j)$, we have&lt;/p&gt;

\[K(x_i,x_j)=\langle \phi(x_i),\phi(x_j)\rangle,\quad K(x_j,x_i)=\langle \phi(x_j),\phi(x_i)\rangle.\]

&lt;p&gt;By the symmetry of inner product, we have \(\langle \phi(x_i),\phi(x_j)\rangle=\langle \phi(x_j),\phi(x_i)\rangle\). It then follows that&lt;/p&gt;

\[K(x_i,x_j)=K(x_j,x_i).\]

&lt;p&gt;Therefore the Gramian matrix $\mathcal{K}=[K(x_{i},x_{j})]_{N\times N}$ is symmetric real matrix. Now we show that \(\forall\alpha\in\mathbb{R}^{R\times 1}, \alpha^T\mathcal{K}\alpha\ge 0.\) The notation is given by&lt;/p&gt;

\[\begin{aligned}\alpha^T\mathcal{K}\alpha=(\alpha_1,\alpha_2,\dots,\alpha_N)\begin{bmatrix}K_{11}&amp;amp;K_{12}&amp;amp;\dots&amp;amp;K_{1N}\\K_{21}&amp;amp;K_{22}&amp;amp;\dots&amp;amp;K_{2N}\\\vdots&amp;amp;\vdots&amp;amp;\ddots&amp;amp;\vdots\\K_{N1}&amp;amp;K_{N2}&amp;amp;\dots&amp;amp;K_{NN}\end{bmatrix}\begin{pmatrix}\alpha_1\\\alpha_2\\\vdots\\\alpha_N\end{pmatrix}\end{aligned},\]

&lt;p&gt;where $K_{ij}=K(x_{ri},x_{rj})$. We then have&lt;/p&gt;

\[\begin{aligned}\alpha^T\mathcal{K}\alpha&amp;amp;=\sum_{i=1}^R\sum_{j=1}^R \alpha_i\alpha_jK_{ij}\\&amp;amp;=\sum_{i=1}^R\sum_{j=1}^R \alpha_i\alpha_j\phi^T(x_{ri})\phi(x_{rj})\\&amp;amp;=\sum_{i=1}^R \alpha_i\phi^T(x_{ri})\sum_{j=1}^R\alpha_j\phi(x_{rj})\\&amp;amp;=\left(\sum_{i=1}^R \alpha_i\phi(x_{ri})\right)^T\left(\sum_{j=1}^R\alpha_j\phi(x_{rj})\right)\\&amp;amp;=\left\langle\left(\sum_{i=1}^R \alpha_i\phi(x_{ri})\right), \left(\sum_{j=1}^R \alpha_i\phi(x_{rj})\right)\right\rangle\\&amp;amp;=\left\vert\left\vert\sum_{i=1}^R \alpha_i\phi(x_{ri})\right\vert\right\vert^2,\end{aligned}\]

&lt;p&gt;therefore, $\alpha^T\mathcal{K}\alpha\ge 0$ and $\mathcal{K}$ is a PSD matrix.$\tag*{$\blacksquare$}$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Let $\mathcal{K}\in\mathbb{R}^{d\times d}$ be a symmetric PSD matrix, then for \(\{x_1,x_2,\dots,x_N\}\) where $x_i\in\mathbb{R}^{d\times 1}$, we have kernel function $K(x_i,x_j)=x_i^T\mathcal{K}x_j$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Proof&lt;/em&gt;:&lt;/p&gt;

&lt;p&gt;Consider the &lt;em&gt;diagonalisation&lt;/em&gt; of $\mathcal{K}=Q^T\Lambda Q$ by an orthogonal matrix $Q$, where $\Lambda$ is a diagnoal matrix containing the non-negative eigenvalues of $\mathcal{K}$. Let $\sqrt{\Lambda}$ be the diagonal matrix with the square roots of the eigenvalues and set $A=\sqrt{\Lambda}Q$.  Then for \(\{x_1,x_2,\dots,x_N\}\) where $x_i\in\mathbb{R}^{d\times 1}$, we have&lt;/p&gt;

\[x_i^T\mathcal{K}x_j=x_i^TQ^T\Lambda Qx_j=x_i^TA^TA x_j=\langle A x_i,Ax_j\rangle.\]

&lt;p&gt;Therefore we have kernel function $K(x_i,x_j)=x_i^T\mathcal{K}x_j=\langle Ax_i,Ax_j\rangle$ with linear transformation $\phi(\cdot)=A\cdot. \tag*{$\blacksquare$}$&lt;/p&gt;

&lt;h1 id=&quot;3-conclusion&quot;&gt;3. Conclusion&lt;/h1&gt;

&lt;p&gt;In this post, we introduced &lt;em&gt;kernel method&lt;/em&gt; for classification problem. Given &lt;em&gt;Cover’s theorem&lt;/em&gt;, we can project non-linear data into high-dimensional space and obtain linearly separable data. To simplify the computation incurred by the duality problem, we can leverage &lt;em&gt;kernel function&lt;/em&gt; to avoid the computing labor.&lt;/p&gt;

&lt;p&gt;This is definitely not a good introduction to kernel methods. For more details of kernel method, I would recommend &lt;a href=&quot;https://people.eecs.berkeley.edu/~jordan/kernels/0521813972c03_p47-84.pdf&quot;&gt;Kernel methods: an overview&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Machine Learning - 05 Support Vector Machine</title>
   <link href="/2020/10/28/support_vector_machine-ml05/"/>
   <updated>2020-10-28T00:00:00+08:00</updated>
   <id>/2020/10/28/support_vector_machine-ml05</id>
   <content type="html">&lt;p&gt;&lt;em&gt;The notes are based on the &lt;a href=&quot;https://github.com/shuhuai007/Machine-Learning-Session&quot;&gt;session&lt;/a&gt;. For the fundamental of linear algebra, one can always refer to &lt;a href=&quot;http://math.mit.edu/~gs/linearalgebra/&quot;&gt;Introduction to Linear Algebra&lt;/a&gt; and &lt;a href=&quot;https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf&quot;&gt;The Matrix Cookbook&lt;/a&gt; for more details. Many thanks to these great works.&lt;/em&gt;&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#0-introduction&quot; id=&quot;markdown-toc-0-introduction&quot;&gt;0. Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#1-hard-margin-svm&quot; id=&quot;markdown-toc-1-hard-margin-svm&quot;&gt;1. Hard-margin SVM&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#11-problem-formulation&quot; id=&quot;markdown-toc-11-problem-formulation&quot;&gt;1.1. Problem Formulation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#12-lagrange-duality&quot; id=&quot;markdown-toc-12-lagrange-duality&quot;&gt;1.2. Lagrange Duality&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#13-karushkuhntucker-conditions&quot; id=&quot;markdown-toc-13-karushkuhntucker-conditions&quot;&gt;1.3. Karush–Kuhn–Tucker Conditions&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-soft-margin-svm&quot; id=&quot;markdown-toc-2-soft-margin-svm&quot;&gt;2. Soft-margin SVM&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3-conclusion&quot; id=&quot;markdown-toc-3-conclusion&quot;&gt;3. Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;0-introduction&quot;&gt;0. Introduction&lt;/h1&gt;

&lt;p&gt;Support vector machine (SVM) is a supervised learning method for classification and regression analysis. It is one of the most robust prediction method. Here we mainly consider its applications in classification. Specifically, for the data of $d$-dimensional, we want to know whether we can separate classes with a $(d-1)$-dimensional &lt;em&gt;hyperplane&lt;/em&gt;. In particular, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class. According to whether the dataset is linearly separable or not, there are &lt;em&gt;hard-margin&lt;/em&gt; SVM, &lt;em&gt;soft-margin&lt;/em&gt; SVM and &lt;em&gt;kernel&lt;/em&gt; SVM.&lt;/p&gt;

&lt;h1 id=&quot;1-hard-margin-svm&quot;&gt;1. Hard-margin SVM&lt;/h1&gt;

&lt;p&gt;Hard-margin SVM works only when data is completely linearly separable without any errors.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/300px-SVM_margin.png&quot; width=&quot;350&quot; /&gt;
&lt;/div&gt;
&lt;h2 id=&quot;11-problem-formulation&quot;&gt;1.1. Problem Formulation&lt;/h2&gt;

&lt;p&gt;Suppose we have data set \(\mathcal{D}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}\) where $x_i\in\mathbb{R}^{d\times 1}$ is the data feature and $y_i\in{-1,1}$ is the corresponding class label. A case where $d=2$ is shown in the figure above. The hyperplane that separates the data is defined as&lt;/p&gt;

\[w^Tx-b=0,\]

&lt;p&gt;where $w\in\mathbb{R}^{d\times 1}$ and $b\in\mathbb{R}$ are parameters to be learned. Then like what we arrived in &lt;a href=&quot;https://2ez4ai.github.io/2020/10/14/linear_classification-ml03/&quot;&gt;perceptron&lt;/a&gt;, a correct classifier should ensure that&lt;/p&gt;

\[y_i(w^Tx_i-b)&amp;gt;0,\forall i=1,2,\dots,N.\]

&lt;p&gt;We further define the &lt;em&gt;margin&lt;/em&gt; as the parallel lines that has the minimum distance from the data to the hyperplane. In &lt;em&gt;hard-margin&lt;/em&gt; SVM, we need to find the &lt;em&gt;maximum-margin&lt;/em&gt; hyperplane that maximizes the distance, which can be described by&lt;/p&gt;

\[\begin{aligned}\max_{w,b}\min_{i}&amp;amp;\quad \frac{\vert w^Tx_i-b\vert}{\vert\vert w\vert\vert}\\\text{s.t.}&amp;amp;\quad y_i(w^Tx_i-b)&amp;gt;0,\forall i=1,2,\dots,N\end{aligned}.\]

&lt;p&gt;The problem further is equivalent to&lt;/p&gt;

\[\begin{alignat*}{3}\max_{w,b}\min_{i}&amp;amp;\quad \frac{y_i(w^Tx_i-b)}{\vert\vert w\vert\vert}\implies\max_{w,b}\frac{1}{\vert\vert w\vert\vert}\min_i&amp;amp;\quad y_i(w^Tx_i-b)\\\text{s.t.}&amp;amp;\quad y_i(w^Tx_i-b)&amp;gt;0,\forall i=1,2,\dots,N\end{alignat*}.\]

&lt;p&gt;For the constraint $y_i(w^Tx_i-b)&amp;gt;0, \forall i=1,2,\dots,N$, there exists a positive parameter $r&amp;gt;0$ such that&lt;/p&gt;

\[\min_i y_i(w^Tx_i-b)=r.\]

&lt;p&gt;As there are many $w,b$ available for the separation as long as they have the same directions. We add a new constraint that $r=1$. Then it follows that&lt;/p&gt;

\[\min_i y_i(w^Tx_i-b)=y_i((w_{\text{old}}^T/r)x_i-b_{\text{old}}/r)=1.\]

&lt;p&gt;The problem then is transformed into&lt;/p&gt;

\[\begin{alignat*}{3}&amp;amp;&amp;amp;\max_{w,b}\frac{1}{\vert\vert w\vert\vert}\min_i y_i(w^Tx_i-b)&amp;amp;\implies\min_{w,b}\quad \frac{1}{2}w^Tw\\&amp;amp;&amp;amp;\text{s.t.}\quad y_i(w^Tx_i-b)&amp;gt;0&amp;amp;\implies\text{s.t.}\quad y_i(w^Tx_i-b)\ge 1,i=1,2,\dots,N\end{alignat*},\]

&lt;p&gt;which is a linearly constrained &lt;em&gt;quadratic optimization&lt;/em&gt; (QP) problem.&lt;/p&gt;

&lt;h2 id=&quot;12-lagrange-duality&quot;&gt;1.2. Lagrange Duality&lt;/h2&gt;

&lt;p&gt;The following content is about &lt;a href=&quot;https://web.stanford.edu/~boyd/cvxbook/&quot;&gt;convex optimization&lt;/a&gt;. In section 1, we have the &lt;em&gt;primal problem&lt;/em&gt;&lt;/p&gt;

\[\begin{aligned}\min_{w,b}&amp;amp;\quad \frac{1}{2}w^Tw\\\text{s.t.}&amp;amp;\quad y_i(w^Tx_i-b)\ge 1,i=1,2,\dots,N\end{aligned}.\]

&lt;p&gt;The &lt;em&gt;Lagrangian&lt;/em&gt; for the problem is a function defined as&lt;/p&gt;

\[L(w,b,\lambda)=\frac{1}{2}w^Tw+\sum_{i=1}^N\lambda_i\left(1-y_i\left(w^Tx_i-b\right)\right),\]

&lt;p&gt;where $\lambda_i\ge 0$ is the &lt;em&gt;Lagrange multiplier&lt;/em&gt; associated with the constraints. Consider the problem of $\max_\lambda L(w,b,\lambda)$,&lt;/p&gt;

\[\max_{\lambda\ge 0} L(w,b,\lambda)=\begin{cases}\frac{1}{2}w^Tw+\infty&amp;amp;\text{if }\exists i\in\{1,2,\dots,N\}\text{ s.t. }1-y_i(w^Tx_i-b)&amp;gt;0,\\\frac{1}{2}w^Tw+0&amp;amp;\text{otherwise.}\end{cases}\]

&lt;p&gt;The problem makes sense (non infinity) only when the original constraint is satisfied. In that case, the primal problem is equivalent to&lt;/p&gt;

\[\begin{aligned}\min_{w,b}\max_\lambda&amp;amp;\quad L(w,b,\lambda)\\\text{s.t.}&amp;amp;\quad \lambda_i\ge 0,i=1,2,\dots,N\end{aligned}.\]

&lt;p&gt;Then we define the &lt;em&gt;Lagrange dual function&lt;/em&gt; for the primal problem as&lt;/p&gt;

\[g(\lambda)=\min_{w,b}L(w,b,\lambda).\]

&lt;blockquote&gt;
  &lt;p&gt;Actually, the correct definition of the &lt;em&gt;Lagrange dual function&lt;/em&gt; should be&lt;/p&gt;

\[g(\lambda)=\inf_{w,b}L(w,b,\lambda).\]

  &lt;p&gt;Here we assume the minimum exists and the infimum is the minimum for understanding.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The &lt;em&gt;Lagrange dual problem&lt;/em&gt; of the original problem is then defined as&lt;/p&gt;

\[\begin{aligned}\max_{\lambda}&amp;amp;\quad g(\lambda)\\\text{s.t.}&amp;amp;\quad \lambda_i\ge 0,i=1,2,\dots,N\end{aligned}.\]

&lt;p&gt;The dual problem is introduced for its convexity. Specifically, notice that the infimum (minimum in this case) of $g(\lambda)$ is unconstrained as opposed to the original constrained minimization problem. Further, $g(\lambda)$ is concave with respect to $\lambda$ regardless of the original problem.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Primal Problem&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Lagrange Dual Problem&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;\(\begin{aligned}\min_{w,b}\max_\lambda&amp;amp;\quad L(w,b,\lambda)\\\text{s.t.}&amp;amp;\quad \lambda_i\ge 0,i=1,2,\dots,N\end{aligned}\)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;\(\begin{aligned}\max_{\lambda}\min_{w,b}&amp;amp;\quad L(w,b,\lambda)\\\text{s.t.}&amp;amp;\quad \lambda_i\ge 0,i=1,2,\dots,N\end{aligned}\)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;A natural problem is whether the two problems are equivalent. Obviously, the equivalence is obtained if and only if&lt;/p&gt;

\[\min_{w,b}\max_{\lambda} L(w,b,\lambda)=\max_{\lambda}\min_{w,b} L(w,b,\lambda).\]

&lt;p&gt;If the equation holds, we say the &lt;em&gt;strong duality&lt;/em&gt; holds. It can also be shown that the &lt;em&gt;weak duality&lt;/em&gt; always holds as&lt;/p&gt;

\[\min_{w,b}\max_{\lambda} L(w,b,\lambda)\ge\max_{\lambda}\min_{w,b} L(w,b,\lambda).\]

&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Obviously, we have&lt;/p&gt;

\[\max_\lambda L(w,b,\lambda)\ge L(w,b,\lambda)\ge\min_{w,b}L(w,b,\lambda).\]

&lt;p&gt;Define $F(w,b)=\max_\lambda L(w,b,\lambda)$ and $G(\lambda)=\min_{w,b}L(w,b,\lambda)$. According the above inequality, it follows that&lt;/p&gt;

\[F(w,b)\ge G(\lambda)\implies\min_{w,b}F(w,b)\ge\max_\lambda G(\lambda).\]

&lt;p&gt;Therefore we have&lt;/p&gt;

\[\min_{w,b}\max_{\lambda} L(w,b,\lambda)\ge\max_{\lambda}\min_{w,b} L(w,b,\lambda).\tag*{$\blacksquare$}\]

&lt;p&gt;Solving the dual problem in fact is used to find nontrivial lower bounds for difficult original problems. In our case, the strong duality holds for the linearly constrained QP problem. Thus to solve the primal problem is to solve the dual problem.&lt;/p&gt;

&lt;h2 id=&quot;13-karushkuhntucker-conditions&quot;&gt;1.3. Karush–Kuhn–Tucker Conditions&lt;/h2&gt;

&lt;p&gt;For the primal problem and its dual problem, if the strong duality holds, then &lt;em&gt;Karush–Kuhn–Tucker (KKT) conditions&lt;/em&gt; are satisfied as&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;(a). Primal Feasibility:&lt;/p&gt;

\[y_i(w^Tx_i-b)\ge1,i=1,2,\dots,N\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;(b). Dual Feasibility:&lt;/p&gt;

\[\lambda_i\ge 0,i=1,2,\dots,N\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;(c). Complementary Slackness:&lt;/p&gt;

\[\hat\lambda_i\left(1-y_i\left(\hat{w}^Tx_i-\hat b\right)\right)=0,i=1,2,\dots,N\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;(d). Zero gradient of Lagrangian with respect to $w,b$:&lt;/p&gt;

\[\frac{\partial L}{\partial b}=0,\quad \frac{\partial L}{\partial w}=0.\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The conditions (a) and (b) are the original constraints. As for condition (c), recall that we define $y_i(w^Tx_i-b)=1$ for the data that is exactly \(1/\vert\vert w\vert\vert\) away from the hyperplane $w^Tx-b=0$, &lt;em&gt;i.e.,&lt;/em&gt; on the margin of the hyperplane. For those which are not on the margin, to satisfy KKT conditions, it must follow that&lt;/p&gt;

\[\hat\lambda_k=0,k\in\{i\vert y_i(w^Tx_i-b)&amp;gt;1,i=1,2,\dots,N\}.\]

&lt;p&gt;The condition (d) is for the dual problem. Specifically, we consider the unconstrained problem $\min_{w,b}L(w,b,\lambda)$ in the dual problem. For the differentiable function $L(w,b,\lambda)$ , by &lt;em&gt;Fermat’s theorem&lt;/em&gt;, the extremum exists when condition (d) is satisfied, $i.e.,$&lt;/p&gt;

\[\frac{\partial L}{\partial b}=\sum_{i=1}^N\lambda_iy_i=0,\\\frac{\partial L}{\partial w}=w-\sum_{i=1}^N\lambda_iy_ix_i=0.\]

&lt;p&gt;Solving the equations we have&lt;/p&gt;

\[\sum_{i=1}^N\lambda_iy_i=0,\forall b,\quad \hat w=\sum_{i=1}^N\lambda_iy_ix_i.\]

&lt;p&gt;Plugging them into $L(w,b,\lambda)$, we can transform the problem into&lt;/p&gt;

\[\begin{aligned}\max_{\lambda\ge0}&amp;amp;\quad \min_{w,b}L(w,b,\lambda)\\\implies\max_{\lambda\ge0}&amp;amp;\quad \frac{1}{2}\sum_{i=1}^{N}\left(\lambda_iy_ix_i^T\right)\sum_{i=1}^{N}\left(\lambda_iy_ix_i\right)+\sum_{i=1}^N\lambda_i-\sum_{i=1}^N\lambda_iy_i\left(\sum_{j=1}^N\lambda_jy_jx_j^T\right)x_i\\\text{s.t.}&amp;amp;\quad \sum_{i=1}^N\lambda_iy_i=0\\\implies\max_{\lambda\ge0}&amp;amp;\quad -\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^N\left(\lambda_i\lambda_jy_iy_jx_i^Tx_j\right)+\sum_{i=1}^N\lambda_i\\\text{s.t.}&amp;amp;\quad \sum_{i=1}^N\lambda_iy_i=0\\\implies \min_\lambda&amp;amp;\quad \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^N\left(\lambda_i\lambda_jy_iy_jx_i^Tx_j\right)-\sum_{i=1}^N\lambda_i\\\text{s.t.}&amp;amp;\quad \lambda_i\ge0,i=1,2,\dots,N\\&amp;amp;\quad \sum_{i=1}^N\lambda_iy_i=0\end{aligned}\]

&lt;p&gt;The optimal $\hat\lambda$ can be obtained by &lt;em&gt;sequential minimal optimization&lt;/em&gt; (SMO) algorithm. Here we assume we already have the optimal value. Notice we have $\sum_{i=1}^N\hat\lambda_iy_i=0$, which means there exists at least one $\hat\lambda_k\ne0$ otherwise $\hat w=0$. According our analysis, $\hat\lambda_k\ne 0$ only when&lt;/p&gt;

\[y_k(w^Tx_k-b)-1=0.\]

&lt;p&gt;Therefore we have the solution&lt;/p&gt;

\[\hat w=\sum_{i=1}^N\hat\lambda_iy_ix_i,\]

\[\hat b=\sum_{i=1}^N\hat\lambda_iy_ix_i^Tx_k-y_k.\]

&lt;p&gt;Accordingly, the hyperplane is the linear combination of the data on the margin with corresponding $\hat\lambda_k&amp;gt;0$. We call those data &lt;em&gt;support vectors&lt;/em&gt; from where the name SVM comes.&lt;/p&gt;

&lt;h1 id=&quot;2-soft-margin-svm&quot;&gt;2. Soft-margin SVM&lt;/h1&gt;

&lt;p&gt;In practice, there are noise and outliers among the data, which makes the data nonlinearly separable. In that case, hard-margin fails to work. Now we introduce &lt;em&gt;soft-margin SVM&lt;/em&gt; which extends SVM to the nonlinearly separable data. Recall that in hard-margin SVM, we have the constraint&lt;/p&gt;

\[y_i(w^Tx_i-b)\ge 1,i=1,2,\dots,N\]

&lt;p&gt;which confines the model to the linearly separable case. To extent the model to general cases, we introduce &lt;em&gt;loss function&lt;/em&gt;, which can be defined as&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The number of wrongly classifying:&lt;/p&gt;

\[\text{loss}=\sum_{i=1}^N I(y_i(w^Tx_i-b)&amp;lt;1),\]

    &lt;p&gt;where $I(\cdot)$ is the indicator function. However, such loss function is not differentiable.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The sum of the distances between the hyperplane and the outliers:&lt;/p&gt;

\[\begin{aligned}\text{loss}_i&amp;amp;=\begin{cases}0&amp;amp;y_i(w^Tx_i-b)\ge 1\\1-y_i(w^Tx_i-b)&amp;amp;y_i(w^Tx_i-b)&amp;lt;1\text{ (wrongly classified)}\end{cases}\\&amp;amp;=\max\{0, 1-y_i(w^Tx_i-b)\}.\\\text{loss}&amp;amp;=\sum_{i=1}^N\text{loss}_i,\end{aligned}\]

    &lt;p&gt;which is called &lt;strong&gt;hinge loss&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, the $\max$ in the hinge loss is not differentiable neither. We now adapt the original constraint as&lt;/p&gt;

\[y_i(w^Tx_i-b)\ge 1-\xi_i,i=1,2,\dots,N\]

&lt;p&gt;where $\xi_i\ge0$ and $\sum_{i=1}^N\xi_i\le$ constant are called &lt;em&gt;slack variables&lt;/em&gt;. The slack variables is introduced to allow for some points to be on the wrong side of the margin. Specifically, for the points that are on the wrong side, it will break the original constraint $y_i(w^Tx_i-b)\ge 1$ as&lt;/p&gt;

\[y_i(w^Tx_i-b)=\xi&amp;lt; 1.\]

&lt;p&gt;With slack variables, such classification is allowed as long as&lt;/p&gt;

\[\xi\ge 1-\xi_i\implies\xi_i\ge1-\xi.\]

&lt;p&gt;Moreover, we do not want the $\xi_i$ to be too large to distinguish points correctly. Thus we have the new formulation&lt;/p&gt;

\[\begin{aligned}\min_{w,b}&amp;amp;\quad \frac{1}{2}w^Tw+C\sum_{i=1}^N\xi_i\\\text{s.t.}&amp;amp;\quad y_i(w^Tx_i-b)\ge 1-\xi_i\\&amp;amp;\quad \xi_i\ge 0\\&amp;amp;\quad i=1,2,\dots,N\end{aligned},\]

&lt;p&gt;where $C$ is the &lt;em&gt;cost&lt;/em&gt; parameter that determines to what extent we allow for outliers. To solve the problem one can refer to the hard-margin case as they are actually similar.&lt;/p&gt;

&lt;h1 id=&quot;3-conclusion&quot;&gt;3. Conclusion&lt;/h1&gt;

&lt;p&gt;In this post, we first introduced hard-margin SVM for linearly separable data. By introducing a loss function and slack variables, soft-margin SVM allows for noise and outliers so that it can handle non linear case. The two models can both be solved by &lt;em&gt;convex optimization&lt;/em&gt; methods. For convex optimization, we briefly reviewed &lt;em&gt;Lagrange duality&lt;/em&gt;, &lt;em&gt;Slater’s condition&lt;/em&gt; and &lt;em&gt;KKT conditions&lt;/em&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Machine Learning - 04 Dimensionality Reduction</title>
   <link href="/2020/10/26/dimensionality_reduction-ml04/"/>
   <updated>2020-10-26T00:00:00+08:00</updated>
   <id>/2020/10/26/dimensionality_reduction-ml04</id>
   <content type="html">&lt;p&gt;&lt;em&gt;The notes are based on the &lt;a href=&quot;https://github.com/shuhuai007/Machine-Learning-Session&quot;&gt;session&lt;/a&gt;. For the fundamental of linear algebra, one can always refer to &lt;a href=&quot;http://math.mit.edu/~gs/linearalgebra/&quot;&gt;Introduction to Linear Algebra&lt;/a&gt; and &lt;a href=&quot;https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf&quot;&gt;The Matrix Cookbook&lt;/a&gt; for more details. Many thanks to these great works.&lt;/em&gt;&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#0-curse-of-dimensionality&quot; id=&quot;markdown-toc-0-curse-of-dimensionality&quot;&gt;0. Curse of Dimensionality&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#1-sample-mean-and-variance&quot; id=&quot;markdown-toc-1-sample-mean-and-variance&quot;&gt;1. Sample Mean and Variance&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-principal-component-analysis&quot; id=&quot;markdown-toc-2-principal-component-analysis&quot;&gt;2. Principal Component Analysis&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3-principal-component-analysis---an-svd-perspective&quot; id=&quot;markdown-toc-3-principal-component-analysis---an-svd-perspective&quot;&gt;3. Principal Component Analysis - An SVD Perspective&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#4-principal-coordinates-analysis&quot; id=&quot;markdown-toc-4-principal-coordinates-analysis&quot;&gt;4. Principal Coordinates Analysis&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#5-probabilistic-principal-component-analysis&quot; id=&quot;markdown-toc-5-probabilistic-principal-component-analysis&quot;&gt;5. Probabilistic Principal Component Analysis&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#6-conclusion&quot; id=&quot;markdown-toc-6-conclusion&quot;&gt;6. Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;0-curse-of-dimensionality&quot;&gt;0. Curse of Dimensionality&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;The following introduction is derived from &lt;a href=&quot;http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf&quot;&gt;Pattern Recognition and Machine Learning&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;High dimensionality often incurs not only calculation issue but also &lt;em&gt;overfitting&lt;/em&gt;. The increasing in dimension can point to the methods becoming rapidly unwieldy and of limited practical utility.&lt;/p&gt;

&lt;p&gt;Our geometrical intuitions, formed through a life spent in a space of three-dimension, can fail badly when we consider spaces of higher dimensionality. As a simple example, consider a sphere of radius $r = 1$ in a space of $d$ dimensions, and ask what is the fraction of the volume of the sphere that lies between radius $r=1-\varepsilon$ and $r=1$. We can evaluate this fraction by noting that the volume of a sphere of radius $r$ in $d$ dimensions must scale as $r^d$, and so we write&lt;/p&gt;

\[V_d(r)=K_dr^d,\]

&lt;p&gt;where the constant $K_d$ depends only on $d$. Thus the required fraction is given by&lt;/p&gt;

\[\frac{V_d(1)-V_d(1-\varepsilon)}{V_d(1)}=\frac{K_dr^d-K_d(1-\varepsilon)^d}{K_dr^d}=1-(1-\varepsilon)^d.\]

&lt;p&gt;Obviously, for large $d$, this fraction tends to 1 even for small values of $\varepsilon$. Thus, in spaces of high dimensionality, most of the volume of a sphere is concentrated in a thin shell near the surface!&lt;/p&gt;

&lt;p&gt;Luckily, for those data of high dimensionality, we can refer to &lt;em&gt;dimensionality reduction&lt;/em&gt; algorithms.&lt;/p&gt;

&lt;h1 id=&quot;1-sample-mean-and-variance&quot;&gt;1. Sample Mean and Variance&lt;/h1&gt;

&lt;p&gt;Before we move on, we give the following definitions. Suppose we have data \(X=(x_1,x_2,\dots,x_N)^T\in\mathbb{R}^{N\times d}\), where $x_i\in\mathbb{R}^{d\times 1}$ is a sample with $d$ features. Specifically,&lt;/p&gt;

\[X=(x_1,x_2,\dots,x_N)^T=\begin{pmatrix}x_{1}^T\\x_{2}^T\\\vdots\\x_{N}^T\end{pmatrix}=\begin{pmatrix}x_{11}&amp;amp;x_{12}&amp;amp;\cdots&amp;amp;x_{1d}\\x_{21}&amp;amp;x_{22}&amp;amp;\cdots&amp;amp;x_{2d}\\\vdots&amp;amp;\vdots&amp;amp;\ddots&amp;amp;\vdots\\x_{N1}&amp;amp;x_{N2}&amp;amp;\cdots&amp;amp;x_{Nd}\end{pmatrix}_{N\times d}.\]

&lt;p&gt;Then we have the &lt;strong&gt;sample mean&lt;/strong&gt;&lt;/p&gt;

\[\bar X=\frac{1}{N}\sum_{i=1}^Nx_i,\]

&lt;p&gt;and the &lt;strong&gt;sample covariance&lt;/strong&gt;&lt;/p&gt;

\[S=\frac{1}{N}\sum_{i=1}^N\left(x_i-\bar X\right)\left(x_i-\bar X\right)^T.\]

&lt;p&gt;However, the &lt;em&gt;sum&lt;/em&gt; operation is inconvenient in calculating. Thus we further transform them into&lt;/p&gt;

\[\bar X=\frac{1}{N}(x_1,x_2,\dots,x_N)\begin{pmatrix}1\\1\\\vdots\\1\end{pmatrix}=\frac{1}{N}X^T\mathbf{1}_{N\times 1},\]

&lt;p&gt;and&lt;/p&gt;

\[\begin{aligned}S&amp;amp;=\frac{1}{N}\underbrace{(x_1-\bar X,x_2-\bar X,\dots,x_N-\bar X)}_{X^T-\bar X(1,1,\dots,1)}\begin{pmatrix}(x_1-\bar X)^T\\(x_2-\bar X)^T\\\vdots\\(x_N-\bar X)^T\end{pmatrix}\\&amp;amp;=\frac{1}{N}\left(X^T-\bar X\mathbf{1}_{1\times N}\right)\left(X^T-\bar X\mathbf{1}_{1\times N}\right)^T\\&amp;amp;=\frac{1}{N}\left(X^T-\frac{1}{N}X^T\mathbf{1}_{N\times 1}\mathbf{1}_{1\times N}\right)\left(X^T-\frac{1}{N}X^T\mathbf{1}_{N\times 1}\mathbf{1}_{1\times N}\right)^T\\&amp;amp;=\frac{1}{N}X^T\underbrace{\left(\mathbf{I}_{N}-\frac{1}{N}\mathbf{1}_{N\times N}\right)}_{H}\left(\mathbf{I}_{N}-\frac{1}{N}\mathbf{1}_{N\times N}\right)^TX\\&amp;amp;=\frac{1}{N}X^THH^TX\\&amp;amp;=\frac{1}{N}X^THX,\end{aligned}\]

&lt;p&gt;where $H$ is &lt;em&gt;centering matrix&lt;/em&gt;, and it can be shown that $H^T=H,H^n=H$.&lt;/p&gt;

&lt;h1 id=&quot;2-principal-component-analysis&quot;&gt;2. Principal Component Analysis&lt;/h1&gt;

&lt;p&gt;Principal component analysis (PCA) is defined as an &lt;a href=&quot;https://en.wikipedia.org/wiki/Orthogonal_transformation&quot;&gt;orthogonal&lt;/a&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_transformation&quot;&gt;linear transformation&lt;/a&gt; that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.&lt;/p&gt;

&lt;p&gt;Recall that in &lt;a href=&quot;https://2ez4ai.github.io/2020/10/14/linear_classification-ml03/&quot;&gt;the previous post&lt;/a&gt;, we introduce &lt;em&gt;linear discriminant analysis&lt;/em&gt; (LDA), which requires a projection that maximizes the distance between different classes. In PCA, instead of classes, we need to find a projection that maximizes the distance (variance) of all the data so that we can reduce the dimensionality (as a projection involves one dimensionality reduction) while losing the least information (as the greatest variance implies most information). Now we give the mathematically formulation.&lt;/p&gt;

&lt;p&gt;The notation in section 1 will be used in this section as well. Since PCA is sensitive to the variance of the data, it is critical to normalize the variables over all dimensions, which yields&lt;/p&gt;

&lt;p&gt;$z_i=x_i-\bar X.$&lt;/p&gt;

&lt;p&gt;In many case, the values of $z_i\in\mathbb{R}^{d\times 1}$ should be scaled in $[0,1]$ but here we just consider the simple case. We then denote the transformation vector as $W=(w_1,w_2,\dots,w_d)\in\mathbb{R}^{d\times d}$ which is the &lt;em&gt;orthogonal basis&lt;/em&gt; of the new coordinate system. Then the new data $\tilde z_i$, &lt;em&gt;i.e.&lt;/em&gt;, the projection of $z_i$ on the new coordinate system is&lt;/p&gt;

\[\tilde z_i=z_i^TW=(z_i^Tw_1,z_i^Tw_2,\dots,z_i^Tw_d),\]

&lt;p&gt;where $z_i^Tw_j\in\mathbb{R}$ is the value of $j$-th dimension of the new data. Now suppose we want reduce the dimension from $d$ to $k$, &lt;em&gt;i.e.,&lt;/em&gt; we want to preserve the first $k$ dimensions of $\tilde z_i$ while maximizing the variance, which gives us the objective function&lt;/p&gt;

\[\mathcal{J}(w)=\sum_{j=1}^k\sum_{i=1}^N\frac{1}{N}\left(z_i^Tw_j-\overline{z^Tw_j}\right)^2,\]

&lt;p&gt;where \(\sum_{i=1}^N\frac{1}{N}\left(z_i^Tw_j-\overline{z^Tw_j}\right)^2\) is the variance of the new data in $j$-th dimension. \(\overline{z^Tw_j}\) is the mean of the new data in $j$-th dimension, which is&lt;/p&gt;

\[\begin{aligned}\overline{z^Tw_j}&amp;amp;=\frac{1}{N}\sum_{i=1}^Nz_i^Tw_j\\&amp;amp;=\frac{1}{N}\sum_{i=1}^N(x_i-\bar X)^Tw_j\\&amp;amp;=\frac{1}{N}\left(\sum_{i=1}^Nx_i^Tw_j-N\bar X^Tw_j\right)\\&amp;amp;=\frac{1}{N}\left(\sum_{i=1}^Nx_i^Tw_j-\sum_{i=1}^Nx_i^Tw_j\right)\\&amp;amp;=0\end{aligned}.\]

&lt;p&gt;Therefore the objective function follows that&lt;/p&gt;

\[\begin{aligned}\mathcal{J}(w)&amp;amp;=\sum_{j=1}^k\sum_{i=1}^N\frac{1}{N}\left(z_i^Tw_j\right)^2\\&amp;amp;=\sum_{j=1}^k\sum_{i=1}^N\frac{1}{N}\left((x_i-\bar X)^Tw_j\right)^2\\&amp;amp;=\sum_{j=1}^k\sum_{i=1}^N\frac{1}{N}w_j^T(x_i-\bar X)(x_i-\bar X)^Tw_j\\&amp;amp;=\sum_{j=1}^kw_j^TSw_j\end{aligned},\]

&lt;p&gt;where each $w_j^TSw_j$ can be maximized independently since $w_j$ are orthogonal. Combined with the constraint that $w_j$ is a unit vector, the problem is then equivalent to&lt;/p&gt;

\[\hat w_j=\arg\max_{w_j} w_j^TSw_j\quad \text{s.t. }w_j^Tw_j=1.\]

&lt;p&gt;The problem can be solved by &lt;a href=&quot;https://en.wikipedia.org/wiki/Lagrange_multiplier&quot;&gt;the method of Lagrange multipliers&lt;/a&gt; as follows,&lt;/p&gt;

\[\begin{aligned}\mathcal{L}(w_j,\lambda_j)&amp;amp;=w_j^TSw_j+\lambda_j(1-w_j^Tw_j),\\\frac{\partial\mathcal{L}(w_j,\lambda_j)}{\partial w_j}&amp;amp;=2Sw_j-2\lambda_j w_j.\end{aligned}\]

&lt;p&gt;Setting the second equation to be zero, we have the standard eigenvalue problem&lt;/p&gt;

\[S\hat{w}_j=\lambda_j\hat{w}_j\implies SW=\text{diag}(\lambda)W ,\]

&lt;p&gt;&lt;em&gt;i.e.,&lt;/em&gt; $\hat w_j$ is actually the eigenvalue of $S$. Plugging the above equation to the objective function, we have&lt;/p&gt;

\[\begin{aligned}\max\mathcal{J(\hat{w})}&amp;amp;=\max\sum_{j=1}^k\hat{w}_j^TS\hat{w}_j\\&amp;amp;=\max\sum_{j=1}^k\hat{w}_j^T\lambda_j\hat{w}_j\\&amp;amp;=\max\sum_{j=1}^k\lambda_j\end{aligned}.\]

&lt;p&gt;Therefore, to reduce the data from $d$ to $k$ dimension, one should select the $k$ eigenvectors of $S$ corresponding to the $k$ largest eigenvalues to construct the $W\in\mathbb{R}^{d\times k}$, which is&lt;/p&gt;

\[W=(w_{(1)},w_{(2)},\dots,w_{(k)}),\]

&lt;p&gt;where $w_{(i)}$ is the eigenvector corresponding to the $i$-th largest eigenvalue.  Then the data in the new coordinate system is&lt;/p&gt;

\[X^\text{new}=(x_1-\bar X,x_2-\bar X,\dots, x_N-\bar X)^TW.\]

&lt;h1 id=&quot;3-principal-component-analysis---an-svd-perspective&quot;&gt;3. Principal Component Analysis - An SVD Perspective&lt;/h1&gt;

&lt;p&gt;Now we consider the &lt;em&gt;singular vector decomposition&lt;/em&gt; (SVD) of the centralized data,&lt;/p&gt;

\[HX=U\Sigma V^T,\]

&lt;p&gt;where $U=(u_1,u_2,\dots,u_N)\in\mathbb{R}^{N\times N},\Sigma\in\mathbb{R}^{N\times d},V=(v_1,v_2,\dots,v_N)\in\mathbb{R}^{d\times d}$, and they have the following properties,&lt;/p&gt;

\[\begin{aligned}UU^T&amp;amp;=U^TU=\mathbf{I}_{N},\\VV^T&amp;amp;=V^TV=\mathbf{I}_{d},\\\Sigma_{ij}=0,\quad i&amp;amp;=0,1,…,N,j=0,1,…,d,i\ne j.\end{aligned}\]

&lt;p&gt;We further represent $\Sigma$ as $\lambda(\sigma_1,\sigma_2,\dots,\sigma_d)$. Then according to the analysis in section 1, the covariance of the data is&lt;/p&gt;

\[\begin{aligned}S&amp;amp;=\frac{1}{N}X^THX\\&amp;amp;=\frac{1}{N}X^TH^THX\\&amp;amp;=\frac{1}{N}(HX)^THX\\&amp;amp;=\frac{1}{N}V\Sigma^TU^TU\Sigma V^T\\&amp;amp;=V\text{diag}\left(\frac{\sigma_1^2}{N},\frac{\sigma_2^2}{N},\dots,\frac{\sigma_d^2}{N}\right)V^T\end{aligned}.\]

&lt;p&gt;By multiplying $V$ on both sides, it follows that&lt;/p&gt;

\[SV=V\text{diag}\left(\frac{\sigma_1^2}{N},\frac{\sigma_2^2}{N},\dots,\frac{\sigma_d^2}{N}\right)\implies Sv_i=\frac{\sigma_i^2}{N}v_i,\]

&lt;p&gt;which is consistent with the conclusion of PCA. Therefore, instead of decomposing the covariance matrix $S$, we can conduct an SVD on the centralized data, which gives the transformation matrix that allows us to obtain the new data&lt;/p&gt;

\[Z=HX\cdot V.\]

&lt;p&gt;By selecting $k$ vectors of $V$ according to the single values, we can reduce the original data matrix from $d$ to $k$ dimensions. Such decomposition may take advantage when $N\ll d$. Intuitively, $HX\in\mathbb{R}^{N\times d}$ and $S\in\mathbb{R}^{d\times d}$. When $N\ll d$, decomposing $HX$ should be more efficient than decomposing $S$.&lt;/p&gt;

&lt;h1 id=&quot;4-principal-coordinates-analysis&quot;&gt;4. Principal Coordinates Analysis&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;Principal coordinates analysis&lt;/em&gt; (PCoA) is a well known technique in many fields. It actually can be derived from PCA. Specifically, we consider a matrix&lt;/p&gt;

\[T=HXX^TH^T.\]

&lt;p&gt;By SVD, we have&lt;/p&gt;

\[T=U\Sigma V^TV\Sigma^TU^T=U\Sigma\Sigma^T U^T,\]

&lt;p&gt;which is similar to the decomposition of $S$ in section 3. Further, by multiplying $U\Sigma$ on the both sides, we have&lt;/p&gt;

\[TU\Sigma=U\Sigma\Sigma^TU^TU\Sigma=U\Sigma\text{diag}\left(\sigma_1^2,\sigma_2^2,\dots,\sigma_d^2\right)\implies T(U\Sigma)_i=\sigma^2_i(U\Sigma)_i.\]

&lt;p&gt;Therefore, $U\Sigma$ is actually composed of $d$  eigenvalues of $T$. Recall that in section 3, we have the new data as&lt;/p&gt;

\[Z=HX\cdot V=U\Sigma V^T\cdot V=U\Sigma,\]

&lt;p&gt;which implies that by the eigenvalue decomposition of $T$, we can get the new data directly. Such a dimensionality reduction technique with a different perspective is PCoA. Notice $T\in\mathbb{R}^{N\times N}$, thus the complexity of PCoA is $O(N^2)$.&lt;/p&gt;

&lt;h1 id=&quot;5-probabilistic-principal-component-analysis&quot;&gt;5. Probabilistic Principal Component Analysis&lt;/h1&gt;

&lt;p&gt;Just like the notations we used in previous sections, we define the new data we want to transform $X$ into is $Z=(z_1,z_2,\dots,z_N)$ where $z_i\in\mathbb{R}^{k\times 1}$. However, in &lt;em&gt;probabilistic principal component analysis&lt;/em&gt; (PPCA), we further introduce randomness as&lt;/p&gt;

\[z_i\sim\mathcal{N}(\mathbf{0}_{k\times 1}, \mathbf{I}_{k\times k}),\]

\[x_i=Wz_i+\mu+\varepsilon,\]

\[\varepsilon\sim\mathcal{N}(\mathbf{0}_{d\times 1},\sigma^2\mathbf{I}_{d\times d}),\]

&lt;p&gt;where $W\in\mathbb{R}^{d\times k}, \mu\in\mathbb{R}^{d\times 1}$ and $\sigma^2$ are the parameters to be learned ($\mu$ can be viewed as the bias term of many machine learning model). Such randomization can generalize the model to the unseen data. One can also refer such a model to &lt;em&gt;linear Gaussian model&lt;/em&gt;. In particular,  there are two phases. The first is learning:&lt;/p&gt;

\[\hat W,\hat \mu, \hat \sigma^2=\arg\max_{W,\mu,\sigma^2}P(X\vert Z;W,\mu,\sigma^2).\]

&lt;p&gt;The second is inference (dimensionality reduction):&lt;/p&gt;

\[Z=\arg\max_{\tilde Z} P(\tilde Z\vert X).\]

&lt;p&gt;The learning part can be dealt with &lt;em&gt;expectation–maximization algorithm&lt;/em&gt;. The following is the details of the inference procedure. According to the definition, we have&lt;/p&gt;

\[x_i\vert z_i\sim\mathcal{N}(Wz_i+\mu,\sigma^2\mathbf{I}_{d\times d}),\]

&lt;p&gt;where $z_i$ is a sample rather than a random variable. Also, we have&lt;/p&gt;

\[\mathbb{E}[x_i]=\mathbb{E}[Wz_i+\mu+\varepsilon]=\mu,\]

\[var[x_i]=var[Wz_i+\mu+\varepsilon]=var[Wz_i]+var[\varepsilon]=WW^T+\sigma^2\mathbf{1}_{d\times d},\]

&lt;p&gt;and&lt;/p&gt;

\[x_i\sim\mathcal{N}(\mu, WW^T+\sigma^2\mathbf{1}_{d\times d}).\]

&lt;p&gt;Then we consider the covariance $Cov(x_i,z_i)$,&lt;/p&gt;

\[\begin{aligned}Cov(x_i,z_i)&amp;amp;=\mathbb{E}[(x_i-\mu)(z_i-\mathbf{0}_{k\times 1})^T]\\&amp;amp;=\mathbb{E}[(x_i-\mu)z_i^T]\\&amp;amp;=\mathbb{E}[(Wz_i+\varepsilon)z_i^T]\\&amp;amp;=\mathbb{E}[Wz_iz_i^T]+\mathbb{E}[\varepsilon z_i^T]\\&amp;amp;=Wvar[z_i]+\mathbb{E}[\varepsilon]\mathbb{E}[z_i^T]\\&amp;amp;=W\end{aligned}.\]

&lt;p&gt;Hence the union distribution of $(x_i,z_i)^T$ is&lt;/p&gt;

\[\begin{pmatrix}x_i\\z_i\end{pmatrix}\sim\mathcal{N}\left(\begin{pmatrix}\mu\\\mathbf{0}_{k\times1} \end{pmatrix},\begin{pmatrix}WW^T+\sigma^2\mathbf{1}_{d\times d}&amp;amp;W\\W^T&amp;amp;\mathbf{1}_{k\times k}\end{pmatrix}\right).\]

&lt;p&gt;By the formula derived in &lt;a href=&quot;https://2ez4ai.github.io/2020/09/28/intro-ml01/&quot;&gt;session 01&lt;/a&gt;, it can be shown that&lt;/p&gt;

\[z_i\vert x_i\sim\mathcal{N}(W^T(WW^T+\sigma^2\mathbf{1}_{d\times d})^{-1}(x_i-\mu),\mathbf{1}_{k\times k}-W^T(WW^T+\sigma^2\mathbf{1}_{d\times d})^{-1}W).\]

&lt;h1 id=&quot;6-conclusion&quot;&gt;6. Conclusion&lt;/h1&gt;

&lt;p&gt;In this post, we introduced the naive &lt;em&gt;principal component analysis&lt;/em&gt; (PCA) model. Then we conducted &lt;em&gt;singular vector decomposition&lt;/em&gt; on the centralized data, which gives us the same conclusion as that of PCA. Such conclusion can also be derived from &lt;em&gt;Principal coordinates analysis&lt;/em&gt; (PCoA) model. Further, by introducing parameters, &lt;em&gt;probabilistic principal component analysis&lt;/em&gt; (PPCA) can generalize PCA to handle unseen data.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Machine Learning - 03 Linear Classification</title>
   <link href="/2020/10/14/linear_classification-ml03/"/>
   <updated>2020-10-14T00:00:00+08:00</updated>
   <id>/2020/10/14/linear_classification-ml03</id>
   <content type="html">&lt;p&gt;&lt;em&gt;The notes are based on the &lt;a href=&quot;https://github.com/shuhuai007/Machine-Learning-Session&quot;&gt;session&lt;/a&gt;. For the fundamental of linear algebra, one can always refer to &lt;a href=&quot;http://math.mit.edu/~gs/linearalgebra/&quot;&gt;Introduction to Linear Algebra&lt;/a&gt; and &lt;a href=&quot;https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf&quot;&gt;The Matrix Cookbook&lt;/a&gt; for more details. Many thanks to these great works.&lt;/em&gt;&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#0-introduction&quot; id=&quot;markdown-toc-0-introduction&quot;&gt;0. Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#1-perceptron&quot; id=&quot;markdown-toc-1-perceptron&quot;&gt;1. Perceptron&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-linear-discriminant-analysis&quot; id=&quot;markdown-toc-2-linear-discriminant-analysis&quot;&gt;2. Linear Discriminant Analysis&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3-discriminant-classifiers&quot; id=&quot;markdown-toc-3-discriminant-classifiers&quot;&gt;3. Discriminant Classifiers&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#31-logistic-regression&quot; id=&quot;markdown-toc-31-logistic-regression&quot;&gt;3.1. Logistic Regression&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#4-generative-classifiers&quot; id=&quot;markdown-toc-4-generative-classifiers&quot;&gt;4. Generative Classifiers&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#41-naive-bayes-classifier&quot; id=&quot;markdown-toc-41-naive-bayes-classifier&quot;&gt;4.1. Naive Bayes Classifier&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#42-gaussian-discriminant-analysis&quot; id=&quot;markdown-toc-42-gaussian-discriminant-analysis&quot;&gt;4.2. Gaussian Discriminant Analysis&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#5-conclusion&quot; id=&quot;markdown-toc-5-conclusion&quot;&gt;5. Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;0-introduction&quot;&gt;0. Introduction&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;The following introduction is derived from the &lt;a href=&quot;http://pages.stat.wisc.edu/~wahba/stat860public/pdf1/liu.zhang.wu.lum.11.pdf&quot;&gt;paper&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As a supervised learning technique, the goal of classification is to construct a classification rule based on a training set where both data and class labels are given. Once obtained, the classification rule can then be used for class prediction of new objects whose covariates are available.&lt;/p&gt;

&lt;p&gt;Among various classification methods, there are two main groups: &lt;em&gt;soft&lt;/em&gt; and &lt;em&gt;hard&lt;/em&gt; classification. In particular, a soft classification rule generally estimates the class &lt;em&gt;conditional probabilities&lt;/em&gt; explicitly and then makes the class prediction &lt;em&gt;based on the estimated probability&lt;/em&gt;. Depending on whether calculating the conditional probability directly or approximating it by a model, there are &lt;em&gt;generative classifiers&lt;/em&gt; and &lt;em&gt;discriminant classifiers&lt;/em&gt; among the &lt;em&gt;soft&lt;/em&gt; methods. In contrast, hard classification bypasses the requirement of class probability estimation and directly estimates the &lt;em&gt;classification boundary&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Typical soft classifiers include some traditional distribution-based likelihood approaches such as logistic regression. On the other hand, some margin-based approaches such as perceptron and the SVM, generally distributional assumption-free, belong to the class of hard classification methods.&lt;/p&gt;

&lt;p&gt;We assume the data set is linearly separable in the following subsections.&lt;/p&gt;

&lt;h1 id=&quot;1-perceptron&quot;&gt;1. Perceptron&lt;/h1&gt;

&lt;p&gt;Perceptron is a &lt;em&gt;hard&lt;/em&gt; method for &lt;em&gt;binary classification&lt;/em&gt;. Suppose we have i.i.d. data \(\mathcal{D}=\{(x_1,y_1), (x_2,y_2),\dots,(x_N,y_N)\},X=\{x_1,x_2,\dots,x_N\}, Y=\{y_1,y_2,\dots,y_N\}\) where $x_i\in\mathbb{R}^{d\times 1}$ can be viewed as the feature and \(y_i\in\{-1,1\}\) is the corresponding label. In particular, we denote \(X_{c1}=\{x_i\vert y_i=+1\}\) and \(X_{c2}=\{x_i\vert y_i=-1\}\) as the set of class $c_1$ and class $c_2$, respectively. Moreover, let $N_1=| X_{c1}|$ and $N_2=|X_{c2}|$, where $N_1+N_2=N$. The model of perceptron follows&lt;/p&gt;

\[f(w)=\text{sign}(w^Tx),\]

&lt;p&gt;where $w\in\mathbb{R}^{d\times 1}$ and $\text{sign}(\cdot)$ is the sign function. Perceptron is actually an error-driven method. Specifically, for data $(x_i,y_i)$, the correctness of perceptron can be described as&lt;/p&gt;

\[y_iw^Tx_i\ge0\iff\begin{cases}w^Tx_i\ge0,\ f(w)=1,&amp;amp;\text{if } y_i=+1\\w^Tx_i&amp;lt;0,\ f(w)=-1, &amp;amp;\text{if }y_i=-1\end{cases}\]

&lt;p&gt;Define \(\tilde D=\{(x,y)\vert y_iw^T x_i&amp;lt;0, i=1,\dots,N\}\) be the set of data that was classified incorrectly. Then the loss function of the model can be defined as the size of $\tilde D$:&lt;/p&gt;

\[\mathcal{L}(w)=\sum_{i=1}^NI(y_iw^Tx_i&amp;lt;0),\]

&lt;p&gt;where $I(\cdot)$ is the indicator function. Though such a loss function is intuitive, it is uncontinuous and can be hard to be optimized. From the standpoint of the model, to make $y_iw^Tx_i\ge0$ is equivalent to make $y_iw^Tx_i$ as larger as possible, thus we can transform the loss function into&lt;/p&gt;

\[\mathcal{L}(w)=\sum_{i=1}^N-y_iw^Tx_i,\]

&lt;p&gt;which can be minimized by various optimization methods such as &lt;em&gt;stochastic gradient descent&lt;/em&gt;.&lt;/p&gt;

&lt;h1 id=&quot;2-linear-discriminant-analysis&quot;&gt;2. Linear Discriminant Analysis&lt;/h1&gt;

&lt;p&gt;Now we introduce &lt;em&gt;linear discriminant analysis&lt;/em&gt; (LDA), which is a method for &lt;em&gt;binary classification&lt;/em&gt;. Note that in some materials, LDA is defined as a dimensionality reduction technique. Further, we introduce LDA method  in this note from a hard classification perspective. The soft perspective can be also found in other materials. The notations for data in this subsection are the same as that in subsection 1. We further define the mean&lt;/p&gt;

\[\bar x_{c1}=\frac{1}{N_1}\sum_{x\in X_{c1}}x,\quad \bar x_{c2}=\frac{1}{N_2}\sum_{x\in X_{c2}}x,\]

&lt;p&gt;and the variance&lt;/p&gt;

\[S_{c1}=\frac{1}{N_1}\sum_{x\in X_{c1}}(x-\bar x_{c1})(x-\bar x_{c1})^T,\quad S_{c2}=\frac{1}{N_2}\sum_{x\in X_{c2}}(x-\bar x_{c2})(x-\bar x_{c2})^T.\]

&lt;blockquote&gt;
  &lt;p&gt;The idea of LDA is proposed by Ronald Fisher in 1988: maximize the distance between the mean of each class and minimize the spreading within the class itself.&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;In LDA, we consider the ‘&lt;em&gt;projection&lt;/em&gt;’ of $x$:&lt;/p&gt;

\[z=w^Tx,\]

&lt;p&gt;where $w\in\mathbb{R}^{d\times 1}$ is a unit vector to be learned. Specifically, the scalar $z$ is the length of the projection of $x$ on $w$, thus we can view such $z$ as the projection of $x$ into a &lt;em&gt;one dimensional subspace&lt;/em&gt;. Note that the definition here is different from the definition of projection in &lt;em&gt;Introduction to Linear Algebra&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Then we have the following definitions about the &lt;em&gt;mean&lt;/em&gt;,&lt;/p&gt;

\[\bar z =\frac{1}{N}\sum_{i=1}^Nw^Tx_i,\quad \bar z_1=\frac{1}{N_1}\sum_{x\in X_{c1}}w^Tx,\quad \bar z_2=\frac{1}{N_2}\sum_{x\in X_{c2}}w^Tx.\]

&lt;p&gt;Similarly, we have the definitions related to the variance as&lt;/p&gt;

\[S=\frac{1}{N}\sum_{i=1}^{N}(w^Tx_i-\bar z)^2,\quad S_1=\frac{1}{N_1}\sum_{x\in X_{c1}}(w^Tx-\bar z_1)^2,\quad S_2=\frac{1}{N_2}\sum_{x\in X_{c2}}(w^Tx-\bar z_2)^2.\]

&lt;p&gt;Then we use the mean to define the distance between the two class and the variance to represent the spreading within the class itself. LDA is then to find the unit vector $\hat w$ that maximizes&lt;/p&gt;

\[\mathcal{J}(w)=\frac{(\bar z_1-\bar z_2)^2}{S_1+S_2}.\]

&lt;p&gt;For the numerator, it follows that&lt;/p&gt;

\[\begin{aligned}(\bar z_1-\bar z_2)^2&amp;amp;=\left(\frac{1}{N_1}\sum_{x\in X_{c1}}w^Tx_i-\frac{1}{N_2}\sum_{x\in X_{c2}}w^Tx_i\right)^2\\&amp;amp;=w^T\left(\bar x_{c1}-\bar x_{c_2}\right)\left(\bar x_{c1}-\bar x_{c_2}\right)^Tw\end{aligned}.\]

&lt;p&gt;For the denominator, it follows that&lt;/p&gt;

\[\begin{aligned}S_1+S_2&amp;amp;=\frac{1}{N_1}\sum_{x\in X_{c1}}(w^Tx-\bar z_1)^2+\frac{1}{N_2}\sum_{x\in X_{c2}}(w^Tx-\bar z_2)^2\\&amp;amp;=w^T\left[\frac{1}{N_1}\sum_{x\in X_{c1}}(x-\bar x_{c1})(x-\bar x_{c1})^T\right]w+w^T\left[\frac{1}{N_2}\sum_{x\in X_{c2}}(x-\bar x_{c2})(x-\bar x_{c2})^T\right]w\\&amp;amp;=w^TS_{c1}w+w^TS_{c2}w\\&amp;amp;=w^T(S_{c1}+S_{c2})w\end{aligned}.\]

&lt;p&gt;Therefore, we have&lt;/p&gt;

\[\mathcal{J}(w)=\frac{w^TS_bw}{w^TS_ww},\]

&lt;p&gt;where $S_b=\left(\bar x_{c1}-\bar x_{c_2}\right)\left(\bar x_{c1}-\bar x_{c_2}\right)^T$ represents the distance &lt;em&gt;between-class&lt;/em&gt;, $S_w=S_{c1}+S_{c2}$ represents the spreading &lt;em&gt;within-class&lt;/em&gt;. Such transformation is actually for computing derivation. $J(w)$ can be maximized by taking the derivative w.r.t $w$ and setting it to be $0$. Specifically,&lt;/p&gt;

\[\begin{aligned}\frac{\partial \mathcal{J}(w)}{\partial w}&amp;amp;=\frac{\left(\frac{\partial}{\partial w}w^TS_b w\right)w^TS_ww-w^TS_bw\left(\frac{\partial}{\partial w}w^TS_w w\right)}{(w^TS_ww)^2}\\&amp;amp;=\frac{(2S_bw)w^TS_ww-w^TS_bw(2S_ww)}{(w^TS_ww)^2}\end{aligned}.\]

&lt;p&gt;Setting it to be 0 is equivalent to&lt;/p&gt;

\[\begin{aligned}(2S_bw)w^TS_ww-w^TS_bw(2S_ww)&amp;amp;=0\\ (w^TS_bw)S_ww&amp;amp;=S_bw(w^TS_ww)\\S_w w&amp;amp;=\frac{w^TS_ww}{w^TS_bw}S_bw\end{aligned}.\]

&lt;p&gt;As $w\in\mathbb{R}^{d\times 1}$ and $S_w,S_b\in\mathbb{R}^{d\times d}$, the term $(w^TS_ww)/(w^TS_bw)\in\mathbb{R}$. For convenience, we denote it as $\lambda$. Then we have an equivalent &lt;em&gt;generalized eigenvalue problem&lt;/em&gt;&lt;/p&gt;

\[S_ww=\lambda S_bw.\]

&lt;p&gt;If one of $S_b$ and $S_w$ has full rank, the generalized eigenvalue problem can be converted into a standard eigenvalue problem. However, to solve the problem entails complex computation. We now assume $S_w^{-1}$ exists. Recall that $w$ is a unit vector. Thus what we need to care is only the direction of $w$:&lt;/p&gt;

\[\begin{aligned}\hat w&amp;amp;\propto \lambda S_w^{-1}S_bw\\&amp;amp;\propto \lambda S_w^{-1}\left(\bar x_{c1}-\bar x_{c_2}\right)\left(\bar x_{c1}-\bar x_{c_2}\right)^Tw\\&amp;amp;\propto\lambda_1S_w^{-1}\left(\bar x_{c1}-\bar x_{c_2}\right)\\&amp;amp;\propto S_w^{-1}\left(\bar x_{c1}-\bar x_{c_2}\right)\end{aligned},\]

&lt;p&gt;where $\lambda_1=\lambda \left(\bar x_{c1}-\bar x_{c_2}\right)^Tw$ is a scalar as $\left(\bar x_{c1}-\bar x_{c_2}\right)^Tw\in\mathbb{R}$.&lt;/p&gt;

&lt;h1 id=&quot;3-discriminant-classifiers&quot;&gt;3. Discriminant Classifiers&lt;/h1&gt;

&lt;p&gt;Discriminant classifiers focus on the classification problem directly. Specifically, discriminant classifiers  model the posterior $P(Y\vert X)$, then makes the class prediction based on the estimated probability.&lt;/p&gt;

&lt;h2 id=&quot;31-logistic-regression&quot;&gt;3.1. Logistic Regression&lt;/h2&gt;

&lt;p&gt;Logistic regression inputs the result of a &lt;em&gt;linear regression&lt;/em&gt; to a &lt;em&gt;sigmoid function&lt;/em&gt; to make classification. A sigmoid function is&lt;/p&gt;

\[\sigma(z)=\frac{1}{1+e^{-z}}\]

&lt;p&gt;which maps $z\in(-\infty,+\infty)$ into a probability $[0,1]$. Therefore we can model the posterior probability as&lt;/p&gt;

\[P(y=1|x;w)=\sigma(w^Tx)=\frac{1}{1+e^{-w^Tx}},\]

&lt;p&gt;where $w$ is the parameter to be learned. As for $P(y=-1\vert x;w)$, it can be obtained by $1-P(y=1\vert x;w)$ since we are considering a binary classification problem. However, for a supervised learning technique, it would be convenient to consider both two labels into one function. To this end, we set the label value to be \(y_i\in\{0,1\}\) . Moreover, we denote $P(y_i=1\vert x_i;w)$ and $P(y_i=0\vert x_i;w)$ as $p_{i\cdot 1}$ and $p_{i\cdot 0}$, respectively. Then logistic regression is to find $\hat w$ that maximizes&lt;/p&gt;

\[\mathcal{J}(w)=P(Y|X;w)=\prod_{i=1}^N p_{i\cdot 1}^{y_i}p_{i\cdot 0}^{1-y_i}.\]

&lt;p&gt;Given the dataset $\mathcal{D}$, such maximization problem can be solved by &lt;em&gt;maximum likelihood estimation&lt;/em&gt;:&lt;/p&gt;

\[\begin{aligned}\hat w&amp;amp;=\arg\max_w \log P(Y\vert X;w)\\&amp;amp;=\arg\max_w \log\prod_{i=1}^N p_{i\cdot 1}^{y_i}p_{i\cdot 0}^{1-y_i}\\&amp;amp;=\arg\max_w \sum_{i=1}^N(y_i\log p_{i\cdot 1}+(1-y_i)\log p_{i\cdot 0})\\&amp;amp;=\arg\max_w \sum_{i=1}^N[y_i\log \sigma(w^Tx_i)+(1-y_i)\log (1-\sigma(w^Tx_i))]\end{aligned}.\]

&lt;blockquote&gt;
  &lt;p&gt;The term \(\sum_{i=1}^N(y_i\log \sigma(w^Tx_i)+(1-y_i)\log (1-\sigma(w^Tx_i)))\) is actually the negative of &lt;em&gt;cross entropy&lt;/em&gt; over $P(Y)$ and $\sigma(w^TX)$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To solve the above problem, one can refer to SGD method.&lt;/p&gt;

&lt;h1 id=&quot;4-generative-classifiers&quot;&gt;4. Generative Classifiers&lt;/h1&gt;

&lt;p&gt;For a binary classification problem, we actually have no need to know the specfic value of \(P(y=1\vert x)\) and \(P(y=0\vert x)\). What matters is whether \(P(y=1\vert x)&amp;gt;P(y=0\vert x)\) or not. Unlike discriminant methods which model and compute the posterior probability directly, in &lt;em&gt;generative classifiers&lt;/em&gt;, we compare the posterior probability in an indirect way. Specifically, by &lt;em&gt;Bayes’s theorem&lt;/em&gt;, we have&lt;/p&gt;

\[P(Y|X)=\frac{P(X|Y)P(Y)}{P(X)}\propto P(X|Y)P(Y).\]

&lt;p&gt;Therefore, to compare the posterior probability is to compare the union probability. The classification predicted by generative classifiers is&lt;/p&gt;

\[\hat y=\arg\max_{y\in\{0,1\}}P(y\vert x)=\arg\max_{y\in\{0,1\}}P(x\vert y)P(y).\]

&lt;p&gt;In generative classifier methods, a key problem is how to model the likelihood \(P(x\vert y)\) and the prior $P(y)$.&lt;/p&gt;

&lt;h2 id=&quot;41-naive-bayes-classifier&quot;&gt;4.1. Naive Bayes Classifier&lt;/h2&gt;

&lt;p&gt;Naive Bayes classifier is the simplest generative classifier. For a binary classification problem, suppose the feature of $x_i$ is composed of $(x_{i1},x_{i2},\dots,x_{id})$. Then naive Bayes classifier assumes not only the independence among the data but also that &lt;em&gt;every pair of the feature is independent&lt;/em&gt;, &lt;em&gt;i.e.,&lt;/em&gt;&lt;/p&gt;

\[x_{im}\vert y_i\perp x_{in}\vert y_i,m,n=1,2,\dots,d \text{ and }m\ne n.\]

&lt;p&gt;Then the likelihood becomes&lt;/p&gt;

\[P(x_i\vert y_i)=\prod_{j=1}^dP(x_{ij}\vert y_i).\]

&lt;p&gt;Further, it models the prior and each feature as,&lt;/p&gt;

\[y_i\sim\text{Bern}(\phi),\quad x_{ij}\vert y_i\sim\mathcal{N}(\mu_{j},\sigma_j^2),\]

&lt;p&gt;where $\phi,\mu_j$, and $\sigma_{j}$ are parameters that can be learned by MLE method. Note that such model is just a common case. The key idea of naive Bayes classifier is its independence assumption. Specifically, naive Bayes classifier is not a single method but a family of methods. By assuming the independence, it can be extremely fast compared with other classification methods.&lt;/p&gt;

&lt;h2 id=&quot;42-gaussian-discriminant-analysis&quot;&gt;4.2. Gaussian Discriminant Analysis&lt;/h2&gt;

&lt;p&gt;As a generative method, &lt;em&gt;Gaussian discriminant analysis&lt;/em&gt; (GDA) models the prior and the likelihood as follows,&lt;/p&gt;

\[y\sim\text{Bern}(\phi),\quad x\vert y=0\sim\mathcal{N}(\mu_1,\Sigma),\quad x\vert y=1\sim\mathcal{N}(\mu_2,\Sigma),\]

&lt;p&gt;where \(\phi,\mu_1,\mu_2\), and \(\Sigma\) are parameters to be learned. We define $w=(\phi, \mu_1,\mu_2,\Sigma)$. Then GDA is to find $\hat w$ that maximizes&lt;/p&gt;

\[\begin{aligned}\mathcal{J}(w)&amp;amp;=\log \prod_{i=1}^N P(x_i\vert y_i;\mu_1,\mu_2,\Sigma)P(y_i;\phi)\\&amp;amp;=\sum_{i=1}^N\left(\log P(x_i\vert y_i;\mu_1,\mu_2,\Sigma)+\log P(y_i;\phi)\right)\end{aligned}.\]

&lt;p&gt;Similar to the case in section 3.1, we represent the likelihood and the prior as&lt;/p&gt;

\[P(x_i\vert y_i;\mu_1,\mu_2,\Sigma)=\rho^{y_i}(\mu_1,\Sigma)\rho^{1-y_i}(\mu_2,\Sigma),\quad P(y_i;\phi)=\phi^{y_i}(1-\phi)^{1-y_i},\]

&lt;p&gt;where $\rho(\mu_1,\Sigma)$ and $\rho(\mu_2,\Sigma)$ are the PDF of \(x\vert y=0\) and \(x\vert y=1\), respectively. Then it follows that&lt;/p&gt;

\[\begin{aligned}\mathcal{J}(w)&amp;amp;=\sum_{i=1}^N\left(\log\rho^{y_i}(\mu_1,\Sigma)\rho^{1-y_i}(\mu_2,\Sigma)+\log\phi^{y_i}(1-\phi)^{1-y_i}\right)\\&amp;amp;=\sum_{i=1}^N\left(y_i\log \rho(\mu_1,\Sigma)+(1-y_i)\log\rho(\mu_2,\Sigma)+y_i\log\phi+(1-y_i)\log(1-\phi)\right)\end{aligned}.\]

&lt;p&gt;Then to find \(\hat w\) that maximizes $\mathcal{J}(w)$ is equivalent to set the derivation of $\mathcal{J}(w)$ w.r.t $w$ to be zero.&lt;/p&gt;

&lt;p&gt;For $\hat \phi$:&lt;/p&gt;

\[\begin{aligned}\frac{\partial \mathcal{J}(w)}{\partial\phi}=\sum_{i=1}^N\left(\frac{y_i}{\phi}-\frac{1-y_i}{1-\phi}\right)\end{aligned}.\]

&lt;p&gt;Solving \(\partial \mathcal{J}(w)/\partial\phi=0\), we have&lt;/p&gt;

\[\hat \phi=\frac{1}{N}\sum_{i=1}^N y_i=\frac{N_1}{N}.\]

&lt;p&gt;For $\hat\mu_1$ (or \(\hat\mu_2\) likewise):&lt;/p&gt;

\[\begin{aligned}\frac{\partial\mathcal{J}(w)}{\partial\mu_1}&amp;amp;=\frac{\partial\left(\sum_{i=1}^N y_i\log \rho(\mu_1,\Sigma)\right)}{\partial\mu_1}\\&amp;amp;=\frac{\partial\left(\sum_{i=1}^Ny_i\log\left(\frac{1}{(2\pi)^{d/2}\vert\Sigma\vert^{1/2}}\exp\left(-\frac{1}{2}(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)\right)\right)\right)}{\partial\mu_1}\\&amp;amp;=\frac{\partial\left(-\frac{1}{2}\sum_{i=1}^Ny_i(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)\right)}{\partial\mu_1}\\&amp;amp;=\frac{\partial\left(-\frac{1}{2}\sum_{i=1}^Ny_i(x_i^T\Sigma^{-1}x_i-x_i^T\Sigma^{-1}\mu_1-\mu_1^T\Sigma^{-1}x_i+\mu_1^T\Sigma^{-1}\mu_1)\right)}{\partial\mu_1}\\&amp;amp;=\sum_{i=1}^Ny_i\left(\Sigma^{-1}\mu_1-\Sigma^{-1}x_i\right)\end{aligned}.\]

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf&quot;&gt;The Matrix Cookbook&lt;/a&gt;:&lt;/p&gt;

\[\frac{\partial x^Ta}{\partial x}=\frac{\partial a^Tx}{\partial x}=a.\]

\[\frac{\partial x^TBx}{\partial x}=(B+B^T)x.\]
&lt;/blockquote&gt;

&lt;p&gt;Solving \(\partial \mathcal{J}(w)/\partial\mu_1=0\), we have&lt;/p&gt;

\[\hat \mu_1=\frac{\sum_{i=1}^N y_ix_i}{\sum_{i=1}^N y_i}=\bar x_{c1}.\]

&lt;p&gt;Similarly, we have&lt;/p&gt;

\[\hat\mu_2=\bar x_{c2}.\]

&lt;p&gt;For $\hat\Sigma$, we first consider the following transformation&lt;/p&gt;

\[\begin{aligned}\sum_{i=1}^N\left(y_i\log \rho(\mu_1,\Sigma)+(1-y_i)\log\rho(\mu_2,\Sigma)\right)&amp;amp;=\sum_{x\in X_{c1}}\log\rho(\mu_1,\Sigma)+\sum_{x\in X_{c2}}\log\rho(\mu_2,\Sigma)\end{aligned}.\]

&lt;p&gt;As $\Sigma$ is shared by both \(\rho(\mu_1,\Sigma)\) and \(\rho(\mu_2,\Sigma)\), we consider the expansion of $\rho(\mu_1,\Sigma)$ for example,&lt;/p&gt;

\[\begin{aligned}\sum_{x\in X_{c1}}\log\rho(\mu_1,\Sigma)&amp;amp;=\sum_{x\in X_{c1}}\log\left(\frac{1}{(2\pi)^{d/2}\vert\Sigma\vert^{1/2}}\exp\left(-\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\right)\right)\\&amp;amp;=\underbrace{\sum_{x\in X_{c1}}-\frac{d}{2}\log2\pi}_{\text{constant }\lambda_1}-\sum_{x\in X_{c1}}\left(\frac{1}{2}\log\vert \Sigma\vert+\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\right)\\&amp;amp;=\lambda_1-\frac{N_1}{2}\log\vert\Sigma\vert-\frac{1}{2}\sum_{x\in X_{c1}}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\end{aligned}.\]

&lt;p&gt;For the third term, notice that $(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\in\mathbb{R}$, thus&lt;/p&gt;

\[\begin{aligned}\sum_{x\in X_{c1}}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)&amp;amp;=\text{tr}\left(\sum_{x\in X_{c1}}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\right)\\&amp;amp;=\text{tr}\left(\sum_{x\in X_{c1}}(x-\mu_1)(x-\mu_1)^T\Sigma^{-1}\right)\\&amp;amp;=N_1\text{tr}\left(S_{c1}\Sigma^{-1}\right)\end{aligned}.\]

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf&quot;&gt;The Matrix Cookbook&lt;/a&gt;:&lt;/p&gt;

\[\text{tr}(ABC)=\text{tr}(CAB)=\text{tr}(BCA).\]

\[\frac{\partial\text{tr}(AB)}{\partial A}=B^T.\]

\[\frac{\partial \vert A\vert}{\partial A}=\vert A\vert A^{-1}.\]
&lt;/blockquote&gt;

&lt;p&gt;Hence,&lt;/p&gt;

\[\sum_{x\in X_{c1}}\log\rho(\mu_1,\Sigma)=\lambda_1-\frac{N_1}{2}\log\vert\Sigma\vert-\frac{N_1}{2}\text{tr}\left(S_{c1}\Sigma^{-1}\right).\]

&lt;p&gt;Similarly,&lt;/p&gt;

\[\sum_{x\in X_{c2}}\log\rho(\mu_2,\Sigma)=\lambda_2-\frac{N_2}{2}\log\vert\Sigma\vert-\frac{N_2}{2}\text{tr}\left(S_{c2}\Sigma^{-1}\right).\]

&lt;p&gt;Then it follows that&lt;/p&gt;

\[\begin{aligned}\frac{\partial\mathcal{J}(w)}{\partial\Sigma}&amp;amp;=\frac{\partial \left(\sum_{i=1}^N\left(y_i\log \rho+(1-y_i)\log\rho\right)\right)}{\partial\Sigma}\\&amp;amp;=\frac{\partial \left(\sum_{x\in X_{c1}}\log\rho+\sum_{x\in X_{c2}}\log\rho\right)}{\partial\Sigma}\\&amp;amp;=-\frac{1}{2}\frac{\partial \left(N\log\vert\Sigma\vert+N_1\text{tr}\left(S_{c1}\Sigma^{-1}\right)+N_2\text{tr}\left(S_{c2}\Sigma^{-1}\right)\right)}{\partial\Sigma}\\&amp;amp;=-\frac{1}{2}\left(N\Sigma^{-1}-N_1S_{c1}\Sigma^{-2}-N_2S_{c2}\Sigma^{-2}\right)\end{aligned}.\]

&lt;p&gt;By setting \(\partial \mathcal{J}(w)/\partial\Sigma=0\), we finally arrive at&lt;/p&gt;

\[\hat\Sigma=\frac{N_1S_{c1}+N_2S_{c2}}{N}.\]

&lt;h1 id=&quot;5-conclusion&quot;&gt;5. Conclusion&lt;/h1&gt;

&lt;p&gt;In this post, we introduced five linear classifiers. Among these models, $\mathcal{L}(w)$ is to be minimized, while $\mathcal{J}(w)$ is to be maximized. We omitted the prediction part of a classification problem. What we focused is actually how to model these data, especially in those generative cases.&lt;/p&gt;

&lt;p&gt;This post is obviously a long story. Moreover, there are many other things stoped my writing occasionally these days. It definitely has some logical problems, let alone typos, to be fixed. Anyway, I made it. Hope next time I can do better.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Machine Learning - 02 Linear Regression</title>
   <link href="/2020/10/07/linear_regression-ml02/"/>
   <updated>2020-10-07T00:00:00+08:00</updated>
   <id>/2020/10/07/linear_regression-ml02</id>
   <content type="html">&lt;p&gt;&lt;em&gt;The notes are based on the &lt;a href=&quot;https://github.com/shuhuai007/Machine-Learning-Session&quot;&gt;session&lt;/a&gt;. Many thanks to the great work.&lt;/em&gt;&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#1-least-squares-method&quot; id=&quot;markdown-toc-1-least-squares-method&quot;&gt;1. Least Squares Method&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-least-squares-method---a-matrix-perspective&quot; id=&quot;markdown-toc-2-least-squares-method---a-matrix-perspective&quot;&gt;2. Least Squares Method - A Matrix Perspective&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3-least-squares-method---a-probabilistic-perspective&quot; id=&quot;markdown-toc-3-least-squares-method---a-probabilistic-perspective&quot;&gt;3. Least Squares Method - A Probabilistic Perspective&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#31-maximum-likelihood-estimation&quot; id=&quot;markdown-toc-31-maximum-likelihood-estimation&quot;&gt;3.1 Maximum Likelihood Estimation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#32-maximum-a-posteriori&quot; id=&quot;markdown-toc-32-maximum-a-posteriori&quot;&gt;3.2 Maximum A Posteriori&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#4-regularization&quot; id=&quot;markdown-toc-4-regularization&quot;&gt;4. Regularization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#5-conclusion&quot; id=&quot;markdown-toc-5-conclusion&quot;&gt;5. Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;1-least-squares-method&quot;&gt;1. Least Squares Method&lt;/h1&gt;

&lt;p&gt;Suppose we have the IID data \(\mathcal{D}=\{(x_1,y_1), (x_2,y_2),\dots,(x_N,y_N)\},\) where $x_i\in\mathbb{R}^{d\times 1}, y_i\in\mathbb{R}, i=1,2,\dots,N$. One can view $x_i$ as a feature vector and $y_i$ is the corresponding label. Denote&lt;/p&gt;

\[X=(x_1,x_2,\dots,x_N)^T=\begin{pmatrix}x_{11}&amp;amp;x_{12}&amp;amp;\dots&amp;amp;x_{1d}\\x_{21}&amp;amp;x_{22}&amp;amp;\dots&amp;amp;x_{2d}\\\vdots&amp;amp;\vdots&amp;amp;\ddots&amp;amp;\vdots\\x_{N1}&amp;amp;x_{12}&amp;amp;\dots&amp;amp;x_{1d}\end{pmatrix}_{N\times d},\quad Y=\begin{pmatrix}y_1\\y_2\\\vdots\\y_N\end{pmatrix}_{N\times 1}.\]

&lt;p&gt;The problem is given the data set, we need to find a function to fit these data while minimizing the sum of squared error. In this case, we only focus on linear function:&lt;/p&gt;

\[f(w)=w^Tx,\]

&lt;p&gt;where $w\in\mathbb{R}^{d\times 1}$ is the unknown weight we need to learn. Here we ignore the bias term as it can be represented by adding a new dimension to the variables. The &lt;em&gt;loss function&lt;/em&gt; of the least squares method is defined as&lt;/p&gt;

\[\mathcal{L}(w)=\sum_{i=1}^N||w^Tx_i-y_i||^2.\]

&lt;p&gt;In this case, $w^Tx_i\in\mathbb{R}$ and $y_i\in\mathbb{R}$. Thus we can expand the loss function as&lt;/p&gt;

\[\begin{aligned}\mathcal{L}(w)&amp;amp;=\underbrace{\begin{pmatrix}w^Tx_1-y_1&amp;amp;w^Tx_2-y_2&amp;amp;\dots&amp;amp;w^Tx_N-y_N\end{pmatrix}}_{w^T\begin{pmatrix}x_1&amp;amp;x_2&amp;amp;\dots&amp;amp;x_N\end{pmatrix}-\begin{pmatrix}y_1&amp;amp;y_2&amp;amp;\dots&amp;amp;y_N\end{pmatrix}}\begin{pmatrix}w^Tx_1-y_1\\w^Tx_2-y_2\\\vdots\\w^Tx_N-y_N\end{pmatrix}\\&amp;amp;=(w^TX^T-Y^T)(Xw-Y)\\&amp;amp;=w^TX^TXw-2w^TX^TY+Y^TY\end{aligned}.\]

&lt;p&gt;Such expansion is for the derivative of the loss. Thus we have the optimal weight where&lt;/p&gt;

\[\begin{alignat*}{3}\hat w&amp;amp;=\arg\min_w \mathcal{L}(w)\Rightarrow\frac{\partial \mathcal{L}(w)}{\partial w}&amp;amp;=0\end{alignat*}.\]

&lt;p&gt;Solving the equation, we have&lt;/p&gt;

\[\hat{w}=(X^TX)^{-1}X^TY.\]

&lt;h1 id=&quot;2-least-squares-method---a-matrix-perspective&quot;&gt;2. Least Squares Method - A Matrix Perspective&lt;/h1&gt;

&lt;p&gt;Knowledge about &lt;a href=&quot;http://math.mit.edu/~gs/linearalgebra/&quot;&gt;vectors and space&lt;/a&gt; is required in this section. We consider using a new approximation linear function&lt;/p&gt;

\[h(w)=Xw,\]

&lt;p&gt;where $w\in\mathbb{R}^{d\times 1}$ and it is quite similar to $f(w)$ we defined in section 1. One should keep in mind that a matrix multiple a vector yields a linear combination of the &lt;strong&gt;column vectors&lt;/strong&gt; of the matrix. We will use $X_1,X_2,\dots,X_N$ to represent the column vector of $X$. Ideally, we want to find the $w$ subject to&lt;/p&gt;

\[X w=Y.\]

&lt;p&gt;If such $w$ exists, then we can solve it directly and easily. However, in practice error is inevitable and such $w$ often does not exist, which means $Y$ is not a linear combination of $X_i$ and they do not share the same space. What we can do is finding a linear combination of $X_i$ so that it has the least Euclidean distance to $Y$.&lt;/p&gt;

&lt;p&gt;As I am too lazy to depict the picture, I would like to give an imaginable example for that. One can also get the picture easily with these examples. The following discussion is based on a three-dimensional space with $xyz$ axes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ideal case&lt;/strong&gt;: Suppose we have the data \(x_1, x_2, x_3 = \{(1,0,0)^T,(0,1,0)^T,(0,0,1)^T\}, Y=(1,1,1)^T\). Thus&lt;/p&gt;

\[X=(x_1,x_2, x_3)^T=\begin{pmatrix}1&amp;amp;0&amp;amp;0\\0&amp;amp;1&amp;amp;0\\0&amp;amp;0&amp;amp;1\end{pmatrix},\quad Y=\begin{pmatrix}1\\1\\1\end{pmatrix}.\]

&lt;p&gt;With the knowledge of vectors, it can be easily found that we can obtain $Y$ by $1\cdot X_1+1\cdot X_2+1\cdot X_3$, which gives the solution for $X\hat{w}=Y$ as $\hat{w}=(1,1,1)^T$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Practical case&lt;/strong&gt;: In practice, things may be different. Suppose we have $x_1, x_2, x_3 = \{(1,0.2,0.5)^T,(0,1,0)^T,(0,0,0)^T\}, Y=(1,1,1)^T$. Thus&lt;/p&gt;

\[X=(x_1,x_2, x_3)^T=\begin{pmatrix}1&amp;amp;0.2&amp;amp;0.5\\0&amp;amp;1&amp;amp;0\\0&amp;amp;0&amp;amp;0\end{pmatrix},\quad Y=\begin{pmatrix}1\\1\\1\end{pmatrix}.\]

&lt;p&gt;With a manual drafting, one can find there is no way to get $Y$ by the linear combination of $X_1,X_2$ and $X_3$: all the $X_i$ lies in $xy$ surface while $Y$ exists in $xyz$ space. In such case, what we can do is finding a line on $xy$ surface as close to $Y$ as possible. Obviously, the closest one is the projection of $Y$ on $xy$ surface, which is $(1,1,0)^T$ .&lt;/p&gt;

&lt;p&gt;The key is how to find the projection $X\hat{w}$ numerically rather than intuitively. Consider the vector $Y-X\hat{w}$, which starts from the end of $X\hat{w}$ pointing to the end of $Y$. $X\hat{w}$ is the projection if and only if $Y-X\hat w$ is perpendicular to all the column vectors of $X$. Therefore it follows that&lt;/p&gt;

\[\begin{aligned}X^T(Y-X\hat w)&amp;amp;=\mathbf{0}\\X^TX\hat w&amp;amp;=X^TY\\\hat w&amp;amp;=(X^TX)^{-1}X^TY,\end{aligned}\]

&lt;p&gt;which is consistent with our conclusion in section 1. Also, plugging the values of the example above, we have $\hat w=(0.64, 1, 0.32)^T$, thus $X\hat w=(1,1,0)^T$, which is consistent with our (imaginary) observation of the projection.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The term $(X^TX)^{-1}X^T$ is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse&quot;&gt;Moore–Penrose inverse&lt;/a&gt; of $X$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;3-least-squares-method---a-probabilistic-perspective&quot;&gt;3. Least Squares Method - A Probabilistic Perspective&lt;/h1&gt;

&lt;p&gt;As we mentioned before, error is inevitable in practice, otherwise there is no need to do such approximation.&lt;/p&gt;

&lt;h2 id=&quot;31-maximum-likelihood-estimation&quot;&gt;3.1 Maximum Likelihood Estimation&lt;/h2&gt;

&lt;p&gt;We now use Gaussian distribution to reflect such noise as $\varepsilon\sim\mathcal{N}(0,\sigma^2)$. With the definition in section 1, we need to find the $w$ subject to&lt;/p&gt;

\[y=w^Tx+\varepsilon.\]

&lt;p&gt;Obviously, we have $y|x;w\sim\mathcal{N}(w^Tx,\sigma^2)$. For $N$ samples, the log-likelihood follows that&lt;/p&gt;

\[\begin{aligned}\mathcal{L}(w)&amp;amp;=\log P(Y|X;w)\\&amp;amp;=\sum_{i=1}^N\log P(y_i|x_i;w)\\&amp;amp;=\sum_{i=1}^N\left(\log \frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{2\sigma^2}(y_i-w^Tx_i)^2\right)\end{aligned}.\]

&lt;p&gt;Therefore, by &lt;em&gt;maximum likelihood estimation&lt;/em&gt; (MLE) method, we have&lt;/p&gt;

\[\begin{aligned}\hat w&amp;amp;=\arg\max_w\mathcal{L}(w)\\&amp;amp;=\arg\max_w -\sum_{i=1}^N\frac{1}{2\sigma^2}(y_i-w^Tx_i)^2\\&amp;amp;=\arg\min_w\sum_{i=1}^N(y_i-w^Tx_i)^2\end{aligned},\]

&lt;p&gt;which is consistent with our analysis in section 1. In fact, least squares method has an assumption that the noise is subject to Gaussian distribution.&lt;/p&gt;

&lt;h2 id=&quot;32-maximum-a-posteriori&quot;&gt;3.2 Maximum A Posteriori&lt;/h2&gt;

&lt;p&gt;As we mentioned in the &lt;a href=&quot;https://2ez4ai.github.io/2020/09/28/intro-ml01/&quot;&gt;previous notes&lt;/a&gt;, in the view of Bayesians, $w$ can also be a random variable. Suppose $w\sim\mathcal{N}(0,\sigma_0^2)$. Still, we have $y\vert x;w\sim\mathcal{N}(w^Tx,\sigma^2)$ (there is a little abuse of notation: $w$ after ‘$|$’ is a sample rather than a random variable). By &lt;em&gt;maximum a posteriori&lt;/em&gt; (MAP) method, we have&lt;/p&gt;

\[\begin{aligned}\hat w&amp;amp;=\arg\max_w P(w|Y)\\&amp;amp;=\arg\max_w\log\left(\frac{\prod_{i=1}^NP(y_i|w)\cdot P(w)}{\prod_{i=1}^NP(y_i)}\right)\\&amp;amp;=\arg\max_w\log\left(\prod_{i=1}^NP(y_i|w)\right)+\log P(w)\\&amp;amp;=\arg\max_w\sum_{i=1}^N\log\left(\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}}\right)+\log\left(\frac{1}{\sqrt{2\pi}\sigma_0}e^{-\frac{||w||^2}{2\sigma_0^2}}\right)\\&amp;amp;=\arg\min_w\sum_{i=1}^N\frac{(y_i-w^Tx_i)^2}{2\sigma^2}+\frac{||w||^2}{2\sigma_0^2}\\&amp;amp;=\arg\min_w\underbrace{\sum_{i=1}^N(y_i-w^Tx_i)^2}_\text{square error}+\underbrace{\frac{\sigma^2}{\sigma_0^2}||w||^2}_\text{regularizer}\end{aligned},\]

&lt;p&gt;which is slightly different from the result of MLE method. However, we will show in next section this is actually equivalent to the &lt;em&gt;regularized least squares method&lt;/em&gt;.&lt;/p&gt;

&lt;h1 id=&quot;4-regularization&quot;&gt;4. Regularization&lt;/h1&gt;

&lt;p&gt;In practice, a common issue is $N\ll d$, which may cause $X^TX$ not invertible and further lead to &lt;em&gt;overfitting&lt;/em&gt;. There are three techniques to avoid overfitting: collecting more data, &lt;em&gt;feature engineering/extracting&lt;/em&gt;, and &lt;em&gt;regularization&lt;/em&gt;. The regularization method is adding a penalty term, and we have the new loss function $\mathcal{L}_r(w)=\mathcal{L}(w)+\lambda P(w)$ where $\lambda$ is a tunable parameter. Depending on what $P(w)$ is, we have different regression.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lasso regression&lt;/strong&gt;: use $L1$ norm as the penalty, which means $P(w)=||w||$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ridge regression&lt;/strong&gt;: use $L2$ norm as the penalty, which means $P(w)=||w||_2^2=w^Tw.$&lt;/p&gt;

&lt;p&gt;Such regularization is often called &lt;em&gt;weight decay&lt;/em&gt;. We now focus on ridge regression. According the above definition, we have&lt;/p&gt;

\[\begin{aligned}\mathcal{L}_r(w)&amp;amp;=\mathcal{L}(w)+\lambda P(w)\\&amp;amp;=\sum_{i=1}^N||w^Tx_i-y_i||^2+\lambda w^Tw\\&amp;amp;=w^TX^TXw-2w^TX^TY+Y^TY+\lambda w^Tw\\&amp;amp;=w^T(X^TX+\lambda\mathbf{I})w-2w^TX^TY+Y^TY\end{aligned}.\]

&lt;p&gt;Therefore, we have the optimal weight where&lt;/p&gt;

\[\begin{alignat*}{3}\hat w&amp;amp;=\arg\min_w \mathcal{L}_r(w)\Rightarrow\frac{\partial \mathcal{L}_r(w)}{\partial w}&amp;amp;=0\end{alignat*}.\]

&lt;p&gt;Solving the equation, we have&lt;/p&gt;

\[\hat{w}=(X^TX+\lambda\mathbf{I})^{-1}X^TY.\]

&lt;p&gt;By introducing the positive definite matrix $\lambda\mathbf{I}$, the problem of the invertible matrix $X^TX$ is avoided.&lt;/p&gt;

&lt;h1 id=&quot;5-conclusion&quot;&gt;5. Conclusion&lt;/h1&gt;

&lt;p&gt;Though linear regression is a naive model of machine learning, the thought of it is inspiring. In this post, we show that least squares is equivalent to MLE method with Gaussian noise in data, while the least squares with $L2$ regularizer is equivalent to MAP method with Gaussian noise in both weight and data.&lt;/p&gt;

&lt;p&gt;Based on the attributes of linear regression, thoughts of many machine learning models can be derived:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Linearity: as its name suggests, the linear regression method exploits linear functions to fit the data. Unsurprisingly, such linearity has a limited performance in general. Hence, there are many models developed from breaking the linearity. Examples:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Polynomial Regression&lt;/strong&gt; is a form of linear regression in which we convert the original features into their higher order terms. For example, transforms $x=(x_1,x_2)$ into $\tilde x=(x_1,x_2,x_1x_2,x_2^3)$.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Logistic Regression&lt;/strong&gt;: introduces a sigmoid function to the linear function, &lt;em&gt;e.g.&lt;/em&gt; $f(x)=\text{sigmoid}(w^Tx)$.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Neural Network&lt;/strong&gt; introduces multiple nonlinear functions and brings the multi-layer structure.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Global Space: the approximation we found by linear regression is applied to the whole space. However, in practice, the data may not be continuous, and we need different approximations for different space.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Decision Tree&lt;/strong&gt; divides the space into smaller sub-spaces depending on the question.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Raw Data: the basic linear regression utilizes all the given data, which incurs the &lt;em&gt;curse of dimensionality&lt;/em&gt;. In this case, dimensionality reduction methods are necessary.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;PCA&lt;/strong&gt; transforms the variables of the higher dimension into a smaller ones that still contain most of the information in the original data set.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Machine Learning - 01 Introduction</title>
   <link href="/2020/09/28/intro-ml01/"/>
   <updated>2020-09-28T00:00:00+08:00</updated>
   <id>/2020/09/28/intro-ml01</id>
   <content type="html">&lt;p&gt;&lt;em&gt;The notes are based on the &lt;a href=&quot;https://github.com/shuhuai007/Machine-Learning-Session&quot;&gt;session&lt;/a&gt;. Many thanks to the great work.&lt;/em&gt;&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#0-motivation&quot; id=&quot;markdown-toc-0-motivation&quot;&gt;0. Motivation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#1-frequentist-vs-bayesian&quot; id=&quot;markdown-toc-1-frequentist-vs-bayesian&quot;&gt;1. Frequentist vs Bayesian&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2maximum-likelihood-estimation-on-gaussian-distribution&quot; id=&quot;markdown-toc-2maximum-likelihood-estimation-on-gaussian-distribution&quot;&gt;2.Maximum Likelihood Estimation on Gaussian Distribution&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3-gaussian-distribution&quot; id=&quot;markdown-toc-3-gaussian-distribution&quot;&gt;3. Gaussian Distribution&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#4-calculations-on-gaussian&quot; id=&quot;markdown-toc-4-calculations-on-gaussian&quot;&gt;4. Calculations on Gaussian&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#41-calculations-on-union-distribution&quot; id=&quot;markdown-toc-41-calculations-on-union-distribution&quot;&gt;4.1. Calculations on Union Distribution&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#42-calculations-on-marginal-and-conditional-distribution&quot; id=&quot;markdown-toc-42-calculations-on-marginal-and-conditional-distribution&quot;&gt;4.2 Calculations on Marginal and Conditional Distribution&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#5-conclusion&quot; id=&quot;markdown-toc-5-conclusion&quot;&gt;5. Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;0-motivation&quot;&gt;0. Motivation&lt;/h1&gt;

&lt;p&gt;After setting up this blog, I actually had no idea about what should I post. I am definitely not good at writing, but still I enjoy it a lot. As I am weak in machine learning and mathematics, for writing and learning, a ‘&lt;em&gt;translation&lt;/em&gt;’ work of the &lt;a href=&quot;https://github.com/shuhuai007/Machine-Learning-Session&quot;&gt;session&lt;/a&gt; may be a good start. So here we are!&lt;/p&gt;

&lt;h1 id=&quot;1-frequentist-vs-bayesian&quot;&gt;1. Frequentist vs Bayesian&lt;/h1&gt;

&lt;p&gt;Many machine learning methods can be illustrated in a &lt;em&gt;probabilistic&lt;/em&gt; way. We now introduce two mainstream views of interpreting probability: &lt;strong&gt;frequentist&lt;/strong&gt; and &lt;strong&gt;bayesian&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Firstly let us consider a simple example. Suppose we have a data $X=(x_1,x_2,\ldots,x_N)^T$ where $x_i$ are i.i.d. samples of the random variable $x$ and is of $d$ dimensions. Also, we define $\theta$ be the parameter so that $x\sim P(x|\theta)$, and we call $P(X|\theta)$ the &lt;em&gt;likelihood&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Frequentists&lt;/strong&gt;: $\theta$ is an unknown &lt;em&gt;constant&lt;/em&gt;. What they care is how to use the data to infer the value of $\theta$. The best known approach for that is &lt;strong&gt;maximum likelihood estimation&lt;/strong&gt; (MLE):&lt;/p&gt;

\[\hat{\theta}_\text{MLE}=\arg\max_{\theta}P(X|\theta).\]

&lt;blockquote&gt;
  &lt;p&gt;Many traditional machine learning methods are based on this view. They construct a probabilistic model, then determine a loss function and minimize it by methods like gradient descent.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Bayesians&lt;/strong&gt;: $\theta$ is random variable, following distribution $P(\theta)$, which is called &lt;em&gt;prior distribution&lt;/em&gt;, and $P(\theta|X)$ is called &lt;em&gt;posterior distribution&lt;/em&gt;. Then by &lt;em&gt;Bayes’s theorem&lt;/em&gt;, we have &lt;strong&gt;maximum a posteriori&lt;/strong&gt; (MAP):&lt;/p&gt;

\[\hat{\theta}_\text{MAP}=\arg\max_{\theta}P(\theta|X).\]

&lt;blockquote&gt;
  &lt;p&gt;In Bayesian inference, the posterior distribution, rather than the maximum, is the key. However, to determine the distribution sometimes is really hard as it may incur complex calculus:&lt;/p&gt;

\[P(\theta|X)=\frac{P(X|\theta)\cdot P(\theta)}{\int_{\theta}P(X|\theta)\cdot P(\theta)\text{d}\theta}\]

  &lt;p&gt;Many powerful and elegant approaches were proposed for this. For examples, MCMC, probabilistic graphical model, &lt;em&gt;etc&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I’d like to cite &lt;a href=&quot;https://www.probabilisticworld.com/frequentist-bayesian-approaches-inferential-statistics/&quot;&gt;this&lt;/a&gt; to summary the differences.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“In short, according to the frequentist definition of probability, only repeatable random events (like the result of flipping a coin) have probabilities. These probabilities are equal to the long-term frequency of occurrence of the events in question. Frequentists don’t attach probabilities to hypotheses or to any fixed but unknown values in general.”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“In contrast, Bayesians view probabilities as a more general concept. As a Bayesian, you can use probabilities to represent the uncertainty in any event or hypothesis. Here, it’s perfectly acceptable to assign probabilities to non-repeatable events.”&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;2maximum-likelihood-estimation-on-gaussian-distribution&quot;&gt;2.Maximum Likelihood Estimation on Gaussian Distribution&lt;/h1&gt;

&lt;p&gt;In this section, we will give an example of MLE on Gaussian distribution. Just like what we defined in section 1, there is a data $X=(x_1,x_2,\ldots,x_N)^T$ where $x_i$ are i.i.d. samples of the random variable $x$ and is of $d=1$  dimension. Thus we have $X\in\mathbb{R}^{N\times 1}$. We further attach a specific distribution to $x$ as $x\sim\mathcal{N}(\mu,\sigma^2)$. The unknown parameter then becomes $\theta=(\mu,\sigma)$. Now we use MLE method to estimate the parameter:&lt;/p&gt;

\[\hat{\theta}_\text{MLE}=\arg\max_\theta P(X|\theta)=\arg\max_\theta \log P(X|\theta).\]

&lt;blockquote&gt;
  &lt;p&gt;Given $x\sim\mathcal{N}(\mu,\sigma^2)$, the PDF of $x$ is&lt;/p&gt;

\[p(x|\mu,\sigma)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}.\]
&lt;/blockquote&gt;

&lt;p&gt;As $x_i,i=1,\ldots,N$ are i.i.d. variables, it follows that&lt;/p&gt;

\[P(X|\theta)=\prod_{i=1}^NP(x_i|\theta).\]

&lt;p&gt;Hence,&lt;/p&gt;

\[\begin{aligned}\arg\max_\theta\log P(X|\theta)&amp;amp;=\arg\max_\theta\log \prod_{i=1}^N P(x_i|\theta)\\&amp;amp;=\arg\max_\theta\sum_{i=1}^N\log P(x_i|\theta)\\&amp;amp;=\arg\max_\theta\sum_{i=1}^N\log\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\\&amp;amp;=\arg\max_\theta\sum_{i=1}^N\left[\log \frac{1}{\sqrt{2\pi}}+\log\frac{1}{\sigma}-\frac{(x_i-\mu)^2}{2\sigma^2}\right]\\&amp;amp;=\arg\max_\theta\sum_{i=1}^N\left[\log\frac{1}{\sigma}-\frac{(x_i-\mu)^2}{2\sigma^2}\right]\end{aligned},\]

&lt;p&gt;which is equivalent to a numerical optimization problem. Denote the term $\sum_{i=1}^N\left[\log\frac{1}{\sigma}-\frac{(x_i-\mu)^2}{2\sigma^2}\right]$ as $\mathcal{L}$. The maximum occurs when&lt;/p&gt;

\[\frac{\partial\mathcal{L}}{\partial\mu}=0,\quad \frac{\partial\mathcal{L}}{\partial\sigma}=0.\]

&lt;p&gt;Solving the equations we have&lt;/p&gt;

\[\mu_\text{MLE}=\frac{1}{N}\sum_{i=1}^N x_i,\quad \sigma^2_\text{MLE}=\frac{1}{N}\sum_{i=1}^N(x_i-\mu_\text{MLE})^2.\]

&lt;blockquote&gt;
  &lt;p&gt;$\mu_\text{MLE}$ is an &lt;em&gt;unbiased estimation&lt;/em&gt; as $\mathbb{E}[\mu_\text{MLE}]=\mu$, while $\sigma_\text{MLE}^2$ is a &lt;em&gt;biased estimation&lt;/em&gt; since $\mathbb{E}[\sigma_\text{MLE}^2]=\frac{N-1}{N}\sigma^2$. The bias is incurred by the exploitation on samples rather than the true distribution. Intuitively, the variance of the samples will never be larger than the true distribution since they are totally generated from that!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;3-gaussian-distribution&quot;&gt;3. Gaussian Distribution&lt;/h1&gt;

&lt;p&gt;Many advanced machine learning techniques, like &lt;em&gt;linear Gaussian model&lt;/em&gt;, &lt;em&gt;Kalman filter&lt;/em&gt;, &lt;em&gt;P-PCA&lt;/em&gt;, &lt;em&gt;etc&lt;/em&gt;, involve Gaussian distribution. Therefore it is quite practical to get familiar with Gaussian distribution. In this section, we are trying to explain why the PDF of bivariate Gaussian distribution is shaped by numerous ellipses.&lt;/p&gt;

&lt;p&gt;Suppose we have a random variable $x\sim \mathcal{N}(\mu,\Sigma)$ of $d$ dimensions. Specifically, the definition follows that&lt;/p&gt;

\[P(x|\mu,\Sigma)=\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)},\]

&lt;p&gt;and&lt;/p&gt;

\[x=\begin{pmatrix}x_{1}\\x_{2}\\\vdots\\x_{d}\end{pmatrix}_{d\times1},\quad \mu=\begin{pmatrix}\mu_1\\\mu_2\\\vdots\\\mu_{d}\end{pmatrix}_{d\times1},\quad\Sigma=\begin{pmatrix}\sigma_{11}&amp;amp;\sigma_{12}&amp;amp;\cdots&amp;amp;\sigma_{1d}\\\sigma_{21}&amp;amp;\sigma_{22}&amp;amp;\cdots&amp;amp;\sigma_{2d}\\\vdots&amp;amp;\vdots&amp;amp;\ddots&amp;amp;\vdots\\\sigma_{d1}&amp;amp;\sigma_{d2}&amp;amp;\cdots&amp;amp;\sigma_{dd}\end{pmatrix}_{d\times d}.\]

&lt;p&gt;As it involves matrix, a simplification will be a wise choice. For simplicity,  we use $\Delta(x)$ to represent $(x-\mu)^T\Sigma^{-1}(x-\mu)$ and suppose the covariance matrix $\Sigma$ to be positive definite, which is reasonable. By the eigenvalue decomposition and the property of positive definite matrix, $\Sigma$ can be decomposed as $\Sigma=U\Lambda U^T$, where $U=(u_1,u_2,\ldots,u_d)$ is an orthogonal matrix whose columns are the eigenvectors of $\Sigma$, and $\Lambda=\text{diag}(\lambda_1,\lambda_2,\ldots,\lambda_d)$ is a diagonal matrix whose entries are the eigenvalues of $\Sigma$. Then we have&lt;/p&gt;

\[\begin{aligned}\Sigma^{-1}&amp;amp;=(U\Lambda U^T)^{-1}\\&amp;amp;=(U^T)^{-1}\Lambda^{-1}U^{-1}\\&amp;amp;=U\Lambda^{-1}U^T\\&amp;amp;=(u_1,u_2,\ldots,u_d)\begin{pmatrix}\frac{1}{\lambda_1}&amp;amp;0&amp;amp;\cdots&amp;amp;0\\0&amp;amp;\frac{1}{\lambda_2}&amp;amp;\cdots&amp;amp;0\\\vdots&amp;amp;\vdots&amp;amp;\ddots&amp;amp;0\\0&amp;amp;0&amp;amp;\cdots&amp;amp;\frac{1}{\lambda_d}\end{pmatrix}\begin{pmatrix}u_1\\u_2\\\vdots\\u_d\end{pmatrix}\\&amp;amp;=\begin{pmatrix}\frac{u_1}{\lambda_1},\frac{u_2}{\lambda_2},\ldots,\frac{u_d}{\lambda_d}\end{pmatrix}\begin{pmatrix}u_1\\u_2\\\vdots\\u_d\end{pmatrix}\\&amp;amp;=\sum_{i=1}^d\frac{u_iu_i^T}{\lambda_i}\end{aligned}.\]

&lt;p&gt;Plugging it in $\Delta(x)$, we arrive at&lt;/p&gt;

\[\begin{aligned}\Delta(x)&amp;amp;=(x-\mu)^T\left(\sum_{i=1}^d\frac{u_iu_i^T}{\lambda_i}\right)(x-\mu)\\&amp;amp;=\sum_{i=1}^d(x-\mu)^Tu_i\frac{1}{\lambda_i}u_i^T(x-\mu)\end{aligned}.\]

&lt;p&gt;Before moving on, we take a pause to be acquainted with those notations. Specifically, $(x-\mu)\in\mathbb{R}^{d\times 1}$ as $x, \mu\in\mathbb{R}^{d\times 1}$. $u_i$ is the eigenvector of $\Sigma$ so $u_i\in\mathbb{R}^{d\times 1}$, while $\lambda_i$ is the eigenvalue of $\Sigma$ so $\lambda_i\in\mathbb{R}$. Therefore $(x-\mu)^Tu_i\in\mathbb{R}$, $u_i^T(x-\mu)\in\mathbb{R}$ and $\Delta(x)\in\mathbb{R}$. It should not be surprising. Recall that the PDF&lt;/p&gt;

\[P(x|\mu,\Sigma)=\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}\Delta(x)}\]

&lt;p&gt;represents a probability, which is definitely a real number. Now we introduce $Y=(y_1,y_2,\ldots,y_d)^T$ where $y_i=(x-\mu)^Tu_i$. Hence&lt;/p&gt;

\[\Delta(x)=\sum_{i=1}^dy_i\frac{1}{\lambda_i}y_i^T=\sum_{i=1}^d\frac{y_i^2}{\lambda_i}.\]

&lt;p&gt;&lt;strong&gt;The PDF of $x$ is rewritten as&lt;/strong&gt;&lt;/p&gt;

\[P(x|\mu,\Sigma)=\alpha e^{-\frac{1}{2}\sum_{i=1}^d y_i^2/\lambda_i},\]

&lt;p&gt;where $\alpha=\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}$ is a constant. Now suppose we are trying to determine where $P(x|\mu,\Sigma)=\beta$. To make it intuitive, we consider the case $d=2$, then it follows that&lt;/p&gt;

\[\begin{alignat*}{3}&amp;amp;&amp;amp; \alpha e^{-\frac{1}{2}\left(\frac{y_1^2}{\lambda_1}+\frac{y_2^2}{\lambda_2}\right)}&amp;amp;=\beta\\\Rightarrow&amp;amp;&amp;amp;\frac{y_1^2}{\lambda_1}+\frac{y_2^2}{\lambda_2}&amp;amp;=2(\ln\alpha-\ln\beta)\\\Rightarrow&amp;amp;&amp;amp;\frac{y_1^2}{a^2}+\frac{y_2^2}{b^2}&amp;amp;=1\end{alignat*},\]

&lt;p&gt;where $a^2=2\lambda_1(\ln\alpha-\ln\beta)$ and $b^2=2\lambda_2(\ln\alpha-\ln\beta)$, and we finally get &lt;strong&gt;a standard equation of ellipse&lt;/strong&gt; with respect to $y_1$ and $y_2$! Actually, the transformation $y_i=(x-\mu)^Tu_i$ is the projection of $x$ on $u_i$. Thus in the coordinate system represented by $x_1$ and $x_2$, the ellipse will be changed linearly hence the ellipse will not be so standard. For different $\beta$, we have ellipses with different major and minor axes, and that is why the graph of bivariate Gaussian distribution looks like it is composed of numerous concentration ellipses.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;PS.&lt;/em&gt; The above discussion leaves much correctness proof to be done.&lt;/p&gt;

&lt;h1 id=&quot;4-calculations-on-gaussian&quot;&gt;4. Calculations on Gaussian&lt;/h1&gt;

&lt;p&gt;In this section, all the random variables are Gaussian and we will not touch the complex PDF. Rather, we mainly focus on the statistics, the expectation and the variance, of Gaussian distribution.&lt;/p&gt;

&lt;h2 id=&quot;41-calculations-on-union-distribution&quot;&gt;4.1. Calculations on Union Distribution&lt;/h2&gt;

&lt;p&gt;Suppose $x\sim\mathcal{N}(\mu, \Sigma)$ is the union distribution of $x_a$ and $x_b$, &lt;em&gt;i.e.&lt;/em&gt; $x=(x_a,x_b)^T$. Then the definition follows that&lt;/p&gt;

\[x=\begin{pmatrix}x_a\\x_b\end{pmatrix}_{d\times1},\quad \mu=\begin{pmatrix}\mu_a\\\mu_b\end{pmatrix}_{d\times1},\quad\Sigma=\begin{pmatrix}\Sigma_{aa}&amp;amp;\Sigma_{ab}\\\Sigma_{ba}&amp;amp;\Sigma_{bb}\end{pmatrix}_{d\times d},\]

&lt;p&gt;where&lt;/p&gt;

\[x_a,\mu_a\in\mathbb{R}^{m\times 1},x_b,\mu_b\in\mathbb{R}^{n\times 1}, m+n=d,\]

\[\Sigma_{aa}\in\mathbb{R}^{m\times m}, \Sigma_{ab},\Sigma_{ba}^T\in\mathbb{R}^{m\times n}, \Sigma_{bb}\in\mathbb{R}^{n\times n}.\]

&lt;p&gt;The problem in this section is how to derive $P(x_a)$ and $P(x_b|x_a)$, or symmetrically, $P(x_b)$ and $P(x_a|x_b)$ given the union distribution.  The derivation of $P(x_a)$ is quite straightforward:&lt;/p&gt;

\[\begin{aligned}\mathbb{E}[x_a]&amp;amp;=\mathbb{E}\left[\begin{pmatrix}\mathbf{I}_{m\times m}&amp;amp;\mathbf{0}_{m\times n}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}\right]\\&amp;amp;=\begin{pmatrix}\mathbf{I}_{m\times m}&amp;amp;\mathbf{0}_{m\times n}\end{pmatrix}\mathbb{E}[x]\\&amp;amp;=\begin{pmatrix}\mathbf{I}_{m\times m}&amp;amp;\mathbf{0}_{m\times n}\end{pmatrix}\begin{pmatrix}\mu_a\\\mu_b\end{pmatrix}\\&amp;amp;=\mu_a\end{aligned},\]

&lt;p&gt;and&lt;/p&gt;

\[\begin{aligned}var[x_a]&amp;amp;=var\left[\begin{pmatrix}\mathbf{I}_{m\times m}&amp;amp;\mathbf{0}_{m\times n}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}\right]\\&amp;amp;=\begin{pmatrix}\mathbf{I}_{m\times m}&amp;amp;\mathbf{0}_{m\times n}\end{pmatrix}var[x]\begin{pmatrix}\mathbf{I}_{m\times m}\\\mathbf{0}_{n\times m}\end{pmatrix}\\&amp;amp;=\begin{pmatrix}\mathbf{I}_{m\times m}&amp;amp;\mathbf{0}_{m\times n}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&amp;amp;\Sigma_{ab}\\\Sigma_{ba}&amp;amp;\Sigma_{bb}\end{pmatrix}\begin{pmatrix}\mathbf{I}_{m\times m}\\\mathbf{0}_{n\times m}\end{pmatrix}\\&amp;amp;=\Sigma_{aa}\end{aligned}.\]

&lt;p&gt;Thus, we have $x_a\sim\mathcal{N}(\mu_a,\Sigma_{aa})$, and similarly, $x_b\sim\mathcal{N}(\mu_b,\Sigma_{bb})$. For the conditional probabilities $P(x_b|x_a)$, we first define variables&lt;/p&gt;

\[x_{b\cdot a}=x_b-\Sigma_{ba}\Sigma^{-1}_{aa}x_a,\]

\[\mu_{b\cdot a}=\mu_b-\Sigma_{ba}\Sigma^{-1}_{aa}\mu_a,\]

&lt;p&gt;and&lt;/p&gt;

\[\Sigma_{bb\cdot a}=\Sigma_{bb}-\Sigma_{ba}\Sigma^{-1}_{aa}\Sigma_{ab}.\]

&lt;p&gt;The term $\Sigma_{bb\cdot a}$ is also the &lt;em&gt;Schur complement&lt;/em&gt; of $\Sigma_{aa}$ of the matrix $\Sigma$. Then it follows that&lt;/p&gt;

\[\begin{aligned}\mathbb{E}[x_{b\cdot a}]&amp;amp;=\mathbb{E}\left[\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;amp;\mathbf{I}_{n\times n}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}\right]\\&amp;amp;=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;amp;\mathbf{I}_{n\times n}\end{pmatrix}\mathbb{E}[x]\\&amp;amp;=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;amp;\mathbf{I}_{n\times n}\end{pmatrix}\begin{pmatrix}\mu_a\\\mu_b\end{pmatrix}\\&amp;amp;=\mu_{b\cdot a}\end{aligned},\]

&lt;p&gt;and&lt;/p&gt;

\[\begin{aligned}var[x_{b\cdot a}]&amp;amp;=var\left[\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;amp;\mathbf{I}_{n\times n}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}\right]\\&amp;amp;=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;amp;\mathbf{I}_{n\times n}\end{pmatrix}var[x]\begin{pmatrix}-\Sigma_{aa}^{-1}\Sigma_{ba}^T\\\mathbf{I}_{n\times n}\end{pmatrix}\\&amp;amp;=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;amp;\mathbf{I}_{n\times n}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&amp;amp;\Sigma_{ab}\\\Sigma_{ba}&amp;amp;\Sigma_{bb}\end{pmatrix}\begin{pmatrix}-\Sigma_{aa}^{-1}\Sigma_{ba}^T\\\mathbf{I}_{n\times n}\end{pmatrix}\\&amp;amp;=\Sigma_{bb\cdot a}\end{aligned}.\]

&lt;p&gt;Hence we have $x_{b\cdot a}\sim\mathcal{N}(\mu_{b\cdot a},\Sigma_{bb\cdot a})$. Before showing the relation between $x_{b\cdot a}$ and $x_b|x_a$, we introduce a helpful theorem.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;:&lt;em&gt;If&lt;/em&gt; $x\sim\mathcal{N}(\mu,\Sigma)$, &lt;em&gt;then&lt;/em&gt; $Mx\perp Nx\iff M\Sigma N=\mathbf{0}.$&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; According to the property of Gaussian, $Mx\sim\mathcal{N}(M\mu,M\Sigma M^T)$ and $Nx\sim\mathcal{N}(N\mu,N\Sigma N^T)$. Further,&lt;/p&gt;

\[\begin{aligned}Cov(Mx,Nx)&amp;amp;=\mathbb{E}[(Mx-M\mu)(Nx-N\mu)^T]\\&amp;amp;=M\mathbb{E}[(x-\mu)(x-\mu)^T]N\\&amp;amp;=M\Sigma N^T.\end{aligned}\]

&lt;p&gt;For Gaussian distribution, uncorrelation implies independence. Therefore $M\Sigma N^T=\mathbf{0}\iff Mx\perp Nx.\tag*{$\blacksquare$}$&lt;/p&gt;

&lt;p&gt;Back to our case, we have&lt;/p&gt;

\[x_{b\cdot a}=\underbrace{\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;amp;\mathbf{I}\end{pmatrix}}_M\underbrace{\begin{pmatrix}x_a\\x_b\end{pmatrix}}_x,\qquad x_a=\underbrace{\begin{pmatrix}\mathbf{I}&amp;amp;\mathbf{0}\end{pmatrix}}_{N}\underbrace{\begin{pmatrix}x_a\\x_b\end{pmatrix}}_x.\]

&lt;p&gt;Check $M\Sigma N$:&lt;/p&gt;

\[\begin{aligned}M\Sigma N&amp;amp;=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;amp;\mathbf{I}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&amp;amp;\Sigma_{ab}\\\Sigma_{ba}&amp;amp;\Sigma_{bb}\end{pmatrix}\begin{pmatrix}\mathbf{I}\\\mathbf{0}\end{pmatrix}\\&amp;amp;=\begin{pmatrix}\mathbf{0}&amp;amp; \Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}\end{pmatrix}\begin{pmatrix}\mathbf{I}\\\mathbf{0}\end{pmatrix}\\&amp;amp;=\mathbf{0}\end{aligned}.\]

&lt;p&gt;Thus $x_{b\cdot a}\perp x_a$, which means $x_{b\cdot a}|x_a=x_{b\cdot a}$.  According to our definition, we finally arrive at&lt;/p&gt;

\[\begin{aligned}x_b|x_a&amp;amp;=x_{b\cdot a}|x_a+\Sigma_{ba}\Sigma_{aa}^{-1}x_a|x_a\\&amp;amp;=x_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a\end{aligned}.\]

&lt;p&gt;There is a little abuse of notation: the term $x_a$ after $|$ is a sample of $x_a$ rather than a random variable. One should notice that $x_a$ below is constant. Therefore,&lt;/p&gt;

\[\mathbb{E}[x_b|x_a]=\mathbb{E}[x_{b\cdot a}]+\Sigma_{ba}\Sigma_{aa}^{-1}x_a=\mu_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a,\]

&lt;p&gt;and&lt;/p&gt;

\[var[x_b|x_a]=var[x_{b\cdot a}]+0=\Sigma_{bb\cdot a}.\]

&lt;p&gt;Removing all the extra variables we defined, we have&lt;/p&gt;

\[x_b|x_a\sim\mathcal{N}(\mu_b+\Sigma_{ba}\Sigma_{aa}^{-1}(x_a-\mu_a), \Sigma_{bb}-\Sigma_{ba}\Sigma^{-1}_{aa}\Sigma_{ab}),\]

&lt;p&gt;and the case for $x_a|x_b$ is similar.&lt;/p&gt;

&lt;h2 id=&quot;42-calculations-on-marginal-and-conditional-distribution&quot;&gt;4.2 Calculations on Marginal and Conditional Distribution&lt;/h2&gt;

&lt;p&gt;In this section, the problem is deriving $P(y)$ and $P(x|y)$ given $P(x)$ and $P(y|x)$. For example, let’s say, $x\sim\mathcal{N}(\mu, \Sigma)$ and $y|x\sim\mathcal{N}(Ax+b,L^{-1})$, which is common in &lt;em&gt;linear Gaussian model&lt;/em&gt;. The abuse of notation here is the same as we mentioned above: the $x\ (y)$  in $y|x\ (x|y)$ represents a sample rather than a random variable. Now we define $y=Ax+b+\varepsilon$, where $\varepsilon\sim\mathcal{N}(0,L^{-1})$ is the noise in practice. Then we have&lt;/p&gt;

\[\mathbb{E}[y]=\mathbb{E}[Ax+b+\varepsilon]=A\mu+b,\]

&lt;p&gt;and&lt;/p&gt;

\[var[y]=var[Ax+b]+var[\varepsilon]=A\Sigma A^T+L^{-1}.\]

&lt;p&gt;Therefore $y\sim\mathcal{N}(A\mu+b, A\Sigma A^T+L^{-1})$. For the distribution of $x|y$, we can refer to the conclusion of section 4.1. Introducing a union variable $z=(x,y)^T$, we obtain&lt;/p&gt;

\[z\sim\mathcal{N}\left(\begin{pmatrix}\mu\\A\mu+b\end{pmatrix},\begin{pmatrix}\Sigma&amp;amp;Cov(x,y)\\Cov(y,x)&amp;amp;A\Sigma A^T+L^{-1}\end{pmatrix}\right).\]

&lt;p&gt;By the property of covariance, we have $Cov(y,x)=Cov(x,y)^T$. Further,&lt;/p&gt;

\[\begin{aligned}Cov(x,y)&amp;amp;=\mathbb{E}[(x-\mu)(y-A\mu-b)^T]\\&amp;amp;=\mathbb{E}[(x-\mu)(Ax+b+\varepsilon-A\mu-b)^T]\\&amp;amp;=\mathbb{E}[(x-\mu)(Ax-A\mu)^T]+\mathbb{E}[(x-\mu)\varepsilon^T]\\&amp;amp;=\mathbb{E}[(x-\mu)(x-\mu)^T]A^T+0\\&amp;amp;=var(x)A^T\\&amp;amp;=\Sigma A^T\end{aligned}.\]

&lt;p&gt;With the known union distribution $z=(x,y)^T$ and $P(y)$, we can derive $P(x|y)$ by the method mentioned in section 4.1.&lt;/p&gt;

&lt;h1 id=&quot;5-conclusion&quot;&gt;5. Conclusion&lt;/h1&gt;

&lt;p&gt;In this post, we talk a lot about Gaussian distribution as it plays an important role in machine learning. Also, we are trying to get familar with those mathematical things.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Introduction to Reinforcement Learning</title>
   <link href="/2020/09/21/intro-rl/"/>
   <updated>2020-09-21T00:00:00+08:00</updated>
   <id>/2020/09/21/intro-rl</id>
   <content type="html">&lt;p&gt;The introductory &lt;a href=&quot;https://github.com/19w6/Reinforcement_Learning_Notes&quot;&gt;notes&lt;/a&gt; included &lt;strong&gt;Bandit Algorithms&lt;/strong&gt;, &lt;strong&gt;MDP&lt;/strong&gt;, &lt;strong&gt;Model-free Methods&lt;/strong&gt;, &lt;strong&gt;Value Function Approximation&lt;/strong&gt;, &lt;strong&gt;Policy Optimization&lt;/strong&gt;. For the state-of-the-art advances, one can refer to paper directly and some excellent blogs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://2ez4ai.github.io/assets/rl/Reinforcement Learning Notes.pdf&quot;&gt;Reinforcement Learning Notes (an integration of the following sections)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://2ez4ai.github.io/assets/rl/Section 1 Introduction.pdf&quot;&gt;Section 1 Introduction&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://2ez4ai.github.io/assets/rl/Section 2 Probability.pdf&quot;&gt;Section 2 Probability&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://2ez4ai.github.io/assets/rl/Section 3 Bandit Algorithms.pdf&quot;&gt;Section 3 Bandit Algorithms&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://2ez4ai.github.io/assets/rl/Section 4 Markov Chains.pdf&quot;&gt;Section 4 Markov Chains&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://2ez4ai.github.io/assets/rl/Section 5 Markov Decision Process.pdf&quot;&gt;Section 5 Markov Decision Process&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://2ez4ai.github.io/assets/rl/Section 6 Model-Free Prediction.pdf&quot;&gt;Section 6 Model-Free Prediction&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://2ez4ai.github.io/assets/rl/Section 7 Model-Free Control.pdf&quot;&gt;Section 7 Model-Free Control&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://2ez4ai.github.io/assets/rl/Section 8 Value Function Approximation.pdf&quot;&gt;Section 8 Value Function Approximation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://2ez4ai.github.io/assets/rl/Section 9 Policy Gradient.pdf&quot;&gt;Section 9 Policy Gradient&lt;/a&gt;&lt;/p&gt;

</content>
 </entry>
 

</feed>
