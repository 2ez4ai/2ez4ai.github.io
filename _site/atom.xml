<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Jingye Wang</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2020-12-05T14:39:53+08:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Jingye Wang</name>
   <email>wangjy5@shanghaitech.edu.cn</email>
 </author>

 
 <entry>
   <title>Machine Learning - 09 Exact Inference of Graphical Models</title>
   <link href="http://localhost:4000/2020/12/06/inference-ml09/"/>
   <updated>2020-12-06T00:00:00+08:00</updated>
   <id>http://localhost:4000/2020/12/06/inference-ml09</id>
   <content type="html">&lt;p&gt;&lt;em&gt;The notes are based on the &lt;a href=&quot;https://github.com/shuhuai007/Machine-Learning-Session&quot;&gt;session&lt;/a&gt;, &lt;a href=&quot;https://ermongroup.github.io/cs228-notes/&quot;&gt;CS228-notes&lt;/a&gt; and &lt;a href=&quot;https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf&quot;&gt;PRML&lt;/a&gt;. For the fundamental of probability, one can refer to &lt;a href=&quot;https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view&quot;&gt;Introduction to Probability&lt;/a&gt;. Many thanks to these great works.&lt;/em&gt;&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#0-introduction&quot; id=&quot;markdown-toc-0-introduction&quot;&gt;0. Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#1-variable-elimination&quot; id=&quot;markdown-toc-1-variable-elimination&quot;&gt;1. Variable Elimination&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-belief-propagation&quot; id=&quot;markdown-toc-2-belief-propagation&quot;&gt;2. Belief Propagation&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#21-message&quot; id=&quot;markdown-toc-21-message&quot;&gt;2.1. Message&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#22-sum-product&quot; id=&quot;markdown-toc-22-sum-product&quot;&gt;2.2. Sum-Product&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#23-max-product&quot; id=&quot;markdown-toc-23-max-product&quot;&gt;2.3 Max-Product&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3-conclusion&quot; id=&quot;markdown-toc-3-conclusion&quot;&gt;3. Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;0-introduction&quot;&gt;0. Introduction&lt;/h1&gt;

&lt;p&gt;In the last &lt;a href=&quot;https://19w6.github.io/2020/11/29/probabilistic_graphical_models-ml08/&quot;&gt;post&lt;/a&gt;, we introduce graphical models which is capable of representing random variables and the conditional independences among them. We now consider the problem of inference in graphical models. Particularly, we wish to compute posterior distributions of one or more nodes contioned on some other known (observed) nodes, and the techniques we shall talk in this post are for &lt;em&gt;exact inference&lt;/em&gt;.&lt;/p&gt;

&lt;h1 id=&quot;1-variable-elimination&quot;&gt;1. Variable Elimination&lt;/h1&gt;

&lt;p&gt;We first consider the marginal inference in a &lt;em&gt;chain&lt;/em&gt; Bayesian network which has the following joint distribution,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x_1,x_2,\dots,x_n)=P(x_1)\prod_{i=2}^nP(x_i\vert x_{i-1}).&lt;/script&gt;

&lt;p&gt;Suppose we want to infer the marginal distribution $P(x_n)$. A naive way to do that is marginalizing $x_1,x_2,\dots,x_{n-1}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x_n)=\sum_{x_1}\sum_{x_2}\dots\sum_{x_{n-1}}P(x_1,x_2,\dots,x_{n-1}).&lt;/script&gt;

&lt;p&gt;However, as there are $n-1$ variables each with $k$ states, the computation needs to sum the probability over $k^{n-1}$ values and would scale exponentially with the length of the chain. To simplify the computation, we can leverage &lt;em&gt;variable elimination&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The key of &lt;em&gt;variable elimination&lt;/em&gt; is in &lt;em&gt;reagrranging the order&lt;/em&gt; of the summations and the multiplications. Specifically, the marginal distribution follows that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}P(x_n)&amp;=\sum_{x_1}\sum_{x_2}\dots\sum_{x_{n-1}}P(x_1)\prod_{i=2}^nP(x_i\vert x_{i-1})\\&amp;=\sum_{x_{n-1}}P(x_n\vert x_{n-1})\cdot \sum_{x_{n-2}}P(x_{n-1}\vert x_{n-2})\cdot\dots\cdot \sum_{x_{1}}P(x_{2}\vert x_{1})P(x_1).\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Such a rearrangement works because multiplication is distributive over addition,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;ab+ac=a(b+c),&lt;/script&gt;

&lt;p&gt;where the number of arithmetic operations are reduced from three (the left-hand side) to two (the right-hand side). Before we move on, we generalize the new expression to a Markov network since every Bayesian network can be tranformed into a Markov network. For the chain Bayesian network we considered, we can remove all the arrows of the graph to obtain a Markov network. The obtained Markov network would have maximum cliques &lt;script type=&quot;math/tex&quot;&gt;\{x_1,x_2\}&lt;/script&gt;, …, &lt;script type=&quot;math/tex&quot;&gt;\{x_{n-2},x_{n-1}\}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\{x_{n-1},x_{n}\}&lt;/script&gt; and the corresponding potentials are&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\psi_{1,2}(x_1,x_2)&amp;=P(x_2\vert x_1)P(x_1),\\&amp;\vdots\\\psi_{x_{n-2},x_{n-1}}(x_{n-2},x_{n-1})&amp;=P(x_{n-2}\vert x_{n-1}),\\\psi_{n-1,n}(x_{n-1},x_n)&amp;=P(x_{n-1}\vert x_n).\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The rearrangement for the Markov network then follows that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x_n)=\sum_{x_{n-1}}\psi_{n-1,n}(x_{n-1}, x_{n})\cdot \sum_{x_{n-2}}\psi_{n-2,n-1}(x_{n-2}, x_{n-1})\cdot\dots\cdot \sum_{x_{1}}\psi_{1,2}(x_1,x_2).&lt;/script&gt;

&lt;p&gt;Now the computation composes of $n-1$ summations. More importantly, unlike the previous one sums over $k^{n-1}$ values, this exression allows each term only need to sum over $k\times k$ values. Specifically, the sum&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{x_i}\psi_{x_i,x_{i+1}}(x_{i},x_{i+1})&lt;/script&gt;

&lt;p&gt;only involves two variables and thus the summation is over $k\times k$ values. Then the overall computation is of $O(nk^2)$ complexity, which is much better than the naive $O(k^n)$ method.&lt;/p&gt;

&lt;p&gt;However, the variable elimination (VE) method requires an ordering over the variables. In fact, the running time of VE on different orderings would vary greatly, while to find the best ordering is still an NP-hard problem. Moreover, VE method for $P(x_n)$ can be hard to be generalized to other marginal distribution as it does not store the intermediate results.&lt;/p&gt;

&lt;h1 id=&quot;2-belief-propagation&quot;&gt;2. Belief Propagation&lt;/h1&gt;

&lt;p&gt;For convenience, we consider undirected graphs with tree structure, where the optimal variable elimination ordering for node $x_i$ is the post-order iteration of the subtree rooted at $x_i$. The relationship between any two directly connected nodes is decided by which node the tree is rooted at and how far the two nodes are away from the root: the close one is the parent of the farther one.&lt;/p&gt;

&lt;h2 id=&quot;21-message&quot;&gt;2.1. Message&lt;/h2&gt;

&lt;p&gt;For a tree graph, its maximum cliques contains only two nodes. By VE algorithm, to compute the marginal $P(x_i)$, we need to eliminate all nodes that are in the subtree of $x_i$. For node $x_j$, the elimination involves computing $\sum_{x_j}\psi_{x_j,x_k}(x_j,x_k)m_{j,k}$ where $x_k$ is the parent of $x_j$ in the tree. The term $m_{j,k}$ can be thought of a &lt;em&gt;message&lt;/em&gt; that $x_j$ sends to $x_k$ about the subtree rooted at $x_j$. Similarly, the computing result can be viewed as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;m_{k,l}=\sum_{x_j}\psi_{x_j,x_k}(x_j,x_k)m_{j,k}&lt;/script&gt;

&lt;p&gt;that contains the information for $x_l$, the parent of $x_k$, about the subtree rooted at $x_k$. By doing so, at the end of VE, $x_i$ would receive messages from all of its immediate children and then marginalize them out to yield the final marginal.&lt;/p&gt;

&lt;p&gt;Suppose that after computing $P(x_i)$, we are interested in computing $P(x_k)$ as well. If we use VE algorithm again, we can find that the computation also involves the messages $m_{j,k}$ as node $x_k$ is still the parent of node $x_j$. Moreover, such a message is exactly the same as the one used in computing $P(x_i)$ since the graph structure does not change. Therefore, it is easy to find that if we store the intermediary messages of VE, we can obtain other marginals quickly.&lt;/p&gt;

&lt;h2 id=&quot;22-sum-product&quot;&gt;2.2. Sum-Product&lt;/h2&gt;

&lt;p&gt;Belief propagation can be viewed as a combination of VE and &lt;em&gt;caching&lt;/em&gt;. For each edge between $x_i$ and $x_j$, the messages passing on it are $m_{i,j}$ and $m_{j,i}$, which depends on the marginal we want to determine. After computing all these messages, one can compute any marginals with these messages.&lt;/p&gt;

&lt;p&gt;Belief propagation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Set a node, for example, node $x_i$, as the root;&lt;/li&gt;
  &lt;li&gt;For each $x_j$ in $N(x_i)$, &lt;em&gt;i.e.,&lt;/em&gt; the neighborhood of $x_i$, collect the messages sent to $x_i$:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;m_{j,i}=\sum_{x_j}\psi_{x_i,x_j}(x_i,x_j)\psi_{x_j}(x_j)\prod_{k\in N(x_j)\setminus i}m_{k,j};&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;For each $x_j$ in $N(x_i)$, collect the messages sent from $x_i$:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;m_{i,j}=\sum_{x_i}\psi_{x_j,x_i}(x_j,x_i)\psi_{x_i}(x_i)\prod_{k\in N(x_i)\setminus j}m_{k,i}.&lt;/script&gt;

&lt;p&gt;By doing so, we can obtain all the messages with &lt;script type=&quot;math/tex&quot;&gt;2\vert E\vert&lt;/script&gt; steps, where $E$ is the set of edges. Then for any marginal we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x_i)=\psi_i(x_i)\prod_{k\in N(x_i)}m_{k,i}.&lt;/script&gt;

&lt;h2 id=&quot;23-max-product&quot;&gt;2.3 Max-Product&lt;/h2&gt;

&lt;p&gt;We now consider a problem of finding the set of values that have the largest probability so that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\text{x}}=\arg\max_{\text{x}} P(\text{x}).&lt;/script&gt;

&lt;p&gt;Notice that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\max_{\text{x}} P(\text{x})=\max_{\text{x}_1}\dots \max_{\text{x}_n}P(\text{x}).&lt;/script&gt;

&lt;p&gt;By &lt;em&gt;sum-product&lt;/em&gt;, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\max_{\text{x}} P(\text{x})&amp;=\max_{x_1}\dots \max_{x_n}\psi_i(x_i)\prod_{k\in N(x_i)}m_{k,i}\\&amp;=\max_{x_n}\max_{x_n-1{}}\psi_{x_n,x_{n-1}}(x_n,x_{n-1})\max_{x_{n-2}}\psi_{x_{n-1},x_{n-2}}(x_{n-1},x_{n-2})\dots\max_{x_1}\psi_{x_2,x_1}(x_2,x_1)\psi_{x_1}(x_1).\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Such a method for maximizing &lt;em&gt;max-product&lt;/em&gt; is known as &lt;em&gt;max-product&lt;/em&gt; algorithm.&lt;/p&gt;

&lt;h1 id=&quot;3-conclusion&quot;&gt;3. Conclusion&lt;/h1&gt;

&lt;p&gt;In this post, we briefly introduced two algorithms for &lt;em&gt;exact inference&lt;/em&gt; in graphical models. Given a proper order of nodes, &lt;em&gt;variable elimination&lt;/em&gt; algorithm is efficient. However, the finding of the proper order is an NP-hard problem. Besides, each query of marginals needs running the algorithm, during which the computation can be highly redundant. To improve computing efficiency, &lt;em&gt;belif propagation&lt;/em&gt; stores the intermediate results as messages. After that, one can get any marginal by the messages. Moreover, we can also exploit those messages to determine the values of random varaibles with the largest probability, which is known as &lt;em&gt;max-product&lt;/em&gt;.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Machine Learning - 08 Probabilistic Graphical Models</title>
   <link href="http://localhost:4000/2020/11/29/probabilistic_graphical_models-ml08/"/>
   <updated>2020-11-29T00:00:00+08:00</updated>
   <id>http://localhost:4000/2020/11/29/probabilistic_graphical_models-ml08</id>
   <content type="html">&lt;p&gt;&lt;em&gt;The notes are based on the &lt;a href=&quot;https://github.com/shuhuai007/Machine-Learning-Session&quot;&gt;session&lt;/a&gt; and &lt;a href=&quot;https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf&quot;&gt;PRML&lt;/a&gt;. For the fundamental of probability, one can refer to &lt;a href=&quot;https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view&quot;&gt;Introduction to Probability&lt;/a&gt;. Many thanks to these great works.&lt;/em&gt;&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#0-introduction&quot; id=&quot;markdown-toc-0-introduction&quot;&gt;0. Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#1-bayesian-networks&quot; id=&quot;markdown-toc-1-bayesian-networks&quot;&gt;1. Bayesian Networks&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#11-conditional-independence&quot; id=&quot;markdown-toc-11-conditional-independence&quot;&gt;1.1. Conditional Independence&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#111-tail-to-tail&quot; id=&quot;markdown-toc-111-tail-to-tail&quot;&gt;1.1.1. Tail-to-tail&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#112-head-to-tail&quot; id=&quot;markdown-toc-112-head-to-tail&quot;&gt;1.1.2. Head-to-tail&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#113-head-to-head&quot; id=&quot;markdown-toc-113-head-to-head&quot;&gt;1.1.3. Head-to-head&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#114-d-separation&quot; id=&quot;markdown-toc-114-d-separation&quot;&gt;1.1.4. D-separation&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#12-markov-blanket&quot; id=&quot;markdown-toc-12-markov-blanket&quot;&gt;1.2. Markov Blanket&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2-markov-network&quot; id=&quot;markdown-toc-2-markov-network&quot;&gt;2. Markov Network&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#21-conditional-independence&quot; id=&quot;markdown-toc-21-conditional-independence&quot;&gt;2.1. Conditional independence&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#22-maximum-clique&quot; id=&quot;markdown-toc-22-maximum-clique&quot;&gt;2.2. Maximum Clique&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#23-moralization&quot; id=&quot;markdown-toc-23-moralization&quot;&gt;2.3. Moralization&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3-factor-graph&quot; id=&quot;markdown-toc-3-factor-graph&quot;&gt;3. Factor Graph&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#4-conclusion&quot; id=&quot;markdown-toc-4-conclusion&quot;&gt;4. Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;0-introduction&quot;&gt;0. Introduction&lt;/h1&gt;

&lt;p&gt;For a vector featured with multiple random variables like &lt;script type=&quot;math/tex&quot;&gt;X=[x_1,x_2,\dots,x_n]&lt;/script&gt;, what we care most are  &lt;em&gt;marginal probability&lt;/em&gt; &lt;script type=&quot;math/tex&quot;&gt;P(x_i)&lt;/script&gt;, &lt;em&gt;joint distribution&lt;/em&gt; &lt;script type=&quot;math/tex&quot;&gt;P(X)&lt;/script&gt; and &lt;em&gt;conditional probability&lt;/em&gt; &lt;script type=&quot;math/tex&quot;&gt;P(x_i\vert x_j)&lt;/script&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Marginal distribution:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x_i)=\sum_{x_n}\dots\sum_{x_{i+1}}\sum_{x_{i-1}}\dots\sum_{x_1}P(X);&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Joint distribution:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X)=P(x_1)\cdot\prod_{i=2}^{n}P(x_i\vert x_1,\dots,x_{i-1});&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Conditional probability (&lt;em&gt;Bayesian rule&lt;/em&gt;):&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x_i\vert x_j)=\frac{P(x_i,x_j)}{P(x_j)}.&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Obviously when $n$ is large, all of the above three can be of high computation complexity. Now we consider simplifying the computation of joint distribution, and to achieve that most methods have been proposed by&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;assuming all the features are totally &lt;em&gt;independent&lt;/em&gt;:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X)=\prod_{i=1}^nP(x_i);&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;or,  assuming that the features are &lt;em&gt;conditional independent&lt;/em&gt;, which is used in &lt;strong&gt;naive Bayes classifier&lt;/strong&gt; as class conditional independence:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X\vert Y)=\prod_{i=1}^n P(x_i\vert Y)\implies P(X)=\int_y \prod_{i=1}^nP(x_i\vert y)P(y)\text{d}y;&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;or, based on &lt;em&gt;conditional independent&lt;/em&gt;, assuming that the features process the &lt;em&gt;Markov Property&lt;/em&gt;, which is used in &lt;strong&gt;hidden Markov models&lt;/strong&gt;:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x_j\vert x_1,x_2,\dots,x_{j-1})=P(x_j\vert x_{j-1})\implies P(X)=P(x_1)\cdot\prod_{i=2}^nP(x_i\vert x_{i-1}).&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Among them, &lt;em&gt;graphical probabilistic models&lt;/em&gt; (PGM) are generally based on the &lt;em&gt;conditional independence assumption&lt;/em&gt;, ‘&lt;em&gt;capturing the way in which the joint distribution over all of the random variables can be decomposed into a product of factors each depending only on a subset of the variables&lt;/em&gt;’. (&lt;a href=&quot;https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf&quot;&gt;PRML&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;By leveraging the manipulations and properties of graph, many probabilistic insights can be obtained, motivating the design of new models.&lt;/p&gt;

&lt;h1 id=&quot;1-bayesian-networks&quot;&gt;1. Bayesian Networks&lt;/h1&gt;

&lt;p&gt;We now consider general graphical probabilistic models &lt;em&gt;Bayesian networks&lt;/em&gt; which are defined by &lt;em&gt;directed acyclic graphs&lt;/em&gt; (DAGs). Given a DAG, the &lt;em&gt;nodes&lt;/em&gt; in a Bayesian network represent random variables and edges represent conditional dependencies among those variables.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;../../../../assets/images/Figure8.2.png&quot; alt=&quot;Figure8.2 in PRML&quot; width=&quot;250&quot; /&gt;
&lt;/div&gt;

&lt;center&gt;
  &lt;p style=&quot;font-size:80%;&quot;&gt;
Figure 1. Example of a directed acyclic graph (Figure 8.2 of PRML).
  &lt;/p&gt;
&lt;/center&gt;

&lt;p&gt;A Bayesian network actually represents a joint distribution over all the random variables represented by its nodes. As it is shown in Figure 1 (from PRML), there are 7 random variables. The edge going from a node $x_i$ to a node $x_j$ indicates that $x_j$ is dependent to $x_i$. The joint distribution of the model in Figure 1 is then given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x_1,x_2,\dots,x_7)=P(x_1)P(x_2)P(x_3)P(x_4\vert x_1,x_2,x_3)P(x_5\vert x_1,x_3)P(x_6\vert x_4)P(x_7\vert x_4,x_5).&lt;/script&gt;

&lt;p&gt;More generally, for a graph with $K$ nodes, the corresponding joint distribution is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X)=\prod_{k=1}^KP(x_k\vert \mathbb{Pa}_k),&lt;/script&gt;

&lt;p&gt;where $\mathbb{Pa}_k$ denotes the set of parents of $x_k$ and $X=(x_1,x_2,\dots,x_k)$. Such a key equation expresses the &lt;em&gt;factorization&lt;/em&gt; properties of the joint distribution for a directed graph. Compared with the joint distribution expression mentioned in Section 0, the expression of that of Figure 1 we show above definitely is simplified a lot. Such simplification is actually introduced by the &lt;em&gt;absence&lt;/em&gt; of edges in the graph, which conveys the &lt;em&gt;conditional independence&lt;/em&gt; information of those variables.&lt;/p&gt;

&lt;h2 id=&quot;11-conditional-independence&quot;&gt;1.1. Conditional Independence&lt;/h2&gt;

&lt;p&gt;Conditional independence sometimes can greatly simplify the computations needed to perform inference and learning under probabilistic models. By using graphical models, we can find the conditional independence properties directly without effort to any analytical manipulations. We start the discussion by considering three simple examples each involving graphs having just three nodes.&lt;/p&gt;

&lt;h3 id=&quot;111-tail-to-tail&quot;&gt;1.1.1. Tail-to-tail&lt;/h3&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;../../../../assets/images/Figure8.15.png&quot; alt=&quot;Figure8.15 in PRML&quot; width=&quot;250&quot; /&gt;
&lt;/div&gt;
&lt;center&gt;
  &lt;p style=&quot;font-size:80%;&quot;&gt;
Figure 2. Example of tail-to-tail for conditional independence (Figure 8.15 of PRML).
  &lt;/p&gt;
&lt;/center&gt;

&lt;p&gt;Intuitively, we can consider the path from node &lt;em&gt;a&lt;/em&gt; to node &lt;em&gt;b&lt;/em&gt; via &lt;em&gt;c&lt;/em&gt; in Figure 2. The node &lt;em&gt;c&lt;/em&gt; is said to be &lt;em&gt;tail-to-tail&lt;/em&gt; as the node is connected to the tails of the two arrows. Given such a path, we have $a$ and $b$ dependent. However, if we condition on node &lt;em&gt;c&lt;/em&gt;, then the conditioned node &lt;em&gt;c&lt;/em&gt; will &lt;em&gt;block&lt;/em&gt; the path and cause $a$ and $b$ to become conditionally independent. A naive proof is as follows.&lt;/p&gt;

&lt;p&gt;By the definition given in section 1, the joint distribution of the graph in Figure 2 is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(a,b,c)=P(c)P(a\vert c)P(b\vert c).&lt;/script&gt;

&lt;p&gt;By the general definition of the marginal distribution, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(a,b)=\sum_c P(c)P(a\vert c)P(b\vert c),&lt;/script&gt;

&lt;p&gt;which generally dose not factorize into the product &lt;script type=&quot;math/tex&quot;&gt;P(a)\cdot P(b)&lt;/script&gt;, therefore $a$ and $b$ are dependent, denoted as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a\not\perp b\ \ \ \ \ \ \text{or, equivalenly,}\ \ \ \ \ \ a\not\!\perp\!\!\!\perp b\vert \emptyset.&lt;/script&gt;

&lt;p&gt;By the general definition of the joint distribution, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(a,b,c)=P(c)P(a\vert c)P(b\vert a,c).&lt;/script&gt;

&lt;p&gt;Thus it follows that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{alignat*}{3}&amp;&amp;P(c)P(a\vert c)P(b\vert a,c)&amp;=P(c)P(a\vert c)P(b\vert c)\\\implies&amp;&amp;P(b\vert a,c)&amp;=P(b\vert c)\\\implies&amp;&amp;\frac{P(a,b\vert c)}{P(a\vert c)}&amp;=P(b\vert c)\\\implies&amp;&amp;P(a,b\vert c)&amp;=P(a\vert c)P(b\vert c),\end{alignat*} %]]&gt;&lt;/script&gt;

&lt;p&gt;which means&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a\perp\!\!\!\perp b\vert c.&lt;/script&gt;

&lt;h3 id=&quot;112-head-to-tail&quot;&gt;1.1.2. Head-to-tail&lt;/h3&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;../../../../assets/images/Figure8.17.png&quot; alt=&quot;Figure8.17 in PRML&quot; width=&quot;250&quot; /&gt;
&lt;/div&gt;
&lt;center&gt;
  &lt;p style=&quot;font-size:80%;&quot;&gt;
Figure 3. Example of head-to-tail for conditional independence (Figure 8.17 of PRML).
  &lt;/p&gt;
&lt;/center&gt;

&lt;p&gt;The example of head-to-tail is shown in Figure 3. Similarly, the node &lt;em&gt;c&lt;/em&gt; is said to be &lt;em&gt;head-to-tail&lt;/em&gt; with respect to the path from node &lt;em&gt;a&lt;/em&gt; to node &lt;em&gt;b&lt;/em&gt;. Given such a path, we have $a$ and $b$ dependent. When node &lt;em&gt;c&lt;/em&gt; is conditioned, the case would be the same as &lt;em&gt;tail-to-tail&lt;/em&gt;: the conditioned node &lt;em&gt;c&lt;/em&gt; would &lt;em&gt;block&lt;/em&gt; the path and render $a$ and $b$ conditional independent. The naive proof of this is as follows.&lt;/p&gt;

&lt;p&gt;The joint distribution of the model in Figure 3 is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(a,b,c)=P(a)P(c\vert a)P(b\vert c).&lt;/script&gt;

&lt;p&gt;Marginalizing the distribution over $c$, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}P(a,b)&amp;=\sum_c P(a)P(c\vert a)P(b\vert c)\\&amp;=P(a)\sum_c P(b,c\vert a)\\&amp;=P(a)P(b\vert a),\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;which means &lt;script type=&quot;math/tex&quot;&gt;a\not\perp b&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;By rewriting the general joint distribution expression, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(a,b,c)=P(a)P(b\vert a)P(c\vert a,b).&lt;/script&gt;

&lt;p&gt;Then we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{alignat*}{3}&amp;&amp;P(a)P(b\vert a)P(c\vert a,b)&amp;=P(a)P(c\vert a)P(b\vert c)\\\implies&amp;&amp; P(b\vert a)\cdot\frac{P(a,b\vert c)P(c)}{P(b\vert a)P(a)}&amp;=\frac{P(a\vert c)P(c)}{P(a)}\cdot P(b\vert c)\\\implies&amp;&amp;P(a,b\vert c)&amp;=P(a\vert c)P(b\vert c)\\\implies&amp;&amp;a&amp;\perp\!\!\!\perp b\vert c.\end{alignat*} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;113-head-to-head&quot;&gt;1.1.3. Head-to-head&lt;/h3&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;../../../../assets/images/Figure8.19.png&quot; alt=&quot;Figure8.19 in PRML&quot; width=&quot;250&quot; /&gt;
&lt;/div&gt;
&lt;center&gt;
  &lt;p style=&quot;font-size:80%;&quot;&gt;
Figure 4. Example of head-to-head for conditional independence (Figure 8.19 of PRML).
  &lt;/p&gt;
&lt;/center&gt;

&lt;p&gt;The third example given in Figure 4 is opposite to the previous two cases. The node &lt;em&gt;c&lt;/em&gt; in Figure 4 is &lt;em&gt;head-to-head&lt;/em&gt; with respect to the path from &lt;em&gt;a&lt;/em&gt; to &lt;em&gt;b&lt;/em&gt; as it connects to the heads of the two arrows. In this case, node &lt;em&gt;c&lt;/em&gt; would &lt;em&gt;block&lt;/em&gt; the path while conditioning on node &lt;em&gt;c&lt;/em&gt; would &lt;em&gt;unblock&lt;/em&gt; the path and render $a$ and $b$ dependent. The proof is much similar to the previous cases.&lt;/p&gt;

&lt;p&gt;The joint distribution of the model in Figure 4 is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(a,b,c)=P(a)P(b)P(c\vert a,b).&lt;/script&gt;

&lt;p&gt;Marginalizing the distribution over $c$, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}P(a,b)&amp;=\sum_c P(a)P(b)P(c\vert a,b)\\&amp;=P(a)P(b)\sum_c P(c\vert a)\\&amp;=P(a)P(b)\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;which means &lt;script type=&quot;math/tex&quot;&gt;a\perp b&lt;/script&gt;. In particular, the third equation is given by the fact ‘&lt;em&gt;conditional probabilities are probabilities&lt;/em&gt;’ (&lt;a href=&quot;https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view&quot;&gt;Introduction to Probability&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;By rewriting the general joint distribution expression, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(a,b,c)=P(a,b\vert c)P(c).&lt;/script&gt;

&lt;p&gt;Then we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{alignat*}{3}&amp;&amp;P(a,b\vert c)P(c)&amp;=P(a)P(b)P(c\vert a,b)\\\implies&amp;&amp;P(a,b\vert c)&amp;=\frac{P(a)P(b)P(c\vert a,b)}{P(c)},\end{alignat*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where the last term in general does not factorize into the product $P(a\vert c)P(b\vert c)$, hence we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a\not\!\perp\!\!\!\perp b\vert c.&lt;/script&gt;

&lt;h3 id=&quot;114-d-separation&quot;&gt;1.1.4. D-separation&lt;/h3&gt;

&lt;p&gt;With the three examples, we now introduce &lt;em&gt;D-separation&lt;/em&gt; which is used to determine the independence of those random variables in a graph. Specifically, given three arbitrary nonintersecting sets $\mathcal{A}$, $\mathcal{B}$ and $\mathcal{C}$ of the nodes of the graph, by D-separation we can determine whether the statement &lt;script type=&quot;math/tex&quot;&gt;\mathcal{A}\perp\!\!\!\perp\mathcal{B}\vert \mathcal{C}&lt;/script&gt; is true under the graph.&lt;/p&gt;

&lt;p&gt;The method proceeds as follows. (I am sure this great &lt;a href=&quot;https://www.youtube.com/watch?v=yDs_q6jKHb0&quot;&gt;video&lt;/a&gt; can help you get into &lt;em&gt;D-separation&lt;/em&gt; easily.)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Find out all possible &lt;em&gt;undirected&lt;/em&gt; paths between any node in $\mathcal{A}$ and any node in $\mathcal{B}$;&lt;/li&gt;
  &lt;li&gt;Check whether those paths are blocked: for each path,
    &lt;ul&gt;
      &lt;li&gt;splitting it into continuous triples;&lt;/li&gt;
      &lt;li&gt;for each triple, its structure (with directionality concerns) must belong to one of the three examples we mentioned before, and we just need to determine whether it is blocked when conditioning on $\mathcal{C}$;&lt;/li&gt;
      &lt;li&gt;if there is at least one triple blocked, the path is said to be blocked, otherwise, the path is unblocked;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;If all the paths are blocked, the statement &lt;script type=&quot;math/tex&quot;&gt;\mathcal{A}\perp\!\!\!\perp\mathcal{B}\vert \mathcal{C}&lt;/script&gt; is true and $\mathcal{A}$ is said to be d-separated from $\mathcal{B}$ by $\mathcal{C}$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;12-markov-blanket&quot;&gt;1.2. Markov Blanket&lt;/h2&gt;

&lt;p&gt;We now introduce the concept of a &lt;em&gt;Markov blanket&lt;/em&gt; or &lt;em&gt;Markov boundary&lt;/em&gt;. Consider a joint distribution $P(x_1,x_2,\dots,x_N)$ represented by a directed graph having $N$ nodes. In particular, we want to determine the conditional distribution&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x_i\vert x_{\{j\ne i\}})=\frac{\prod_k P(x_k\vert \mathbb{Pa}_k)}{\int\prod_k P(x_k\vert \mathbb{Pa}_k)\text{d}x_i}.&lt;/script&gt;

&lt;p&gt;Notice that if the term &lt;script type=&quot;math/tex&quot;&gt;p(x_k\vert \mathbb{Pa}_k)&lt;/script&gt; does not involve $x_i$, that is to say $k\ne i$ and/or $x_i\notin\mathbb{Pa}_k$, we then can remove the term from both numerator and denominator. The remaining terms in the conditional distribution then must be&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x_k\vert \mathbb{Pa}_k), k\in\{i\}\cup\{k\vert x_i\in\mathbb{Pa}_k\},&lt;/script&gt;

&lt;p&gt;and the conditional distribution &lt;script type=&quot;math/tex&quot;&gt;P(x_i\vert x_{\{j\ne i\}})&lt;/script&gt; depends only on those terms. We now discuss which nodes those terms are related to. Obviously, when &lt;script type=&quot;math/tex&quot;&gt;k\in\{i\}&lt;/script&gt;, the term &lt;script type=&quot;math/tex&quot;&gt;P(x_i\vert \mathbb{Pa}_i)&lt;/script&gt; would only involve $x_i$ and &lt;strong&gt;the parents&lt;/strong&gt; of it. When &lt;script type=&quot;math/tex&quot;&gt;k\in\{j\vert x_i\in\mathbb{Pa}_j\}&lt;/script&gt; and $k\ne i$, the term $P(x_k\vert \mathbb{Pa}_k)$ are related to two parts. The first part $x_k$ is &lt;strong&gt;the child&lt;/strong&gt; of $x_i$ as $x_i\in\mathbb{Pa}_k$, while the second part $\mathbb{Pa}_k$ is &lt;strong&gt;the co-parents&lt;/strong&gt; of the child $x_k$.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;../../../../assets/images/Figure8.26.png&quot; alt=&quot;Figure8.26 in PRML&quot; width=&quot;250&quot; /&gt;
&lt;/div&gt;
&lt;center&gt;
  &lt;p style=&quot;font-size:80%;&quot;&gt;
Figure 5. The Markov blanket of $x_i$ is denoted by colored nodes. (Figure 8.26 of PRML).
  &lt;/p&gt;
&lt;/center&gt;

&lt;p&gt;As shown in Figure 5, we say that a &lt;em&gt;Markov blanket&lt;/em&gt; $\mathcal{M}_i$ of a node $x_i$ comprises the set of its &lt;em&gt;parents&lt;/em&gt;, &lt;em&gt;child&lt;/em&gt; and &lt;em&gt;co-parents&lt;/em&gt;. Given the Markov blanket, the conditional distribution &lt;script type=&quot;math/tex&quot;&gt;P(x_i\vert x_{\{j\ne i\}})&lt;/script&gt; can be rewritten as &lt;script type=&quot;math/tex&quot;&gt;P(x_i\vert \mathcal{M}_i)&lt;/script&gt;.&lt;/p&gt;

&lt;h1 id=&quot;2-markov-network&quot;&gt;2. Markov Network&lt;/h1&gt;

&lt;p&gt;The graphs we talked in the previous sections are directed. When it comes to undirected graphs, some concepts of directed graphs still play important roles while others do not. The graphical probabilistic models defined by &lt;em&gt;undirected graphs&lt;/em&gt; is called &lt;em&gt;Markov networks&lt;/em&gt;, also known as &lt;em&gt;Markov random fields&lt;/em&gt;. Similar to Bayesian networks, the nodes in a Markov network represent random variables. However, as edges carry no arrows in undirected graphs, the function of edges changes a lot.&lt;/p&gt;

&lt;h2 id=&quot;21-conditional-independence&quot;&gt;2.1. Conditional independence&lt;/h2&gt;

&lt;p&gt;The conditional independence of an undirected graph is given by the &lt;em&gt;absence&lt;/em&gt; of edges. Specifically, for a graph with nodes representing random variable $x_1,x_2,\dots, x_N$, we have:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pairwise Markov Property&lt;/strong&gt;: the &lt;em&gt;absence&lt;/em&gt; of an edge between two nodes $x_i$ and $x_j$ means the corresponding random variables of the two nodes are conditionally independent given all the other random variables, which is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_i\text{ and }x_j\text{ are not adjacent}\implies x_i\perp\!\!\!\perp x_j\vert X_{\setminus \{i,j\}},&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;X_{\setminus\{i,j\}}&lt;/script&gt; denotes the set of all the variables with $x_i$ and $x_j$ removed.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Local Markov Property&lt;/strong&gt;: a random variable $x_i$ is conditionally independent of all other random variables given its neighbors, which is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_i\perp\!\!\!\perp X_{\setminus \mathbb{Ne}_i}\vert \mathbb{Ne}_{i},&lt;/script&gt;

&lt;p&gt;where $\mathbb{Ne}_i$ is the set of neighbors of $x_i$, &lt;em&gt;i.e.,&lt;/em&gt; every node directly connected with $x_i$ is in $\mathbb{Ne}_i$. Recalling the definition of the &lt;em&gt;Markov blanket&lt;/em&gt;, we can find that &lt;script type=&quot;math/tex&quot;&gt;\mathbb{Ne}_i&lt;/script&gt; is the &lt;em&gt;Markov blanket&lt;/em&gt; in the undirected graph.&lt;/p&gt;

&lt;p&gt;The property below is to Markov networks as &lt;em&gt;D-separation&lt;/em&gt; is to Bayesian networks&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Global Markov Property&lt;/strong&gt;: for any three nonintersecting sets $\mathcal{A}$, $\mathcal{B}$ and $\mathcal{C}$ of the nodes of the graph, we can determine whether&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{A}\perp\!\!\!\perp \mathcal{B}\vert\mathcal{C}&lt;/script&gt;

&lt;p&gt;by the following steps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Find out all possible paths between any node in $\mathcal{A}$ and any node in $\mathcal{B}$;&lt;/li&gt;
  &lt;li&gt;Check whether every path from $\mathcal{A}$ to $\mathcal{B}$ passes through at least one node in $\mathcal{C}$;&lt;/li&gt;
  &lt;li&gt;If so, the statement is true.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Notice that compared with Bayesian networks, the way we check a statement of the conditional independence in Markov networks actually does not entail the concept ‘&lt;em&gt;block&lt;/em&gt;’. Testing for conditional independence in undirected graphs is therefore simpler than in directed graphs.&lt;/p&gt;

&lt;p&gt;It can be shown that the three properties above are equivalent.&lt;/p&gt;

&lt;h2 id=&quot;22-maximum-clique&quot;&gt;2.2. Maximum Clique&lt;/h2&gt;

&lt;p&gt;As a Bayesian network can represent a joint distribution over finite random variables, there also exists a probability density function for each Markov network that is consistent with the three properties we mentioned above. Before moving on, we introduce a concept for a Markov network called a &lt;em&gt;clique&lt;/em&gt;, which is defined as a subset of fully connected notes of the undirected graph. Obviously, there may be many different cliques for a Markov network. Among them, we particularly focus on the cliques each of which allows no other nodes to be added without it ceasing to be a clique, and we call such a clique a &lt;em&gt;maximal clique&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Denote the maximal cliques set of a Markov network with &lt;script type=&quot;math/tex&quot;&gt;\{x_1,x_2,\dots,x_N\}&lt;/script&gt; by &lt;script type=&quot;math/tex&quot;&gt;C_m=\{C\vert C\text{ is a maximal clique}\}&lt;/script&gt;, and the nodes in maximal clique $C$ by &lt;script type=&quot;math/tex&quot;&gt;\text{x}_C=\{x_i\vert x_i\in C\}&lt;/script&gt;. Then the joint distribution represented by the Markov network can be written as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x_1,x_2,\dots,x_N)=\frac{1}{Z}\prod_{C}^{C_m}\psi_C(\text{x}_C),&lt;/script&gt;

&lt;p&gt;where $\psi_C(\cdot)$ are positive functions called &lt;em&gt;potential functions&lt;/em&gt;, and the quantity $Z$ is called &lt;em&gt;partition function&lt;/em&gt; that validates the distribution, &lt;em&gt;i.e.&lt;/em&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z=\sum_x\prod_C^{C_m}\psi_C(\text{x}_C).&lt;/script&gt;

&lt;p&gt;Given a Markov network, the equivalence of the joint distribution and the conditional independence can be shown by &lt;em&gt;Hammesley-Clifford theorem&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;We now consider the choice of potential functions. Given the existence of partition function, we have great flexibilities in choosing potential functions. However, it naturally raises the question of how to motivate a choice of potential function for a particular application. Since it requires the potential functions to be positive, a widely used function is &lt;em&gt;exponential function&lt;/em&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\psi_C(\text{x}_C)=\exp\{-E(\text{x}_C)\},&lt;/script&gt;

&lt;p&gt;where $E(\text{x}_C)$ is called &lt;em&gt;energy function&lt;/em&gt;. The joint distribution then is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}P(x_1,x_2,\dots,x_N)&amp;=\frac{1}{Z}\prod_{C}^{C_m}\psi_C(\text{x}_C)\\&amp;=\frac{1}{Z}\exp\left\{-\sum_{C}^{C_m}E(\text{x}_C)\right\},\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;which is known as &lt;em&gt;Boltzmann distribution&lt;/em&gt; (or, &lt;em&gt;Gibbs distribution&lt;/em&gt;). Moreover, we can see that the distribution is consistent with the definition of &lt;a href=&quot;https://19w6.github.io/2020/11/12/exponential_family-ml07/&quot;&gt;exponential families&lt;/a&gt;. The joint distribution of any Markov network in which every potential has the form of exponentials is in exponential families.&lt;/p&gt;

&lt;h2 id=&quot;23-moralization&quot;&gt;2.3. Moralization&lt;/h2&gt;

&lt;p&gt;We now consider the relation between the two graphical models. Particularly, we consider a problem of how to converting a Bayesian network to a Markov network. We start the discussion from the three examples of Bayesian networks mentioned in section 1.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Tail-to-tail: Given the tail-to-tail case shown in Figure 2, we have the joint distribution&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;P(a,b,c)=P(c)P(a\vert c)P(b\vert c).&lt;/script&gt;

    &lt;p&gt;A factorization can be easily obtained by identifying&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}P(a,b,c)&amp;=\psi(a,c)\psi(b,c),\\\psi(a,c)&amp;=P(c)P(a\vert c),\\\psi(b,c)&amp;=P(b\vert c),\end{aligned} %]]&gt;&lt;/script&gt;

    &lt;p&gt;which is actually the joint distribution represented by the Markov network whose maximum cliques are &lt;script type=&quot;math/tex&quot;&gt;\{a,c\}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\{b,c\}&lt;/script&gt; and potential functions are &lt;script type=&quot;math/tex&quot;&gt;\psi(a,c)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\psi(b,c)&lt;/script&gt;. Obviously, such a Markov network is the same as the Bayesian network in Figure 2 with removing its arrows.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Head-to-tail: For the graph in Figure 3, we have&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;P(a,b,c)=P(a)P(c\vert a)P(b\vert c).&lt;/script&gt;

    &lt;p&gt;Similarly, by identifying&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\psi(a,c)&amp;=P(a)P(c\vert a),\\\psi(b,c)&amp;=P(b\vert c).\end{aligned} %]]&gt;&lt;/script&gt;

    &lt;p&gt;We have the corresponding Markov network with maximum cliques &lt;script type=&quot;math/tex&quot;&gt;\{a,c\}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\{b,c\}&lt;/script&gt;, which also can be obtained by removing the arrows of the graph.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Head-to-head: The case shown in Figure 4 is a little tricky. Given the joint distribution&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;P(a,b,c)=P(a)P(b)P(c\vert a,b),&lt;/script&gt;

    &lt;p&gt;we can find that the term &lt;script type=&quot;math/tex&quot;&gt;P(c\vert a,b)&lt;/script&gt; leads to a factor that depends on three nodes. Therefore the corresponding Markov network must have a maximum clique consists of &lt;script type=&quot;math/tex&quot;&gt;\{a,b,c\}&lt;/script&gt;. To this end, we need to not only remove the arrows of the graph but also add an edge between $a$ and $b$. Then we have the corresponding Markov network with potential function&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\psi(a,b,c)=P(a)P(b)P(c\vert a,b).&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Given the above discussion, to convert a directed graph into an undirected graph, the general steps are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Remove all the arrows in the directed graph;&lt;/li&gt;
  &lt;li&gt;Add additional edges between all pairs of parents for each node;&lt;/li&gt;
  &lt;li&gt;Initialize all the potential functions to 1;&lt;/li&gt;
  &lt;li&gt;Multiply each conditional distribution factor into the potential function whose corresponding clique contains all the variables of the factor.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The step &lt;em&gt;adding additional edges&lt;/em&gt; is known as &lt;em&gt;moralization&lt;/em&gt;. And the resulting undirected graph after &lt;em&gt;removing arrows&lt;/em&gt; is called &lt;em&gt;moral graph&lt;/em&gt;.&lt;/p&gt;

&lt;h1 id=&quot;3-factor-graph&quot;&gt;3. Factor Graph&lt;/h1&gt;

&lt;p&gt;Notice that in moralization, we may invite loops in the moral graph, which can be tricky in some cases. To avoid the issues incurred by the loops, we can leverage &lt;em&gt;factor graphs&lt;/em&gt;. Given a joint distribution of a moral graph, we can construct a factor graph by&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Remove all the edges in the graph;&lt;/li&gt;
  &lt;li&gt;Rewrite the joint distribution as a multiplication of multiple functions where the functions can depend on an arbitrary set of the nodes;&lt;/li&gt;
  &lt;li&gt;Add new nodes for each function of the new expression;&lt;/li&gt;
  &lt;li&gt;Add edges between each function and the nodes it depends on.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;div style=&quot;display:flex&quot;&gt;
            &lt;figure&gt;
&lt;img src=&quot;../../../../assets/images/Figure8.42a.png&quot; alt=&quot;Figure8.42 (a) in PRML&quot; /&gt;
                &lt;figcaption&gt;&lt;center&gt;(a)&lt;/center&gt;&lt;/figcaption&gt;
            &lt;/figure&gt;
            &lt;figure&gt;
&lt;img src=&quot;../../../../assets/images/Figure8.41a.png&quot; alt=&quot;Figure8.41 (a) in PRML&quot; /&gt;
                &lt;figcaption&gt;&lt;center&gt;(b)&lt;/center&gt;&lt;/figcaption&gt;
            &lt;/figure&gt;
            &lt;figure&gt;
&lt;img src=&quot;../../../../assets/images/Figure8.42c.png&quot; alt=&quot;Figure8.42 (c) in PRML&quot; /&gt;
                &lt;figcaption&gt;&lt;center&gt;(c)&lt;/center&gt;&lt;/figcaption&gt;
            &lt;/figure&gt;
    &lt;/div&gt;
&lt;/figure&gt;
&lt;center&gt;
&lt;p style=&quot;font-size:80%;&quot;&gt;
Figure 6. (a) A directed graph. (b) The corresponding moral graph of the directed graph. (c) A factor graph of the graph where the factors are depicted by small solid squares. (Figure 8.41 and Figure 8.42 of PRML).
  &lt;/p&gt;
&lt;/center&gt;
&lt;p&gt;An example is shown in Figure 6. The directed graph represents the joint distribution&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x_1,x_2,x_3)=P(x_1)P(x_2)P(x_3\vert x_1,x_2).&lt;/script&gt;

&lt;p&gt;The moralization of the directed graph incurs a loop among $x_1,x_2,x_3$ as shown in Figure 6 (b). Defining $f_a(x_1)=P(x_1)$, $f_b(x_2)=P(x_2)$ and &lt;script type=&quot;math/tex&quot;&gt;f_c(x_1,x_2,x_3)=P(x_3\vert x_1,x_2)&lt;/script&gt;, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x_1,x_2,x_3)=f_a(x_1)f_b(x_2)f_c(x_1,x_2,x_3).&lt;/script&gt;

&lt;p&gt;The factor graph corresponding to such a factorization is shown in Figure 6 (c). Moreover, all factor graphs are &lt;em&gt;bipartite&lt;/em&gt; as they consist of two distinct kinds of nodes. With factor graphs, we can conduct the related computation based on the factor nodes rather than variable nodes so that the loop can be avoided.&lt;/p&gt;

&lt;h1 id=&quot;4-conclusion&quot;&gt;4. Conclusion&lt;/h1&gt;

&lt;p&gt;In this post, we first introduced two kinds of probabilistic graphical models. One is &lt;em&gt;Bayesian networks&lt;/em&gt; that is based on directed acyclic graph. The other is &lt;em&gt;Markov network&lt;/em&gt; that is based on undirected graph. Both two models can be used to represent the joint distribution and reflect the conditional independences over a set of random variables. Then we discussed how to convert a Bayesian network into a Markov network. The loops in the &lt;em&gt;moral graph&lt;/em&gt; incurred by the conversion can be avoided by transforming the graph into a &lt;em&gt;factor graph&lt;/em&gt;.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Machine Learning - 07 Exponential Family</title>
   <link href="http://localhost:4000/2020/11/12/exponential_family-ml07/"/>
   <updated>2020-11-12T00:00:00+08:00</updated>
   <id>http://localhost:4000/2020/11/12/exponential_family-ml07</id>
   <content type="html">&lt;p&gt;&lt;em&gt;The notes are based on the &lt;a href=&quot;https://github.com/shuhuai007/Machine-Learning-Session&quot;&gt;session&lt;/a&gt; and the &lt;a href=&quot;https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter8.pdf&quot;&gt;material&lt;/a&gt;. For the fundamental of linear algebra, one can always refer to &lt;a href=&quot;http://math.mit.edu/~gs/linearalgebra/&quot;&gt;Introduction to Linear Algebra&lt;/a&gt; and &lt;a href=&quot;https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf&quot;&gt;The Matrix Cookbook&lt;/a&gt; for more details. Many thanks to these great works.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;0-introduction&quot;&gt;0. Introduction&lt;/h1&gt;

&lt;p&gt;An exponential family is a family of distributions which share some properties in common. The real message of this note is the simplicity and elegance of the exponential family. Once the ideas are mastered, it is often easier to work within the general exponential family framework than with specific instances.&lt;/p&gt;

&lt;h1 id=&quot;1-exponential-family&quot;&gt;1. Exponential Family&lt;/h1&gt;

&lt;p&gt;Given one real-vector parameter &lt;script type=&quot;math/tex&quot;&gt;\mathbf{\theta}=[\theta_1,\theta_2,\dots,\theta_d]^T&lt;/script&gt;, we define an &lt;em&gt;exponential family&lt;/em&gt; of probability distributions as those distributions whose density have the following general form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_X(x\vert\eta)=h(x)\exp\left(\eta^T T(x)-A(\eta)\right).&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Canonical parameter&lt;/strong&gt;: $\eta$ is called &lt;em&gt;canonical&lt;/em&gt;, or &lt;em&gt;natural parameter&lt;/em&gt; (function), which can be viewed as a transformation of $\mathcal{\theta}$. The set of values of $\eta$ is always convex.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sufficient statistic&lt;/strong&gt;: Given a data set sampled from &lt;script type=&quot;math/tex&quot;&gt;f_X(x\vert\eta)&lt;/script&gt;, the sufficient statistic $T(x)$ is a function of the data that holds all information the data set provides with regard to the unknown parameter $\mathbf{\theta}$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Log-partition function&lt;/strong&gt;: $A(\eta)$ is the &lt;em&gt;log-partition function&lt;/em&gt; to normalize &lt;script type=&quot;math/tex&quot;&gt;f_X(x\vert \eta)&lt;/script&gt; to be a probability distribution,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A(\eta)=\log\left(\int_{X}h(x)\exp(\eta^T T(x))\text{d}x\right).&lt;/script&gt;

&lt;p&gt;In the following sections, we will discuss them detailedly.&lt;/p&gt;

&lt;h1 id=&quot;2-sufficient-statistic&quot;&gt;2. Sufficient Statistic&lt;/h1&gt;

&lt;p&gt;Consider the problem of estimating the unknown parameters by &lt;em&gt;maximum likelihood estimation&lt;/em&gt; (MLE) in exponential family cases. Specifically, for an &lt;em&gt;i.i.d.&lt;/em&gt; data set &lt;script type=&quot;math/tex&quot;&gt;\mathcal{D}=\{x_1,x_2,\dots,x_N\}&lt;/script&gt;, we have the log likelihood&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}(\eta\vert\mathcal{D})=\log\left(\prod_{i=1}^Nh(x_i)\right)+\eta^T\left(\sum_{i=1}^NT(x_i)\right)-NA(\eta).&lt;/script&gt;

&lt;p&gt;By &lt;em&gt;MLE&lt;/em&gt;, we have the estimation $\hat\eta$ when its gradient with respect to $\eta$ is zero:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}’(\eta\vert\mathcal{D})=\sum_{i=1}^NT(x_i)-NA’(\eta)=0.&lt;/script&gt;

&lt;p&gt;Solving the equation, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A’(\hat\eta)=\frac{1}{N}\sum_{i=1}^NT(x_i),&lt;/script&gt;

&lt;p&gt;which is the general formula of MLE for the parameters in the exponential family. Further, notice that our formula involves the data only via the sufficient statistic $T(x_i)$. This gives the operational meaning to &lt;em&gt;sufficiency&lt;/em&gt;—for the purpose of estimating parameters we retain only the sufficient statistic.&lt;/p&gt;

&lt;h1 id=&quot;3-log-partition-function&quot;&gt;3. Log-partition Function&lt;/h1&gt;

&lt;p&gt;As we mentioned in section 1, $A(\eta)$ can be viewed as a normalization factor. In fact, $A(\eta)$ is not a degree of freedom in the specification of an exponential family density; it is determined once $T(x)$ and $h(x)$ are determined. The relation between $A(\eta)$ and $T(x)$ can be further characterized by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}A’(\eta)&amp;=\frac{\text{d}\log\left(\int_{X}h(x)\exp(\eta^T T(x))\text{d}x\right)}{\text{d}\eta}\\&amp;=\frac{\int_{X}h(x)\exp(\eta^T T(x))\cdot  T(x)\text{d}x}{\int_{X}h(x)\exp(\eta^T T(x))\text{d}x}\\&amp;=\frac{\int_{X}h(x)\exp(\eta^T T(x))\cdot T(x)\text{d}x}{\exp({A(\mathbf{\theta})})}\\&amp;=\int_{X}\underbrace{h(x)\exp(\eta^T T(x)-A(\eta))}_{f_X(x\vert \eta)}\cdot T(x)\text{d}x\\&amp;=\mathbb{E}_{f_X(x\vert\eta)}[T(x)].\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Further, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}A’’(\eta)&amp;=\int_{X}f_X(x\vert \eta)\cdot(T(x)-A’(\eta)) T(x)\text{d}x\\&amp;=\int_{X}f_X(x\vert \eta)\cdot(T(x))^2\text{d}x-A’(\eta)\int_{X}f_X(x\vert \mathbf{\eta})\cdot T(x)\text{d}x\\&amp;=\mathbb{E}_{f_X(x\vert\eta)}[(T(x))^2]-\left(\mathbb{E}_{f_X(x\vert\eta)}[T(x)]\right)^2\\&amp;=var[T(x)],\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;which also shows that $A(\eta)$ is convex as $var[T(x)]\ge 0$.&lt;/p&gt;

&lt;h1 id=&quot;4-maximum-entropy&quot;&gt;4. Maximum Entropy&lt;/h1&gt;

&lt;p&gt;The entropy of $P$ with distribution $p(x)$ supported on $X$ is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(P)=\mathbb{E}_{P}[-\log p(x)].&lt;/script&gt;

&lt;p&gt;The &lt;em&gt;maximum entropy&lt;/em&gt; principle is that: given some constraints (prior information) about the distribution $P$, we consider all probability distributions satisfying said constraints such that the constraints are being utilized as &lt;em&gt;objective&lt;/em&gt; as possible, &lt;em&gt;i.e.,&lt;/em&gt; be as uncertain as possible.&lt;/p&gt;

&lt;p&gt;For example, consider the case where the very constraint is $\sum_Xp(x)=1$, which formulates&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\max&amp;\quad H(P)\\\text{s.t.}&amp;\quad \sum_{X}p(x)=1.\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;By the definition we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(P)=-\sum_{i=1}^{\vert X\vert}p(x_i)\log p(x_i).&lt;/script&gt;

&lt;p&gt;Then the &lt;em&gt;Lagrangian&lt;/em&gt; for the optimization problem is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(P,\lambda)=\sum_{i=1}^{\vert X\vert}p(x_i)\log p(x_i)+\lambda\left(1-\sum_{i=1}^{\vert X\vert}p(x_i)\right).&lt;/script&gt;

&lt;p&gt;Setting the first derivation of the Lagrangian to be zero yields&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial L}{\partial p(x_i)}=0\implies \hat{p}(x_i)=\exp(\lambda-1),&lt;/script&gt;

&lt;p&gt;which gives that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(x_1)=\hat{p}(x_2)=\dots=\hat{p}(x_{\vert X\vert})=\frac{1}{\vert X\vert},&lt;/script&gt;

&lt;p&gt;&lt;em&gt;i.e.,&lt;/em&gt; the distribution with maximum entropy is &lt;em&gt;uniform distribution&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;We now consider a general case where $p(x)$ is continuous with a general constraint $\mathbb{E}_P[\Phi(x)]=\alpha$, where $\Phi(x)=[\phi_1(x),\phi_2(x),\dots,\phi_d(x)]\in\mathbb{R}^d$ and $\alpha=[\alpha_1,\alpha_2,\dots,\alpha_d]\in\mathbb{R}^d$, which formulates&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\max&amp;\quad H(P)\\\text{s.t.}&amp;\quad \mathbb{E}_P[\Phi(x)]=\alpha\\\implies\min&amp;\quad \int_Xp(x)\log p(x)\text{d}x\\\text{s.t.}&amp;\quad \int_X p(x)\phi_i(x)\text{d}x=\alpha_i,\ i=1,2,\dots, d,\\&amp;\quad \int_X p(x)\text{d}x=1.\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Similarly, we obtain the Lagrangian as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(P,\theta,\lambda)=\int_X p(x)\log p(x)\text{d}x+\sum_{i=1}^d\theta_i\left(\alpha_i-\int_X p(x)\phi_i(x)\text{d}x\right)+\lambda\left(\int_X p(x)\text{d}x-1\right).&lt;/script&gt;

&lt;p&gt;By treating the density $P=[p(x)]_{x\in X}$ as a finite vector such that $\int_X p(x)\text{d}x$ is similar to $\sum_X p(x)$, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\frac{\partial L}{\partial p(x)}&amp;=\frac{\partial }{\partial p(x)}\left(\sum_X p(x)\log p(x)-\sum_{i=1}^d\theta_i\sum_X p(x)\phi_i(x)+\lambda\sum_X p(x)\right)\\&amp;=1+\log p(x)-\sum_{i=1}^d\theta_i\phi_i(x)+\lambda\\&amp;=1+\log p(x)-\theta^T\Phi(x)+\lambda.\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Setting the derivation to be zero for all $x$, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x)=\exp\left\{\theta^T\Phi(x)-(\lambda+1)\right\},&lt;/script&gt;

&lt;p&gt;which is in the exponential family form with&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\eta&amp;=\theta,\\T(x)&amp;=\Phi(x),\\A(\eta)&amp;=\lambda+1,\\h(x)&amp;=1.\end{aligned} %]]&gt;&lt;/script&gt;

&lt;h1 id=&quot;5-gaussian-distribution&quot;&gt;5. Gaussian Distribution&lt;/h1&gt;

&lt;p&gt;In this section, we consider an example, Gaussian distribution, which is of the exponential family and exemplifies the properties we mentioned above.&lt;/p&gt;

&lt;p&gt;We first rewritten the PDF of one-dimension Gaussian distribution to show it is in the exponential family .&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; Given unknown parameter &lt;script type=&quot;math/tex&quot;&gt;\mathbf{\theta}=[\mu,\sigma^2]&lt;/script&gt;, the Gaussian density can be written as follows,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}f_X(x\vert \mathbf{\theta})&amp;=\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{-\frac{1}{2\sigma^2}(x-\mu)^2\right\}\\&amp;=\frac{1}{\sqrt{2\pi}}\exp\left\{\frac{\mu}{\sigma^2}x-\frac{1}{2\sigma^2}x^2-\frac{1}{2\sigma^2}\mu^2-\log\sigma\right\}\\&amp;=\frac{1}{\sqrt{2\pi}}\exp\left\{\begin{bmatrix}\frac{\mu}{\sigma^2}&amp;-\frac{1}{2}\sigma^2\end{bmatrix}\begin{bmatrix}x\\x^2\end{bmatrix}-\left(\frac{\mu^2}{2\sigma^2}+\log\sigma\right)\right\},\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;which is in the exponential family form with&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\eta&amp;=\begin{bmatrix}\frac{\mu}{\sigma^2}&amp;-\frac{1}{2\sigma^2}\end{bmatrix}^T,\\T(x)&amp;=\begin{bmatrix}x&amp;x^2\end{bmatrix}^T,\\A(\eta)&amp;=\frac{\mu^2}{2\sigma^2}+\log\sigma=-\frac{\eta_1^2}{4\eta_2}-\frac{1}{2}\log(-2\eta_2),\\h(x)&amp;=\frac{1}{\sqrt{2\pi}}.\end{aligned} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\tag*{$\blacksquare$}&lt;/script&gt;

&lt;p&gt;Then we verify the relation between the sufficient statistic and MLE method.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; Given a data set &lt;script type=&quot;math/tex&quot;&gt;\mathcal{D}=\{x_1,x_2,\dots,x_N\}&lt;/script&gt;, as we mentioned in section 2, we can derive the parameters via the sufficient statistic as follows,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{cases}A’(\eta)=\frac{1}{N}\sum_{i=1}^NT(x)\implies\begin{cases}A’(\hat\eta_1)=\frac{1}{N}\sum_{i=1}^N x_i\\A’(\hat\eta_2)=\frac{1}{N}\sum_{i=1}^Nx_i^2\end{cases}\\A(\eta)=-\frac{\eta_1^2}{4\eta_2}-\frac{1}{2}\log(-2\eta_2)\implies\begin{cases}A’(\hat\eta_1)=-\frac{\eta_1}{2\eta_2}=\hat\mu\\A’(\hat\eta_2)=\frac{\eta_1^2}{4\eta_2^2}-\frac{1}{2\eta_2}=\hat\sigma^2+\hat\mu^2\end{cases}\end{cases}&lt;/script&gt;

&lt;p&gt;Solving the equations we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat\mu=\frac{1}{N}\sum_{i=1}^Nx_i,\quad \hat\sigma=\frac{1}{N}\sum_{i=1}^Nx_i^2-\hat\mu^2,&lt;/script&gt;

&lt;p&gt;which is consistent with the result in the &lt;a href=&quot;https://19w6.github.io/2020/09/28/intro-ml01/&quot;&gt;post&lt;/a&gt;. $\tag*{$\blacksquare$}$&lt;/p&gt;

&lt;p&gt;Now we show that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A’’(\hat\eta)=var[T(x)].&lt;/script&gt;

&lt;p&gt;&lt;em&gt;Proof&lt;/em&gt;: Firstly, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{cases}A’’(\hat\eta_1)=-\frac{1}{2\eta_2}=\sigma^2\\A’’(\hat\eta_2)=-\frac{\eta_1^2}{2\eta_2^3}+\frac{1}{2\eta_2^2}=4\sigma^2\mu^2+2\sigma^4\end{cases}&lt;/script&gt;

&lt;p&gt;For &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
T(x)=\begin{bmatrix}x&amp;x^2\end{bmatrix}^T %]]&gt;&lt;/script&gt;, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;var[x]=\sigma^2, \text{ as }x\sim\mathcal{N}(\mu,\sigma^2),&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;var[x^2]=\mathbb{E}[x^4]-\left(\mathbb{E}[x^2]\right)^2.&lt;/script&gt;

&lt;p&gt;For $\mathbb{E}[x^2]$, it follows that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}[x^2]=var[x]+(\mathbb{E}[x])^2=\sigma^2+\mu^2.&lt;/script&gt;

&lt;p&gt;For $\mathbb{E}[x^4]$, to compute it we leverage &lt;em&gt;moment generating functions&lt;/em&gt; which follows that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;M_X(t)=e^{\mu t+\frac{1}{2}\sigma^2t^2},\quad \mathbb{E}[x^4]=M^{(4)}_X(0).&lt;/script&gt;

&lt;p&gt;After a laborious computing, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}var[x^2]&amp;=\mathbb{E}[x^4]-\left(\mathbb{E}[x^2]\right)^2\\&amp;=3\sigma^4+6\sigma^2\mu^2+\mu^4-\sigma^4-2\sigma^2-\mu^4\\&amp;=4\sigma^2\mu^2+2\sigma^4.\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Therefore, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A’’(\hat\eta)=\begin{bmatrix}var[x]\\var[x^2]\end{bmatrix}.\tag*{$\blacksquare$}&lt;/script&gt;

&lt;p&gt;Finally, we show that $X\sim\mathcal{N}(\mu,\sigma^2)$ is the distribution that maximizes the entropy over all distributions $P$ satisfying&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}_P\left[\left(\frac{X-\mu}{\sigma}\right)^2\right]=1.&lt;/script&gt;

&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; Consider the expression we formulated in section 4,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x)=\exp\left\{\theta^T\Phi(x)-(\lambda+1)\right\},&lt;/script&gt;

&lt;p&gt;which maximizes the entropy while satisfying $\mathbb{E}_P[\Phi(x)]=\alpha$. Now letting&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\alpha&amp;=1,\\\Phi(x)&amp;=\frac{(x-\mu)^2}{\sigma^2},\\\theta&amp;=-\frac{1}{2},\\\exp\{-\lambda-1\}&amp;=\frac{1}{\sqrt{2\pi}\sigma}.\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Therefore we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{-\frac{1}{2\sigma^2}(x-\mu)^2\right\}.\tag*{$\blacksquare$}&lt;/script&gt;

&lt;h1 id=&quot;6-conclusion&quot;&gt;6. Conclusion&lt;/h1&gt;

&lt;p&gt;In this post, we briefly introduced the basic form of the exponential family. Then we discussed its properties from three perspectives: sufficient statistic, log-partition function and maximum entropy. Moreover, with one-dimension Gaussian distribution, we exemplified the properties.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Machine Learning - 06 Kernel Method</title>
   <link href="http://localhost:4000/2020/11/05/kernel_method-ml06/"/>
   <updated>2020-11-05T00:00:00+08:00</updated>
   <id>http://localhost:4000/2020/11/05/kernel_method-ml06</id>
   <content type="html">&lt;p&gt;&lt;em&gt;The notes are based on the &lt;a href=&quot;https://github.com/shuhuai007/Machine-Learning-Session&quot;&gt;session&lt;/a&gt;. For the fundamental of linear algebra, one can always refer to &lt;a href=&quot;http://math.mit.edu/~gs/linearalgebra/&quot;&gt;Introduction to Linear Algebra&lt;/a&gt; and &lt;a href=&quot;https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf&quot;&gt;The Matrix Cookbook&lt;/a&gt; for more details. Many thanks to these great works.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;0-introduction&quot;&gt;0. Introduction&lt;/h1&gt;

&lt;p&gt;Kernel methods are a class of algorithms for pattern analysis. The name of kernel methods comes from the use of &lt;em&gt;kernel function&lt;/em&gt;, which enable operations in a high-dimensional and implicit space. Specifically, by &lt;a href=&quot;https://en.wikipedia.org/wiki/Cover%27s_theorem&quot;&gt;Cover’s theorem&lt;/a&gt;, given a set of training data that is not &lt;em&gt;linearly separable&lt;/em&gt;, one can with high probability transform it into a training set that is linearly separable by projecting it into a higher-dimensional space via some &lt;em&gt;non-linear transformation&lt;/em&gt;. With the help of kernel function, the operation, &lt;em&gt;i.e.,&lt;/em&gt; inner product, it involves after transforming can be often computationally cheaper than the explicit computation. Such an approach is called the &lt;em&gt;kernel trick&lt;/em&gt;. In this post, we will focus on the application of kernel method to SVM.&lt;/p&gt;

&lt;h2 id=&quot;1-kernel-method&quot;&gt;1. Kernel method&lt;/h2&gt;

&lt;p&gt;Define the data set as &lt;script type=&quot;math/tex&quot;&gt;\mathcal{D}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}, X=\{x_1,x_2,\dots,x_N\}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Y=\{y_1,y_2,\dots,y_N\}&lt;/script&gt; where $x_i\in\mathbb{R}^{d\times 1}$ and &lt;script type=&quot;math/tex&quot;&gt;y_i\in\{-1,1\}&lt;/script&gt;. We further assume that the data set is non-linearly separable. Kernel method supposes that there is a non-linear transformation $\phi(x):\mathbb{R}^{d\times 1}\to\mathbb{R}^{p\times 1},d&amp;lt;p,$  such that &lt;script type=&quot;math/tex&quot;&gt;\mathcal{D}_p=\{(\phi(x_1),y_1),(\phi(x_2),y_2),\dots,(\phi(x_N),y_N)\}&lt;/script&gt; are linearly separable. For such a linearly separable data set, recalling the problem we formulated in section 1.3 of &lt;a href=&quot;https://19w6.github.io/2020/10/28/support_vector_machine-ml05/&quot;&gt;SVM&lt;/a&gt;, we have the duality problem&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\min_\lambda&amp;\quad \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^N\left(\lambda_i\lambda_jy_iy_j\phi^T(x_i)\phi(x_j)\right)-\sum_{i=1}^N\lambda_i\\\text{s.t.}&amp;\quad \lambda_i\ge0,i=1,2,\dots,N\\&amp;\quad \sum_{i=1}^N\lambda_iy_i=0.\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;However, after transforming, the inner product $\phi(x_i)^T\phi(x_j)=\langle\phi(x_i),\phi(x_j)\rangle$ could be hard to obtain (consider the case that $\phi(\cdot)$ has infinite dimensions), which requires the aid of &lt;em&gt;kernel function&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;2-kernel-function&quot;&gt;2. Kernel function&lt;/h2&gt;

&lt;p&gt;A kernel function is defined as $K:\mathbb{R}^{d\times 1}\times\mathbb{R}^{d\times 1}\to\mathbb{R}$. Specifically, for non-linear transformation $\phi(\cdot)\in\mathcal{H}\text{ (Hilbert space) }:\mathbb{R}^{d\times 1}\to\mathbb{R}^{p\times 1}$ and any $x_i,x_j\in\mathbb{R}^{d\times 1}$, we call $K(x_i,x_j)=\langle\phi(x_i),\phi(x_j)\rangle$ a kernel function. Such a kernel function is regarded &lt;em&gt;positive definite&lt;/em&gt;, which satisfies&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;K(x_i,x_j)=K(x_j,x_i),&lt;/script&gt;

&lt;p&gt;and for &lt;script type=&quot;math/tex&quot;&gt;x_{1},x_{2},\dots,x_{N}\in\mathbb{R}^{d\times 1},&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{K}=[K(x_{i},x_{j})]_{N\times N}\text{ is a positive semi-definite (PSD) matrix},&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\mathcal{K}&lt;/script&gt; is called &lt;em&gt;Gram matrix&lt;/em&gt; of $K$ over set &lt;script type=&quot;math/tex&quot;&gt;\{x_{1},x_{2},\dots,x_{N}\}&lt;/script&gt;. When the explicit expression of $\phi(\cdot)$ is hard to be determined, it quite often to show the positive definiteness of a kernel function via its corresponding Gram matrix.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(Properties)&lt;/strong&gt; We now show two &lt;em&gt;properties&lt;/em&gt; of kernel functions.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Let $K$ be kernel function such that &lt;script type=&quot;math/tex&quot;&gt;K:\mathbb{R}^{d\times 1}\times\mathbb{R}^{d\times 1}\to\mathbb{R}&lt;/script&gt;, then we define its Gram matrix $\mathcal{K}\in\mathbb{R}^{N\times N}$ over &lt;script type=&quot;math/tex&quot;&gt;\{x_1,x_2,\dots,x_N\}&lt;/script&gt; where $x_i\in\mathbb{R}^{d\times 1}$. Considering the mapping function $\phi(\cdot):\mathbb{R}^{d\times 1}\to\mathbb{R}^{p\times 1}$, we have&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;K(x_i,x_j)=\langle\phi(x_i),\phi(x_j)\rangle, \phi(\cdot)\in\mathcal{H}\implies \begin{cases}K(x_i,x_j)=K(x_j,x_i)\\ \mathcal{K}\text{ is a PSD matrix}\end{cases}.&lt;/script&gt;

&lt;p&gt;&lt;em&gt;Proof&lt;/em&gt;:&lt;/p&gt;

&lt;p&gt;By the definition of $K(x_i,x_j)$, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;K(x_i,x_j)=\langle \phi(x_i),\phi(x_j)\rangle,\quad K(x_j,x_i)=\langle \phi(x_j),\phi(x_i)\rangle.&lt;/script&gt;

&lt;p&gt;By the symmetry of inner product, we have &lt;script type=&quot;math/tex&quot;&gt;\langle \phi(x_i),\phi(x_j)\rangle=\langle \phi(x_j),\phi(x_i)\rangle&lt;/script&gt;. It then follows that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;K(x_i,x_j)=K(x_j,x_i).&lt;/script&gt;

&lt;p&gt;Therefore the Gramian matrix $\mathcal{K}=[K(x_{i},x_{j})]_{N\times N}$ is symmetric real matrix. Now we show that &lt;script type=&quot;math/tex&quot;&gt;\forall\alpha\in\mathbb{R}^{R\times 1}, \alpha^T\mathcal{K}\alpha\ge 0.&lt;/script&gt; The notation is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\alpha^T\mathcal{K}\alpha=(\alpha_1,\alpha_2,\dots,\alpha_N)\begin{bmatrix}K_{11}&amp;K_{12}&amp;\dots&amp;K_{1N}\\K_{21}&amp;K_{22}&amp;\dots&amp;K_{2N}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\K_{N1}&amp;K_{N2}&amp;\dots&amp;K_{NN}\end{bmatrix}\begin{pmatrix}\alpha_1\\\alpha_2\\\vdots\\\alpha_N\end{pmatrix}\end{aligned}, %]]&gt;&lt;/script&gt;

&lt;p&gt;where $K_{ij}=K(x_{ri},x_{rj})$. We then have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\alpha^T\mathcal{K}\alpha&amp;=\sum_{i=1}^R\sum_{j=1}^R \alpha_i\alpha_jK_{ij}\\&amp;=\sum_{i=1}^R\sum_{j=1}^R \alpha_i\alpha_j\phi^T(x_{ri})\phi(x_{rj})\\&amp;=\sum_{i=1}^R \alpha_i\phi^T(x_{ri})\sum_{j=1}^R\alpha_j\phi(x_{rj})\\&amp;=\left(\sum_{i=1}^R \alpha_i\phi(x_{ri})\right)^T\left(\sum_{j=1}^R\alpha_j\phi(x_{rj})\right)\\&amp;=\left\langle\left(\sum_{i=1}^R \alpha_i\phi(x_{ri})\right), \left(\sum_{j=1}^R \alpha_i\phi(x_{rj})\right)\right\rangle\\&amp;=\left\vert\left\vert\sum_{i=1}^R \alpha_i\phi(x_{ri})\right\vert\right\vert^2,\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;therefore, $\alpha^T\mathcal{K}\alpha\ge 0$ and $\mathcal{K}$ is a PSD matrix.$\tag*{$\blacksquare$}$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Let $\mathcal{K}\in\mathbb{R}^{d\times d}$ be a symmetric PSD matrix, then for &lt;script type=&quot;math/tex&quot;&gt;\{x_1,x_2,\dots,x_N\}&lt;/script&gt; where $x_i\in\mathbb{R}^{d\times 1}$, we have kernel function $K(x_i,x_j)=x_i^T\mathcal{K}x_j$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Proof&lt;/em&gt;:&lt;/p&gt;

&lt;p&gt;Consider the &lt;em&gt;diagonalisation&lt;/em&gt; of $\mathcal{K}=Q^T\Lambda Q$ by an orthogonal matrix $Q$, where $\Lambda$ is a diagnoal matrix containing the non-negative eigenvalues of $\mathcal{K}$. Let $\sqrt{\Lambda}$ be the diagonal matrix with the square roots of the eigenvalues and set $A=\sqrt{\Lambda}Q$.  Then for &lt;script type=&quot;math/tex&quot;&gt;\{x_1,x_2,\dots,x_N\}&lt;/script&gt; where $x_i\in\mathbb{R}^{d\times 1}$, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_i^T\mathcal{K}x_j=x_i^TQ^T\Lambda Qx_j=x_i^TA^TA x_j=\langle A x_i,Ax_j\rangle.&lt;/script&gt;

&lt;p&gt;Therefore we have kernel function $K(x_i,x_j)=x_i^T\mathcal{K}x_j=\langle Ax_i,Ax_j\rangle$ with linear transformation $\phi(\cdot)=A\cdot. \tag*{$\blacksquare$}$&lt;/p&gt;

&lt;h1 id=&quot;3-conclusion&quot;&gt;3. Conclusion&lt;/h1&gt;

&lt;p&gt;In this post, we introduced &lt;em&gt;kernel method&lt;/em&gt; for classification problem. Given &lt;em&gt;Cover’s theorem&lt;/em&gt;, we can project non-linear data into high-dimensional space and obtain linearly separable data. To simplify the computation incurred by the duality problem, we can leverage &lt;em&gt;kernel function&lt;/em&gt; to avoid the computing labor.&lt;/p&gt;

&lt;p&gt;This is definitely not a good introduction to kernel methods. For more details of kernel method, I would recommend &lt;a href=&quot;https://people.eecs.berkeley.edu/~jordan/kernels/0521813972c03_p47-84.pdf&quot;&gt;Kernel methods: an overview&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Machine Learning - 05 Support Vector Machine</title>
   <link href="http://localhost:4000/2020/10/28/support_vector_machine-ml05/"/>
   <updated>2020-10-28T00:00:00+08:00</updated>
   <id>http://localhost:4000/2020/10/28/support_vector_machine-ml05</id>
   <content type="html">&lt;p&gt;&lt;em&gt;The notes are based on the &lt;a href=&quot;https://github.com/shuhuai007/Machine-Learning-Session&quot;&gt;session&lt;/a&gt;. For the fundamental of linear algebra, one can always refer to &lt;a href=&quot;http://math.mit.edu/~gs/linearalgebra/&quot;&gt;Introduction to Linear Algebra&lt;/a&gt; and &lt;a href=&quot;https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf&quot;&gt;The Matrix Cookbook&lt;/a&gt; for more details. Many thanks to these great works.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;0-introduction&quot;&gt;0. Introduction&lt;/h1&gt;

&lt;p&gt;Support vector machine (SVM) is a supervised learning method for classification and regression analysis. It is one of the most robust prediction method. Here we mainly consider its applications in classification. Specifically, for the data of $d$-dimensional, we want to know whether we can separate classes with a $(d-1)$-dimensional &lt;em&gt;hyperplane&lt;/em&gt;. In particular, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class. According to whether the dataset is linearly separable or not, there are &lt;em&gt;hard-margin&lt;/em&gt; SVM, &lt;em&gt;soft-margin&lt;/em&gt; SVM and &lt;em&gt;kernel&lt;/em&gt; SVM.&lt;/p&gt;

&lt;h1 id=&quot;1-hard-margin-svm&quot;&gt;1. Hard-margin SVM&lt;/h1&gt;

&lt;p&gt;Hard-margin SVM works only when data is completely linearly separable without any errors.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/300px-SVM_margin.png&quot; width=&quot;350&quot; /&gt;
&lt;/div&gt;
&lt;h2 id=&quot;11-problem-formulation&quot;&gt;1.1. Problem formulation&lt;/h2&gt;

&lt;p&gt;Suppose we have data set &lt;script type=&quot;math/tex&quot;&gt;\mathcal{D}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}&lt;/script&gt; where $x_i\in\mathbb{R}^{d\times 1}$ is the data feature and $y_i\in{-1,1}$ is the corresponding class label. A case where $d=2$ is shown in the figure above. The hyperplane that separates the data is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w^Tx-b=0,&lt;/script&gt;

&lt;p&gt;where $w\in\mathbb{R}^{d\times 1}$ and $b\in\mathbb{R}$ are parameters to be learned. Then like what we arrived in &lt;a href=&quot;https://19w6.github.io/2020/10/14/linear_classification-ml03/&quot;&gt;perceptron&lt;/a&gt;, a correct classifier should ensure that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_i(w^Tx_i-b)&gt;0,\forall i=1,2,\dots,N.&lt;/script&gt;

&lt;p&gt;We further define the &lt;em&gt;margin&lt;/em&gt; as the parallel lines that has the minimum distance from the data to the hyperplane. In &lt;em&gt;hard-margin&lt;/em&gt; SVM, we need to find the &lt;em&gt;maximum-margin&lt;/em&gt; hyperplane that maximizes the distance, which can be described by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\max_{w,b}\min_{i}&amp;\quad \frac{\vert w^Tx_i-b\vert}{\vert\vert w\vert\vert}\\\text{s.t.}&amp;\quad y_i(w^Tx_i-b)&gt;0,\forall i=1,2,\dots,N\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;p&gt;The problem further is equivalent to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{alignat*}{3}\max_{w,b}\min_{i}&amp;\quad \frac{y_i(w^Tx_i-b)}{\vert\vert w\vert\vert}\implies\max_{w,b}\frac{1}{\vert\vert w\vert\vert}\min_i&amp;\quad y_i(w^Tx_i-b)\\\text{s.t.}&amp;\quad y_i(w^Tx_i-b)&gt;0,\forall i=1,2,\dots,N\end{alignat*}. %]]&gt;&lt;/script&gt;

&lt;p&gt;For the constraint $y_i(w^Tx_i-b)&amp;gt;0, \forall i=1,2,\dots,N$, there exists a positive parameter $r&amp;gt;0$ such that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_i y_i(w^Tx_i-b)=r.&lt;/script&gt;

&lt;p&gt;As there are many $w,b$ available for the separation as long as they have the same directions. We add a new constraint that $r=1$. Then it follows that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_i y_i(w^Tx_i-b)=y_i((w_{\text{old}}^T/r)x_i-b_{\text{old}}/r)=1.&lt;/script&gt;

&lt;p&gt;The problem then is transformed into&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{alignat*}{3}&amp;&amp;\max_{w,b}\frac{1}{\vert\vert w\vert\vert}\min_i y_i(w^Tx_i-b)&amp;\implies\min_{w,b}\quad \frac{1}{2}w^Tw\\&amp;&amp;\text{s.t.}\quad y_i(w^Tx_i-b)&gt;0&amp;\implies\text{s.t.}\quad y_i(w^Tx_i-b)\ge 1,i=1,2,\dots,N\end{alignat*}, %]]&gt;&lt;/script&gt;

&lt;p&gt;which is a linearly constrained &lt;em&gt;quadratic optimization&lt;/em&gt; (QP) problem.&lt;/p&gt;

&lt;h2 id=&quot;12-lagrange-duality&quot;&gt;1.2. Lagrange duality&lt;/h2&gt;

&lt;p&gt;The following content is about &lt;a href=&quot;https://web.stanford.edu/~boyd/cvxbook/&quot;&gt;convex optimization&lt;/a&gt;. In section 1, we have the &lt;em&gt;primal problem&lt;/em&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\min_{w,b}&amp;\quad \frac{1}{2}w^Tw\\\text{s.t.}&amp;\quad y_i(w^Tx_i-b)\ge 1,i=1,2,\dots,N\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;p&gt;The &lt;em&gt;Lagrangian&lt;/em&gt; for the problem is a function defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(w,b,\lambda)=\frac{1}{2}w^Tw+\sum_{i=1}^N\lambda_i\left(1-y_i\left(w^Tx_i-b\right)\right),&lt;/script&gt;

&lt;p&gt;where $\lambda_i\ge 0$ is the &lt;em&gt;Lagrange multiplier&lt;/em&gt; associated with the constraints. Consider the problem of $\max_\lambda L(w,b,\lambda)$,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\max_{\lambda\ge 0} L(w,b,\lambda)=\begin{cases}\frac{1}{2}w^Tw+\infty&amp;\text{if }\exists i\in\{1,2,\dots,N\}\text{ s.t. }1-y_i(w^Tx_i-b)&gt;0,\\\frac{1}{2}w^Tw+0&amp;\text{otherwise.}\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;The problem makes sense (non infinity) only when the original constraint is satisfied. In that case, the primal problem is equivalent to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\min_{w,b}\max_\lambda&amp;\quad L(w,b,\lambda)\\\text{s.t.}&amp;\quad \lambda_i\ge 0,i=1,2,\dots,N\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;p&gt;Then we define the &lt;em&gt;Lagrange dual function&lt;/em&gt; for the primal problem as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g(\lambda)=\min_{w,b}L(w,b,\lambda).&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;Actually, the correct definition of the &lt;em&gt;Lagrange dual function&lt;/em&gt; should be&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;g(\lambda)=\inf_{w,b}L(w,b,\lambda).&lt;/script&gt;

  &lt;p&gt;Here we assume the minimum exists and the infimum is the minimum for understanding.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The &lt;em&gt;Lagrange dual problem&lt;/em&gt; of the original problem is then defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\max_{\lambda}&amp;\quad g(\lambda)\\\text{s.t.}&amp;\quad \lambda_i\ge 0,i=1,2,\dots,N\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;p&gt;The dual problem is introduced for its convexity. Specifically, notice that the infimum (minimum in this case) of $g(\lambda)$ is unconstrained as opposed to the original constrained minimization problem. Further, $g(\lambda)$ is concave with respect to $\lambda$ regardless of the original problem.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Primal Problem&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Lagrange Dual Problem&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned}\min_{w,b}\max_\lambda&amp;\quad L(w,b,\lambda)\\\text{s.t.}&amp;\quad \lambda_i\ge 0,i=1,2,\dots,N\end{aligned} %]]&gt;&lt;/script&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned}\max_{\lambda}\min_{w,b}&amp;\quad L(w,b,\lambda)\\\text{s.t.}&amp;\quad \lambda_i\ge 0,i=1,2,\dots,N\end{aligned} %]]&gt;&lt;/script&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;A natural problem is whether the two problems are equivalent. Obviously, the equivalence is obtained if and only if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{w,b}\max_{\lambda} L(w,b,\lambda)=\max_{\lambda}\min_{w,b} L(w,b,\lambda).&lt;/script&gt;

&lt;p&gt;If the equation holds, we say the &lt;em&gt;strong duality&lt;/em&gt; holds. It can also be shown that the &lt;em&gt;weak duality&lt;/em&gt; always holds as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{w,b}\max_{\lambda} L(w,b,\lambda)\ge\max_{\lambda}\min_{w,b} L(w,b,\lambda).&lt;/script&gt;

&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Obviously, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\max_\lambda L(w,b,\lambda)\ge L(w,b,\lambda)\ge\min_{w,b}L(w,b,\lambda).&lt;/script&gt;

&lt;p&gt;Define $F(w,b)=\max_\lambda L(w,b,\lambda)$ and $G(\lambda)=\min_{w,b}L(w,b,\lambda)$. According the above inequality, it follows that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F(w,b)\ge G(\lambda)\implies\min_{w,b}F(w,b)\ge\max_\lambda G(\lambda).&lt;/script&gt;

&lt;p&gt;Therefore we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{w,b}\max_{\lambda} L(w,b,\lambda)\ge\max_{\lambda}\min_{w,b} L(w,b,\lambda).\tag*{$\blacksquare$}&lt;/script&gt;

&lt;p&gt;Solving the dual problem in fact is used to find nontrivial lower bounds for difficult original problems. In our case, the strong duality holds for the linearly constrained QP problem. Thus to solve the primal problem is to solve the dual problem.&lt;/p&gt;

&lt;h2 id=&quot;13-karushkuhntucker-conditions&quot;&gt;1.3. Karush–Kuhn–Tucker conditions&lt;/h2&gt;

&lt;p&gt;For the primal problem and its dual problem, if the strong duality holds, then &lt;em&gt;Karush–Kuhn–Tucker (KKT) conditions&lt;/em&gt; are satisfied as&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;(a). Primal Feasibility:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;y_i(w^Tx_i-b)\ge1,i=1,2,\dots,N&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;(b). Dual Feasibility:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\lambda_i\ge 0,i=1,2,\dots,N&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;(c). Complementary Slackness:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat\lambda_i\left(1-y_i\left(\hat{w}^Tx_i-\hat b\right)\right)=0,i=1,2,\dots,N&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;(d). Zero gradient of Lagrangian with respect to $w,b$:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial L}{\partial b}=0,\quad \frac{\partial L}{\partial w}=0.&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The conditions (a) and (b) are the original constraints. As for condition (c), recall that we define $y_i(w^Tx_i-b)=1$ for the data that is exactly &lt;script type=&quot;math/tex&quot;&gt;1/\vert\vert w\vert\vert&lt;/script&gt; away from the hyperplane $w^Tx-b=0$, &lt;em&gt;i.e.,&lt;/em&gt; on the margin of the hyperplane. For those which are not on the margin, to satisfy KKT conditions, it must follow that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat\lambda_k=0,k\in\{i\vert y_i(w^Tx_i-b)&gt;1,i=1,2,\dots,N\}.&lt;/script&gt;

&lt;p&gt;The condition (d) is for the dual problem. Specifically, we consider the unconstrained problem $\min_{w,b}L(w,b,\lambda)$ in the dual problem. For the differentiable function $L(w,b,\lambda)$ , by &lt;em&gt;Fermat’s theorem&lt;/em&gt;, the extremum exists when condition (d) is satisfied, $i.e.,$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial L}{\partial b}=\sum_{i=1}^N\lambda_iy_i=0,\\\frac{\partial L}{\partial w}=w-\sum_{i=1}^N\lambda_iy_ix_i=0.&lt;/script&gt;

&lt;p&gt;Solving the equations we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^N\lambda_iy_i=0,\forall b,\quad \hat w=\sum_{i=1}^N\lambda_iy_ix_i.&lt;/script&gt;

&lt;p&gt;Plugging them into $L(w,b,\lambda)$, we can transform the problem into&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\max_{\lambda\ge0}&amp;\quad \min_{w,b}L(w,b,\lambda)\\\implies\max_{\lambda\ge0}&amp;\quad \frac{1}{2}\sum_{i=1}^{N}\left(\lambda_iy_ix_i^T\right)\sum_{i=1}^{N}\left(\lambda_iy_ix_i\right)+\sum_{i=1}^N\lambda_i-\sum_{i=1}^N\lambda_iy_i\left(\sum_{j=1}^N\lambda_jy_jx_j^T\right)x_i\\\text{s.t.}&amp;\quad \sum_{i=1}^N\lambda_iy_i=0\\\implies\max_{\lambda\ge0}&amp;\quad -\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^N\left(\lambda_i\lambda_jy_iy_jx_i^Tx_j\right)+\sum_{i=1}^N\lambda_i\\\text{s.t.}&amp;\quad \sum_{i=1}^N\lambda_iy_i=0\\\implies \min_\lambda&amp;\quad \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^N\left(\lambda_i\lambda_jy_iy_jx_i^Tx_j\right)-\sum_{i=1}^N\lambda_i\\\text{s.t.}&amp;\quad \lambda_i\ge0,i=1,2,\dots,N\\&amp;\quad \sum_{i=1}^N\lambda_iy_i=0\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The optimal $\hat\lambda$ can be obtained by &lt;em&gt;sequential minimal optimization&lt;/em&gt; (SMO) algorithm. Here we assume we already have the optimal value. Notice we have $\sum_{i=1}^N\hat\lambda_iy_i=0$, which means there exists at least one $\hat\lambda_k\ne0$ otherwise $\hat w=0$. According our analysis, $\hat\lambda_k\ne 0$ only when&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_k(w^Tx_k-b)-1=0.&lt;/script&gt;

&lt;p&gt;Therefore we have the solution&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat w=\sum_{i=1}^N\hat\lambda_iy_ix_i,&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat b=\sum_{i=1}^N\hat\lambda_iy_ix_i^Tx_k-y_k.&lt;/script&gt;

&lt;p&gt;Accordingly, the hyperplane is the linear combination of the data on the margin with corresponding $\hat\lambda_k&amp;gt;0$. We call those data &lt;em&gt;support vectors&lt;/em&gt; from where the name SVM comes.&lt;/p&gt;

&lt;h2 id=&quot;2-soft-margin-svm&quot;&gt;2. Soft-margin SVM&lt;/h2&gt;

&lt;p&gt;In practice, there are noise and outliers among the data, which makes the data nonlinearly separable. In that case, hard-margin fails to work. Now we introduce &lt;em&gt;soft-margin SVM&lt;/em&gt; which extends SVM to the nonlinearly separable data. Recall that in hard-margin SVM, we have the constraint&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_i(w^Tx_i-b)\ge 1,i=1,2,\dots,N&lt;/script&gt;

&lt;p&gt;which confines the model to the linearly separable case. To extent the model to general cases, we introduce &lt;em&gt;loss function&lt;/em&gt;, which can be defined as&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The number of wrongly classifying:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\text{loss}=\sum_{i=1}^N I(y_i(w^Tx_i-b)&lt;1), %]]&gt;&lt;/script&gt;

    &lt;p&gt;where $I(\cdot)$ is the indicator function. However, such loss function is not differentiable.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The sum of the distances between the hyperplane and the outliers:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\text{loss}_i&amp;=\begin{cases}0&amp;y_i(w^Tx_i-b)\ge 1\\1-y_i(w^Tx_i-b)&amp;y_i(w^Tx_i-b)&lt;1\text{ (wrongly classified)}\end{cases}\\&amp;=\max\{0, 1-y_i(w^Tx_i-b)\}.\\\text{loss}&amp;=\sum_{i=1}^N\text{loss}_i,\end{aligned} %]]&gt;&lt;/script&gt;

    &lt;p&gt;which is called &lt;strong&gt;hinge loss&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, the $\max$ in the hinge loss is not differentiable neither. We now adapt the original constraint as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_i(w^Tx_i-b)\ge 1-\xi_i,i=1,2,\dots,N&lt;/script&gt;

&lt;p&gt;where $\xi_i\ge0$ and $\sum_{i=1}^N\xi_i\le$ constant are called &lt;em&gt;slack variables&lt;/em&gt;. The slack variables is introduced to allow for some points to be on the wrong side of the margin. Specifically, for the points that are on the wrong side, it will break the original constraint $y_i(w^Tx_i-b)\ge 1$ as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
y_i(w^Tx_i-b)=\xi&lt; 1. %]]&gt;&lt;/script&gt;

&lt;p&gt;With slack variables, such classification is allowed as long as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\xi\ge 1-\xi_i\implies\xi_i\ge1-\xi.&lt;/script&gt;

&lt;p&gt;Moreover, we do not want the $\xi_i$ to be too large to distinguish points correctly. Thus we have the new formulation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\min_{w,b}&amp;\quad \frac{1}{2}w^Tw+C\sum_{i=1}^N\xi_i\\\text{s.t.}&amp;\quad y_i(w^Tx_i-b)\ge 1-\xi_i\\&amp;\quad \xi_i\ge 0\\&amp;\quad i=1,2,\dots,N\end{aligned}, %]]&gt;&lt;/script&gt;

&lt;p&gt;where $C$ is the &lt;em&gt;cost&lt;/em&gt; parameter that determines to what extent we allow for outliers. To solve the problem one can refer to the hard-margin case as they are actually similar.&lt;/p&gt;

&lt;h1 id=&quot;4-conclusion&quot;&gt;4. Conclusion&lt;/h1&gt;

&lt;p&gt;In this post, we first introduced hard-margin SVM for linearly separable data. By introducing a loss function and slack variables, soft-margin SVM allows for noise and outliers so that it can handle non linear case. The two models can both be solved by &lt;em&gt;convex optimization&lt;/em&gt; methods. For convex optimization, we briefly reviewed &lt;em&gt;Lagrange duality&lt;/em&gt;, &lt;em&gt;Slater’s condition&lt;/em&gt; and &lt;em&gt;KKT conditions&lt;/em&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Machine Learning - 04 Dimensionality Reduction</title>
   <link href="http://localhost:4000/2020/10/26/dimensionality_reduction-ml04/"/>
   <updated>2020-10-26T00:00:00+08:00</updated>
   <id>http://localhost:4000/2020/10/26/dimensionality_reduction-ml04</id>
   <content type="html">&lt;p&gt;&lt;em&gt;The notes are based on the &lt;a href=&quot;https://github.com/shuhuai007/Machine-Learning-Session&quot;&gt;session&lt;/a&gt;. For the fundamental of linear algebra, one can always refer to &lt;a href=&quot;http://math.mit.edu/~gs/linearalgebra/&quot;&gt;Introduction to Linear Algebra&lt;/a&gt; and &lt;a href=&quot;https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf&quot;&gt;The Matrix Cookbook&lt;/a&gt; for more details. Many thanks to these great works.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;0-curse-of-dimensionality&quot;&gt;0. Curse of Dimensionality&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;The following introduction is derived from &lt;a href=&quot;http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf&quot;&gt;Pattern Recognition and Machine Learning&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;High dimensionality often incurs not only calculation issue but also &lt;em&gt;overfitting&lt;/em&gt;. The increasing in dimension can point to the methods becoming rapidly unwieldy and of limited practical utility.&lt;/p&gt;

&lt;p&gt;Our geometrical intuitions, formed through a life spent in a space of three-dimension, can fail badly when we consider spaces of higher dimensionality. As a simple example, consider a sphere of radius $r = 1$ in a space of $d$ dimensions, and ask what is the fraction of the volume of the sphere that lies between radius $r=1-\varepsilon$ and $r=1$. We can evaluate this fraction by noting that the volume of a sphere of radius $r$ in $d$ dimensions must scale as $r^d$, and so we write&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V_d(r)=K_dr^d,&lt;/script&gt;

&lt;p&gt;where the constant $K_d$ depends only on $d$. Thus the required fraction is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{V_d(1)-V_d(1-\varepsilon)}{V_d(1)}=\frac{K_dr^d-K_d(1-\varepsilon)^d}{K_dr^d}=1-(1-\varepsilon)^d.&lt;/script&gt;

&lt;p&gt;Obviously, for large $d$, this fraction tends to 1 even for small values of $\varepsilon$. Thus, in spaces of high dimensionality, most of the volume of a sphere is concentrated in a thin shell near the surface!&lt;/p&gt;

&lt;p&gt;Luckily, for those data of high dimensionality, we can refer to &lt;em&gt;dimensionality reduction&lt;/em&gt; algorithms.&lt;/p&gt;

&lt;h1 id=&quot;1-sample-mean-and-variance&quot;&gt;1. Sample Mean and Variance&lt;/h1&gt;

&lt;p&gt;Before we move on, we give the following definitions. Suppose we have data &lt;script type=&quot;math/tex&quot;&gt;X=(x_1,x_2,\dots,x_N)^T\in\mathbb{R}^{N\times d}&lt;/script&gt;, where $x_i\in\mathbb{R}^{d\times 1}$ is a sample with $d$ features. Specifically,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
X=(x_1,x_2,\dots,x_N)^T=\begin{pmatrix}x_{1}^T\\x_{2}^T\\\vdots\\x_{N}^T\end{pmatrix}=\begin{pmatrix}x_{11}&amp;x_{12}&amp;\cdots&amp;x_{1d}\\x_{21}&amp;x_{22}&amp;\cdots&amp;x_{2d}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\x_{N1}&amp;x_{N2}&amp;\cdots&amp;x_{Nd}\end{pmatrix}_{N\times d}. %]]&gt;&lt;/script&gt;

&lt;p&gt;Then we have the &lt;strong&gt;sample mean&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar X=\frac{1}{N}\sum_{i=1}^Nx_i,&lt;/script&gt;

&lt;p&gt;and the &lt;strong&gt;sample covariance&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S=\frac{1}{N}\sum_{i=1}^N\left(x_i-\bar X\right)\left(x_i-\bar X\right)^T.&lt;/script&gt;

&lt;p&gt;However, the &lt;em&gt;sum&lt;/em&gt; operation is inconvenient in calculating. Thus we further transform them into&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar X=\frac{1}{N}(x_1,x_2,\dots,x_N)\begin{pmatrix}1\\1\\\vdots\\1\end{pmatrix}=\frac{1}{N}X^T\mathbf{1}_{N\times 1},&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}S&amp;=\frac{1}{N}\underbrace{(x_1-\bar X,x_2-\bar X,\dots,x_N-\bar X)}_{X^T-\bar X(1,1,\dots,1)}\begin{pmatrix}(x_1-\bar X)^T\\(x_2-\bar X)^T\\\vdots\\(x_N-\bar X)^T\end{pmatrix}\\&amp;=\frac{1}{N}\left(X^T-\bar X\mathbf{1}_{1\times N}\right)\left(X^T-\bar X\mathbf{1}_{1\times N}\right)^T\\&amp;=\frac{1}{N}\left(X^T-\frac{1}{N}X^T\mathbf{1}_{N\times 1}\mathbf{1}_{1\times N}\right)\left(X^T-\frac{1}{N}X^T\mathbf{1}_{N\times 1}\mathbf{1}_{1\times N}\right)^T\\&amp;=\frac{1}{N}X^T\underbrace{\left(\mathbf{I}_{N}-\frac{1}{N}\mathbf{1}_{N\times N}\right)}_{H}\left(\mathbf{I}_{N}-\frac{1}{N}\mathbf{1}_{N\times N}\right)^TX\\&amp;=\frac{1}{N}X^THH^TX\\&amp;=\frac{1}{N}X^THX,\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $H$ is &lt;em&gt;centering matrix&lt;/em&gt;, and it can be shown that $H^T=H,H^n=H$.&lt;/p&gt;

&lt;h1 id=&quot;2-principal-component-analysis&quot;&gt;2. Principal Component Analysis&lt;/h1&gt;

&lt;p&gt;Principal component analysis (PCA) is defined as an &lt;a href=&quot;https://en.wikipedia.org/wiki/Orthogonal_transformation&quot;&gt;orthogonal&lt;/a&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_transformation&quot;&gt;linear transformation&lt;/a&gt; that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.&lt;/p&gt;

&lt;p&gt;Recall that in &lt;a href=&quot;https://19w6.github.io/2020/10/14/linear_classification-ml03/&quot;&gt;the previous post&lt;/a&gt;, we introduce &lt;em&gt;linear discriminant analysis&lt;/em&gt; (LDA), which requires a projection that maximizes the distance between different classes. In PCA, instead of classes, we need to find a projection that maximizes the distance (variance) of all the data so that we can reduce the dimensionality (as a projection involves one dimensionality reduction) while losing the least information (as the greatest variance implies most information). Now we give the mathematically formulation.&lt;/p&gt;

&lt;p&gt;The notation in section 1 will be used in this section as well. Since PCA is sensitive to the variance of the data, it is critical to normalize the variables over all dimensions, which yields&lt;/p&gt;

&lt;p&gt;$z_i=x_i-\bar X.$&lt;/p&gt;

&lt;p&gt;In many case, the values of $z_i\in\mathbb{R}^{d\times 1}$ should be scaled in $[0,1]$ but here we just consider the simple case. We then denote the transformation vector as $W=(w_1,w_2,\dots,w_d)\in\mathbb{R}^{d\times d}$ which is the &lt;em&gt;orthogonal basis&lt;/em&gt; of the new coordinate system. Then the new data $\tilde z_i$, &lt;em&gt;i.e.&lt;/em&gt;, the projection of $z_i$ on the new coordinate system is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\tilde z_i=z_i^TW=(z_i^Tw_1,z_i^Tw_2,\dots,z_i^Tw_d),&lt;/script&gt;

&lt;p&gt;where $z_i^Tw_j\in\mathbb{R}$ is the value of $j$-th dimension of the new data. Now suppose we want reduce the dimension from $d$ to $k$, &lt;em&gt;i.e.,&lt;/em&gt; we want to preserve the first $k$ dimensions of $\tilde z_i$ while maximizing the variance, which gives us the objective function&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{J}(w)=\sum_{j=1}^k\sum_{i=1}^N\frac{1}{N}\left(z_i^Tw_j-\overline{z^Tw_j}\right)^2,&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^N\frac{1}{N}\left(z_i^Tw_j-\overline{z^Tw_j}\right)^2&lt;/script&gt; is the variance of the new data in $j$-th dimension. &lt;script type=&quot;math/tex&quot;&gt;\overline{z^Tw_j}&lt;/script&gt; is the mean of the new data in $j$-th dimension, which is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\overline{z^Tw_j}&amp;=\frac{1}{N}\sum_{i=1}^Nz_i^Tw_j\\&amp;=\frac{1}{N}\sum_{i=1}^N(x_i-\bar X)^Tw_j\\&amp;=\frac{1}{N}\left(\sum_{i=1}^Nx_i^Tw_j-N\bar X^Tw_j\right)\\&amp;=\frac{1}{N}\left(\sum_{i=1}^Nx_i^Tw_j-\sum_{i=1}^Nx_i^Tw_j\right)\\&amp;=0\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;p&gt;Therefore the objective function follows that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\mathcal{J}(w)&amp;=\sum_{j=1}^k\sum_{i=1}^N\frac{1}{N}\left(z_i^Tw_j\right)^2\\&amp;=\sum_{j=1}^k\sum_{i=1}^N\frac{1}{N}\left((x_i-\bar X)^Tw_j\right)^2\\&amp;=\sum_{j=1}^k\sum_{i=1}^N\frac{1}{N}w_j^T(x_i-\bar X)(x_i-\bar X)^Tw_j\\&amp;=\sum_{j=1}^kw_j^TSw_j\end{aligned}, %]]&gt;&lt;/script&gt;

&lt;p&gt;where each $w_j^TSw_j$ can be maximized independently since $w_j$ are orthogonal. Combined with the constraint that $w_j$ is a unit vector, the problem is then equivalent to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat w_j=\arg\max_{w_j} w_j^TSw_j\quad \text{s.t. }w_j^Tw_j=1.&lt;/script&gt;

&lt;p&gt;The problem can be solved by &lt;a href=&quot;https://en.wikipedia.org/wiki/Lagrange_multiplier&quot;&gt;the method of Lagrange multipliers&lt;/a&gt; as follows,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\mathcal{L}(w_j,\lambda_j)&amp;=w_j^TSw_j+\lambda_j(1-w_j^Tw_j),\\\frac{\partial\mathcal{L}(w_j,\lambda_j)}{\partial w_j}&amp;=2Sw_j-2\lambda_j w_j.\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Setting the second equation to be zero, we have the standard eigenvalue problem&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S\hat{w}_j=\lambda_j\hat{w}_j\implies SW=\text{diag}(\lambda)W ,&lt;/script&gt;

&lt;p&gt;&lt;em&gt;i.e.,&lt;/em&gt; $\hat w_j$ is actually the eigenvalue of $S$. Plugging the above equation to the objective function, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\max\mathcal{J(\hat{w})}&amp;=\max\sum_{j=1}^k\hat{w}_j^TS\hat{w}_j\\&amp;=\max\sum_{j=1}^k\hat{w}_j^T\lambda_j\hat{w}_j\\&amp;=\max\sum_{j=1}^k\lambda_j\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;p&gt;Therefore, to reduce the data from $d$ to $k$ dimension, one should select the $k$ eigenvectors of $S$ corresponding to the $k$ largest eigenvalues to construct the $W\in\mathbb{R}^{d\times k}$, which is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;W=(w_{(1)},w_{(2)},\dots,w_{(k)}),&lt;/script&gt;

&lt;p&gt;where $w_{(i)}$ is the eigenvector corresponding to the $i$-th largest eigenvalue.  Then the data in the new coordinate system is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X^\text{new}=(x_1-\bar X,x_2-\bar X,\dots, x_N-\bar X)^TW.&lt;/script&gt;

&lt;h1 id=&quot;3-principal-component-analysis---an-svd-perspective&quot;&gt;3. Principal Component Analysis - An SVD Perspective&lt;/h1&gt;

&lt;p&gt;Now we consider the &lt;em&gt;singular vector decomposition&lt;/em&gt; (SVD) of the centralized data,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;HX=U\Sigma V^T,&lt;/script&gt;

&lt;p&gt;where $U=(u_1,u_2,\dots,u_N)\in\mathbb{R}^{N\times N},\Sigma\in\mathbb{R}^{N\times d},V=(v_1,v_2,\dots,v_N)\in\mathbb{R}^{d\times d}$, and they have the following properties,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}UU^T&amp;=U^TU=\mathbf{I}_{N},\\VV^T&amp;=V^TV=\mathbf{I}_{d},\\\Sigma_{ij}=0,\quad i&amp;=0,1,…,N,j=0,1,…,d,i\ne j.\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;We further represent $\Sigma$ as $\lambda(\sigma_1,\sigma_2,\dots,\sigma_d)$. Then according to the analysis in section 1, the covariance of the data is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}S&amp;=\frac{1}{N}X^THX\\&amp;=\frac{1}{N}X^TH^THX\\&amp;=\frac{1}{N}(HX)^THX\\&amp;=\frac{1}{N}V\Sigma^TU^TU\Sigma V^T\\&amp;=V\text{diag}\left(\frac{\sigma_1^2}{N},\frac{\sigma_2^2}{N},\dots,\frac{\sigma_d^2}{N}\right)V^T\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;p&gt;By multiplying $V$ on both sides, it follows that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;SV=V\text{diag}\left(\frac{\sigma_1^2}{N},\frac{\sigma_2^2}{N},\dots,\frac{\sigma_d^2}{N}\right)\implies Sv_i=\frac{\sigma_i^2}{N}v_i,&lt;/script&gt;

&lt;p&gt;which is consistent with the conclusion of PCA. Therefore, instead of decomposing the covariance matrix $S$, we can conduct an SVD on the centralized data, which gives the transformation matrix that allows us to obtain the new data&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z=HX\cdot V.&lt;/script&gt;

&lt;p&gt;By selecting $k$ vectors of $V$ according to the single values, we can reduce the original data matrix from $d$ to $k$ dimensions. Such decomposition may take advantage when $N\ll d$. Intuitively, $HX\in\mathbb{R}^{N\times d}$ and $S\in\mathbb{R}^{d\times d}$. When $N\ll d$, decomposing $HX$ should be more efficient than decomposing $S$.&lt;/p&gt;

&lt;h1 id=&quot;4-principal-coordinates-analysis&quot;&gt;4. Principal Coordinates Analysis&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;Principal coordinates analysis&lt;/em&gt; (PCoA) is a well known technique in many fields. It actually can be derived from PCA. Specifically, we consider a matrix&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T=HXX^TH^T.&lt;/script&gt;

&lt;p&gt;By SVD, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T=U\Sigma V^TV\Sigma^TU^T=U\Sigma\Sigma^T U^T,&lt;/script&gt;

&lt;p&gt;which is similar to the decomposition of $S$ in section 3. Further, by multiplying $U\Sigma$ on the both sides, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;TU\Sigma=U\Sigma\Sigma^TU^TU\Sigma=U\Sigma\text{diag}\left(\sigma_1^2,\sigma_2^2,\dots,\sigma_d^2\right)\implies T(U\Sigma)_i=\sigma^2_i(U\Sigma)_i.&lt;/script&gt;

&lt;p&gt;Therefore, $U\Sigma$ is actually composed of $d$  eigenvalues of $T$. Recall that in section 3, we have the new data as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z=HX\cdot V=U\Sigma V^T\cdot V=U\Sigma,&lt;/script&gt;

&lt;p&gt;which implies that by the eigenvalue decomposition of $T$, we can get the new data directly. Such a dimensionality reduction technique with a different perspective is PCoA. Notice $T\in\mathbb{R}^{N\times N}$, thus the complexity of PCoA is $O(N^2)$.&lt;/p&gt;

&lt;h1 id=&quot;5-probabilistic-principal-component-analysis&quot;&gt;5. Probabilistic Principal Component Analysis&lt;/h1&gt;

&lt;p&gt;Just like the notations we used in previous sections, we define the new data we want to transform $X$ into is $Z=(z_1,z_2,\dots,z_N)$ where $z_i\in\mathbb{R}^{k\times 1}$. However, in &lt;em&gt;probabilistic principal component analysis&lt;/em&gt; (PPCA), we further introduce randomness as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z_i\sim\mathcal{N}(\mathbf{0}_{k\times 1}, \mathbf{I}_{k\times k}),&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_i=Wz_i+\mu+\varepsilon,&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\varepsilon\sim\mathcal{N}(\mathbf{0}_{d\times 1},\sigma^2\mathbf{I}_{d\times d}),&lt;/script&gt;

&lt;p&gt;where $W\in\mathbb{R}^{d\times k}, \mu\in\mathbb{R}^{d\times 1}$ and $\sigma^2$ are the parameters to be learned ($\mu$ can be viewed as the bias term of many machine learning model). Such randomization can generalize the model to the unseen data. One can also refer such a model to &lt;em&gt;linear Gaussian model&lt;/em&gt;. In particular,  there are two phases. The first is learning:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat W,\hat \mu, \hat \sigma^2=\arg\max_{W,\mu,\sigma^2}P(X\vert Z;W,\mu,\sigma^2).&lt;/script&gt;

&lt;p&gt;The second is inference (dimensionality reduction):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z=\arg\max_{\tilde Z} P(\tilde Z\vert X).&lt;/script&gt;

&lt;p&gt;The learning part can be dealt with &lt;em&gt;expectation–maximization algorithm&lt;/em&gt;. The following is the details of the inference procedure. According to the definition, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_i\vert z_i\sim\mathcal{N}(Wz_i+\mu,\sigma^2\mathbf{I}_{d\times d}),&lt;/script&gt;

&lt;p&gt;where $z_i$ is a sample rather than a random variable. Also, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}[x_i]=\mathbb{E}[Wz_i+\mu+\varepsilon]=\mu,&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;var[x_i]=var[Wz_i+\mu+\varepsilon]=var[Wz_i]+var[\varepsilon]=WW^T+\sigma^2\mathbf{1}_{d\times d},&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_i\sim\mathcal{N}(\mu, WW^T+\sigma^2\mathbf{1}_{d\times d}).&lt;/script&gt;

&lt;p&gt;Then we consider the covariance $Cov(x_i,z_i)$,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}Cov(x_i,z_i)&amp;=\mathbb{E}[(x_i-\mu)(z_i-\mathbf{0}_{k\times 1})^T]\\&amp;=\mathbb{E}[(x_i-\mu)z_i^T]\\&amp;=\mathbb{E}[(Wz_i+\varepsilon)z_i^T]\\&amp;=\mathbb{E}[Wz_iz_i^T]+\mathbb{E}[\varepsilon z_i^T]\\&amp;=Wvar[z_i]+\mathbb{E}[\varepsilon]\mathbb{E}[z_i^T]\\&amp;=W\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;p&gt;Hence the union distribution of $(x_i,z_i)^T$ is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{pmatrix}x_i\\z_i\end{pmatrix}\sim\mathcal{N}\left(\begin{pmatrix}\mu\\\mathbf{0}_{k\times1} \end{pmatrix},\begin{pmatrix}WW^T+\sigma^2\mathbf{1}_{d\times d}&amp;W\\W^T&amp;\mathbf{1}_{k\times k}\end{pmatrix}\right). %]]&gt;&lt;/script&gt;

&lt;p&gt;By the formula derived in &lt;a href=&quot;https://19w6.github.io/2020/09/28/intro-ml01/&quot;&gt;session 01&lt;/a&gt;, it can be shown that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z_i\vert x_i\sim\mathcal{N}(W^T(WW^T+\sigma^2\mathbf{1}_{d\times d})^{-1}(x_i-\mu),\mathbf{1}_{k\times k}-W^T(WW^T+\sigma^2\mathbf{1}_{d\times d})^{-1}W).&lt;/script&gt;

&lt;h1 id=&quot;6-conclusion&quot;&gt;6. Conclusion&lt;/h1&gt;

&lt;p&gt;In this post, we introduced the naive &lt;em&gt;principal component analysis&lt;/em&gt; (PCA) model. Then we conducted &lt;em&gt;singular vector decomposition&lt;/em&gt; on the centralized data, which gives us the same conclusion as that of PCA. Such conclusion can also be derived from &lt;em&gt;Principal coordinates analysis&lt;/em&gt; (PCoA) model. Further, by introducing parameters, &lt;em&gt;probabilistic principal component analysis&lt;/em&gt; (PPCA) can generalize PCA to handle unseen data.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Machine Learning - 03 Linear Classification</title>
   <link href="http://localhost:4000/2020/10/14/linear_classification-ml03/"/>
   <updated>2020-10-14T00:00:00+08:00</updated>
   <id>http://localhost:4000/2020/10/14/linear_classification-ml03</id>
   <content type="html">&lt;p&gt;&lt;em&gt;The notes are based on the &lt;a href=&quot;https://github.com/shuhuai007/Machine-Learning-Session&quot;&gt;session&lt;/a&gt;. For the fundamental of linear algebra, one can always refer to &lt;a href=&quot;http://math.mit.edu/~gs/linearalgebra/&quot;&gt;Introduction to Linear Algebra&lt;/a&gt; and &lt;a href=&quot;https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf&quot;&gt;The Matrix Cookbook&lt;/a&gt; for more details. Many thanks to these great works.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;0-introduction&quot;&gt;0. Introduction&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;The following introduction is derived from the &lt;a href=&quot;http://pages.stat.wisc.edu/~wahba/stat860public/pdf1/liu.zhang.wu.lum.11.pdf&quot;&gt;paper&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As a supervised learning technique, the goal of classification is to construct a classification rule based on a training set where both data and class labels are given. Once obtained, the classification rule can then be used for class prediction of new objects whose covariates are available.&lt;/p&gt;

&lt;p&gt;Among various classification methods, there are two main groups: &lt;em&gt;soft&lt;/em&gt; and &lt;em&gt;hard&lt;/em&gt; classification. In particular, a soft classification rule generally estimates the class &lt;em&gt;conditional probabilities&lt;/em&gt; explicitly and then makes the class prediction &lt;em&gt;based on the estimated probability&lt;/em&gt;. Depending on whether calculating the conditional probability directly or approximating it by a model, there are &lt;em&gt;generative classifiers&lt;/em&gt; and &lt;em&gt;discriminant classifiers&lt;/em&gt; among the &lt;em&gt;soft&lt;/em&gt; methods. In contrast, hard classification bypasses the requirement of class probability estimation and directly estimates the &lt;em&gt;classification boundary&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Typical soft classifiers include some traditional distribution-based likelihood approaches such as logistic regression. On the other hand, some margin-based approaches such as perceptron and the SVM, generally distributional assumption-free, belong to the class of hard classification methods.&lt;/p&gt;

&lt;p&gt;We assume the data set is linearly separable in the following subsections.&lt;/p&gt;

&lt;h1 id=&quot;1-perceptron&quot;&gt;1. Perceptron&lt;/h1&gt;

&lt;p&gt;Perceptron is a &lt;em&gt;hard&lt;/em&gt; method for &lt;em&gt;binary classification&lt;/em&gt;. Suppose we have i.i.d. data &lt;script type=&quot;math/tex&quot;&gt;\mathcal{D}=\{(x_1,y_1), (x_2,y_2),\dots,(x_N,y_N)\},X=\{x_1,x_2,\dots,x_N\}, Y=\{y_1,y_2,\dots,y_N\}&lt;/script&gt; where $x_i\in\mathbb{R}^{d\times 1}$ can be viewed as the feature and &lt;script type=&quot;math/tex&quot;&gt;y_i\in\{-1,1\}&lt;/script&gt; is the corresponding label. In particular, we denote &lt;script type=&quot;math/tex&quot;&gt;X_{c1}=\{x_i\vert y_i=+1\}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;X_{c2}=\{x_i\vert y_i=-1\}&lt;/script&gt; as the set of class $c_1$ and class $c_2$, respectively. Moreover, let $N_1=| X_{c1}|$ and $N_2=|X_{c2}|$, where $N_1+N_2=N$. The model of perceptron follows&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(w)=\text{sign}(w^Tx),&lt;/script&gt;

&lt;p&gt;where $w\in\mathbb{R}^{d\times 1}$ and $\text{sign}(\cdot)$ is the sign function. Perceptron is actually an error-driven method. Specifically, for data $(x_i,y_i)$, the correctness of perceptron can be described as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
y_iw^Tx_i\ge0\iff\begin{cases}w^Tx_i\ge0,\ f(w)=1,&amp;\text{if } y_i=+1\\w^Tx_i&lt;0,\ f(w)=-1, &amp;\text{if }y_i=-1\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;Define &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\tilde D=\{(x,y)\vert y_iw^T x_i&lt;0, i=1,\dots,N\} %]]&gt;&lt;/script&gt; be the set of data that was classified incorrectly. Then the loss function of the model can be defined as the size of $\tilde D$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mathcal{L}(w)=\sum_{i=1}^NI(y_iw^Tx_i&lt;0), %]]&gt;&lt;/script&gt;

&lt;p&gt;where $I(\cdot)$ is the indicator function. Though such a loss function is intuitive, it is uncontinuous and can be hard to be optimized. From the standpoint of the model, to make $y_iw^Tx_i\ge0$ is equivalent to make $y_iw^Tx_i$ as larger as possible, thus we can transform the loss function into&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}(w)=\sum_{i=1}^N-y_iw^Tx_i,&lt;/script&gt;

&lt;p&gt;which can be minimized by various optimization methods such as &lt;em&gt;stochastic gradient descent&lt;/em&gt;.&lt;/p&gt;

&lt;h1 id=&quot;2-linear-discriminant-analysis&quot;&gt;2. Linear Discriminant Analysis&lt;/h1&gt;

&lt;p&gt;Now we introduce &lt;em&gt;linear discriminant analysis&lt;/em&gt; (LDA), which is a method for &lt;em&gt;binary classification&lt;/em&gt;. Note that in some materials, LDA is defined as a dimensionality reduction technique. Further, we introduce LDA method  in this note from a hard classification perspective. The soft perspective can be also found in other materials. The notations for data in this subsection are the same as that in subsection 1. We further define the mean&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar x_{c1}=\frac{1}{N_1}\sum_{x\in X_{c1}}x,\quad \bar x_{c2}=\frac{1}{N_2}\sum_{x\in X_{c2}}x,&lt;/script&gt;

&lt;p&gt;and the variance&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S_{c1}=\frac{1}{N_1}\sum_{x\in X_{c1}}(x-\bar x_{c1})(x-\bar x_{c1})^T,\quad S_{c2}=\frac{1}{N_2}\sum_{x\in X_{c2}}(x-\bar x_{c2})(x-\bar x_{c2})^T.&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;The idea of LDA is proposed by Ronald Fisher in 1988: maximize the distance between the mean of each class and minimize the spreading within the class itself.&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;In LDA, we consider the ‘&lt;em&gt;projection&lt;/em&gt;’ of $x$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z=w^Tx,&lt;/script&gt;

&lt;p&gt;where $w\in\mathbb{R}^{d\times 1}$ is a unit vector to be learned. Specifically, the scalar $z$ is the length of the projection of $x$ on $w$, thus we can view such $z$ as the projection of $x$ into a &lt;em&gt;one dimensional subspace&lt;/em&gt;. Note that the definition here is different from the definition of projection in &lt;em&gt;Introduction to Linear Algebra&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Then we have the following definitions about the &lt;em&gt;mean&lt;/em&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar z =\frac{1}{N}\sum_{i=1}^Nw^Tx_i,\quad \bar z_1=\frac{1}{N_1}\sum_{x\in X_{c1}}w^Tx,\quad \bar z_2=\frac{1}{N_2}\sum_{x\in X_{c2}}w^Tx.&lt;/script&gt;

&lt;p&gt;Similarly, we have the definitions related to the variance as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S=\frac{1}{N}\sum_{i=1}^{N}(w^Tx_i-\bar z)^2,\quad S_1=\frac{1}{N_1}\sum_{x\in X_{c1}}(w^Tx-\bar z_1)^2,\quad S_2=\frac{1}{N_2}\sum_{x\in X_{c2}}(w^Tx-\bar z_2)^2.&lt;/script&gt;

&lt;p&gt;Then we use the mean to define the distance between the two class and the variance to represent the spreading within the class itself. LDA is then to find the unit vector $\hat w$ that maximizes&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{J}(w)=\frac{(\bar z_1-\bar z_2)^2}{S_1+S_2}.&lt;/script&gt;

&lt;p&gt;For the numerator, it follows that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}(\bar z_1-\bar z_2)^2&amp;=\left(\frac{1}{N_1}\sum_{x\in X_{c1}}w^Tx_i-\frac{1}{N_2}\sum_{x\in X_{c2}}w^Tx_i\right)^2\\&amp;=w^T\left(\bar x_{c1}-\bar x_{c_2}\right)\left(\bar x_{c1}-\bar x_{c_2}\right)^Tw\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;p&gt;For the denominator, it follows that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}S_1+S_2&amp;=\frac{1}{N_1}\sum_{x\in X_{c1}}(w^Tx-\bar z_1)^2+\frac{1}{N_2}\sum_{x\in X_{c2}}(w^Tx-\bar z_2)^2\\&amp;=w^T\left[\frac{1}{N_1}\sum_{x\in X_{c1}}(x-\bar x_{c1})(x-\bar x_{c1})^T\right]w+w^T\left[\frac{1}{N_2}\sum_{x\in X_{c2}}(x-\bar x_{c2})(x-\bar x_{c2})^T\right]w\\&amp;=w^TS_{c1}w+w^TS_{c2}w\\&amp;=w^T(S_{c1}+S_{c2})w\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;p&gt;Therefore, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{J}(w)=\frac{w^TS_bw}{w^TS_ww},&lt;/script&gt;

&lt;p&gt;where $S_b=\left(\bar x_{c1}-\bar x_{c_2}\right)\left(\bar x_{c1}-\bar x_{c_2}\right)^T$ represents the distance &lt;em&gt;between-class&lt;/em&gt;, $S_w=S_{c1}+S_{c2}$ represents the spreading &lt;em&gt;within-class&lt;/em&gt;. Such transformation is actually for computing derivation. $J(w)$ can be maximized by taking the derivative w.r.t $w$ and setting it to be $0$. Specifically,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\frac{\partial \mathcal{J}(w)}{\partial w}&amp;=\frac{\left(\frac{\partial}{\partial w}w^TS_b w\right)w^TS_ww-w^TS_bw\left(\frac{\partial}{\partial w}w^TS_w w\right)}{(w^TS_ww)^2}\\&amp;=\frac{(2S_bw)w^TS_ww-w^TS_bw(2S_ww)}{(w^TS_ww)^2}\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;p&gt;Setting it to be 0 is equivalent to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}(2S_bw)w^TS_ww-w^TS_bw(2S_ww)&amp;=0\\ (w^TS_bw)S_ww&amp;=S_bw(w^TS_ww)\\S_w w&amp;=\frac{w^TS_ww}{w^TS_bw}S_bw\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;p&gt;As $w\in\mathbb{R}^{d\times 1}$ and $S_w,S_b\in\mathbb{R}^{d\times d}$, the term $(w^TS_ww)/(w^TS_bw)\in\mathbb{R}$. For convenience, we denote it as $\lambda$. Then we have an equivalent &lt;em&gt;generalized eigenvalue problem&lt;/em&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S_ww=\lambda S_bw.&lt;/script&gt;

&lt;p&gt;If one of $S_b$ and $S_w$ has full rank, the generalized eigenvalue problem can be converted into a standard eigenvalue problem. However, to solve the problem entails complex computation. We now assume $S_w^{-1}$ exists. Recall that $w$ is a unit vector. Thus what we need to care is only the direction of $w$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\hat w&amp;\propto \lambda S_w^{-1}S_bw\\&amp;\propto \lambda S_w^{-1}\left(\bar x_{c1}-\bar x_{c_2}\right)\left(\bar x_{c1}-\bar x_{c_2}\right)^Tw\\&amp;\propto\lambda_1S_w^{-1}\left(\bar x_{c1}-\bar x_{c_2}\right)\\&amp;\propto S_w^{-1}\left(\bar x_{c1}-\bar x_{c_2}\right)\end{aligned}, %]]&gt;&lt;/script&gt;

&lt;p&gt;where $\lambda_1=\lambda \left(\bar x_{c1}-\bar x_{c_2}\right)^Tw$ is a scalar as $\left(\bar x_{c1}-\bar x_{c_2}\right)^Tw\in\mathbb{R}$.&lt;/p&gt;

&lt;h1 id=&quot;3-discriminant-classifiers&quot;&gt;3. Discriminant Classifiers&lt;/h1&gt;

&lt;p&gt;Discriminant classifiers focus on the classification problem directly. Specifically, discriminant classifiers  model the posterior $P(Y\vert X)$, then makes the class prediction based on the estimated probability.&lt;/p&gt;

&lt;h2 id=&quot;31-logistic-regression&quot;&gt;3.1. Logistic regression&lt;/h2&gt;

&lt;p&gt;Logistic regression inputs the result of a &lt;em&gt;linear regression&lt;/em&gt; to a &lt;em&gt;sigmoid function&lt;/em&gt; to make classification. A sigmoid function is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma(z)=\frac{1}{1+e^{-z}}&lt;/script&gt;

&lt;p&gt;which maps $z\in(-\infty,+\infty)$ into a probability $[0,1]$. Therefore we can model the posterior probability as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(y=1|x;w)=\sigma(w^Tx)=\frac{1}{1+e^{-w^Tx}},&lt;/script&gt;

&lt;p&gt;where $w$ is the parameter to be learned. As for $P(y=-1\vert x;w)$, it can be obtained by $1-P(y=1\vert x;w)$ since we are considering a binary classification problem. However, for a supervised learning technique, it would be convenient to consider both two labels into one function. To this end, we set the label value to be &lt;script type=&quot;math/tex&quot;&gt;y_i\in\{0,1\}&lt;/script&gt; . Moreover, we denote $P(y_i=1\vert x_i;w)$ and $P(y_i=0\vert x_i;w)$ as $p_{i\cdot 1}$ and $p_{i\cdot 0}$, respectively. Then logistic regression is to find $\hat w$ that maximizes&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{J}(w)=P(Y|X;w)=\prod_{i=1}^N p_{i\cdot 1}^{y_i}p_{i\cdot 0}^{1-y_i}.&lt;/script&gt;

&lt;p&gt;Given the dataset $\mathcal{D}$, such maximization problem can be solved by &lt;em&gt;maximum likelihood estimation&lt;/em&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\hat w&amp;=\arg\max_w \log P(Y\vert X;w)\\&amp;=\arg\max_w \log\prod_{i=1}^N p_{i\cdot 1}^{y_i}p_{i\cdot 0}^{1-y_i}\\&amp;=\arg\max_w \sum_{i=1}^N(y_i\log p_{i\cdot 1}+(1-y_i)\log p_{i\cdot 0})\\&amp;=\arg\max_w \sum_{i=1}^N[y_i\log \sigma(w^Tx_i)+(1-y_i)\log (1-\sigma(w^Tx_i))]\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;The term &lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^N(y_i\log \sigma(w^Tx_i)+(1-y_i)\log (1-\sigma(w^Tx_i)))&lt;/script&gt; is actually the negative of &lt;em&gt;cross entropy&lt;/em&gt; over $P(Y)$ and $\sigma(w^TX)$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To solve the above problem, one can refer to SGD method.&lt;/p&gt;

&lt;h1 id=&quot;4-generative-classifiers&quot;&gt;4. Generative Classifiers&lt;/h1&gt;

&lt;p&gt;For a binary classification problem, we actually have no need to know the specfic value of &lt;script type=&quot;math/tex&quot;&gt;P(y=1\vert x)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;P(y=0\vert x)&lt;/script&gt;. What matters is whether &lt;script type=&quot;math/tex&quot;&gt;P(y=1\vert x)&gt;P(y=0\vert x)&lt;/script&gt; or not. Unlike discriminant methods which model and compute the posterior probability directly, in &lt;em&gt;generative classifiers&lt;/em&gt;, we compare the posterior probability in an indirect way. Specifically, by &lt;em&gt;Bayes’s theorem&lt;/em&gt;, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(Y|X)=\frac{P(X|Y)P(Y)}{P(X)}\propto P(X|Y)P(Y).&lt;/script&gt;

&lt;p&gt;Therefore, to compare the posterior probability is to compare the union probability. The classification predicted by generative classifiers is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat y=\arg\max_{y\in\{0,1\}}P(y\vert x)=\arg\max_{y\in\{0,1\}}P(x\vert y)P(y).&lt;/script&gt;

&lt;p&gt;In generative classifier methods, a key problem is how to model the likelihood &lt;script type=&quot;math/tex&quot;&gt;P(x\vert y)&lt;/script&gt; and the prior $P(y)$.&lt;/p&gt;

&lt;h2 id=&quot;41-naive-bayes-classifier&quot;&gt;4.1. Naive Bayes classifier&lt;/h2&gt;

&lt;p&gt;Naive Bayes classifier is the simplest generative classifier. For a binary classification problem, suppose the feature of $x_i$ is composed of $(x_{i1},x_{i2},\dots,x_{id})$. Then naive Bayes classifier assumes not only the independence among the data but also that &lt;em&gt;every pair of the feature is independent&lt;/em&gt;, &lt;em&gt;i.e.,&lt;/em&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{im}\vert y_i\perp x_{in}\vert y_i,m,n=1,2,\dots,d \text{ and }m\ne n.&lt;/script&gt;

&lt;p&gt;Then the likelihood becomes&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x_i\vert y_i)=\prod_{j=1}^dP(x_{ij}\vert y_i).&lt;/script&gt;

&lt;p&gt;Further, it models the prior and each feature as,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y_i\sim\text{Bern}(\phi),\quad x_{ij}\vert y_i\sim\mathcal{N}(\mu_{j},\sigma_j^2),&lt;/script&gt;

&lt;p&gt;where $\phi,\mu_j$, and $\sigma_{j}$ are parameters that can be learned by MLE method. Note that such model is just a common case. The key idea of naive Bayes classifier is its independence assumption. Specifically, naive Bayes classifier is not a single method but a family of methods. By assuming the independence, it can be extremely fast compared with other classification methods.&lt;/p&gt;

&lt;h2 id=&quot;42-gaussian-discriminant-analysis&quot;&gt;4.2. Gaussian discriminant analysis&lt;/h2&gt;

&lt;p&gt;As a generative method, &lt;em&gt;Gaussian discriminant analysis&lt;/em&gt; (GDA) models the prior and the likelihood as follows,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y\sim\text{Bern}(\phi),\quad x\vert y=0\sim\mathcal{N}(\mu_1,\Sigma),\quad x\vert y=1\sim\mathcal{N}(\mu_2,\Sigma),&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\phi,\mu_1,\mu_2&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;\Sigma&lt;/script&gt; are parameters to be learned. We define $w=(\phi, \mu_1,\mu_2,\Sigma)$. Then GDA is to find $\hat w$ that maximizes&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\mathcal{J}(w)&amp;=\log \prod_{i=1}^N P(x_i\vert y_i;\mu_1,\mu_2,\Sigma)P(y_i;\phi)\\&amp;=\sum_{i=1}^N\left(\log P(x_i\vert y_i;\mu_1,\mu_2,\Sigma)+\log P(y_i;\phi)\right)\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;p&gt;Similar to the case in section 3.1, we represent the likelihood and the prior as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x_i\vert y_i;\mu_1,\mu_2,\Sigma)=\rho^{y_i}(\mu_1,\Sigma)\rho^{1-y_i}(\mu_2,\Sigma),\quad P(y_i;\phi)=\phi^{y_i}(1-\phi)^{1-y_i},&lt;/script&gt;

&lt;p&gt;where $\rho(\mu_1,\Sigma)$ and $\rho(\mu_2,\Sigma)$ are the PDF of &lt;script type=&quot;math/tex&quot;&gt;x\vert y=0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;x\vert y=1&lt;/script&gt;, respectively. Then it follows that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\mathcal{J}(w)&amp;=\sum_{i=1}^N\left(\log\rho^{y_i}(\mu_1,\Sigma)\rho^{1-y_i}(\mu_2,\Sigma)+\log\phi^{y_i}(1-\phi)^{1-y_i}\right)\\&amp;=\sum_{i=1}^N\left(y_i\log \rho(\mu_1,\Sigma)+(1-y_i)\log\rho(\mu_2,\Sigma)+y_i\log\phi+(1-y_i)\log(1-\phi)\right)\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;p&gt;Then to find &lt;script type=&quot;math/tex&quot;&gt;\hat w&lt;/script&gt; that maximizes $\mathcal{J}(w)$ is equivalent to set the derivation of $\mathcal{J}(w)$ w.r.t $w$ to be zero.&lt;/p&gt;

&lt;p&gt;For $\hat \phi$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}\frac{\partial \mathcal{J}(w)}{\partial\phi}=\sum_{i=1}^N\left(\frac{y_i}{\phi}-\frac{1-y_i}{1-\phi}\right)\end{aligned}.&lt;/script&gt;

&lt;p&gt;Solving &lt;script type=&quot;math/tex&quot;&gt;\partial \mathcal{J}(w)/\partial\phi=0&lt;/script&gt;, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat \phi=\frac{1}{N}\sum_{i=1}^N y_i=\frac{N_1}{N}.&lt;/script&gt;

&lt;p&gt;For $\hat\mu_1$ (or &lt;script type=&quot;math/tex&quot;&gt;\hat\mu_2&lt;/script&gt; likewise):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\frac{\partial\mathcal{J}(w)}{\partial\mu_1}&amp;=\frac{\partial\left(\sum_{i=1}^N y_i\log \rho(\mu_1,\Sigma)\right)}{\partial\mu_1}\\&amp;=\frac{\partial\left(\sum_{i=1}^Ny_i\log\left(\frac{1}{(2\pi)^{d/2}\vert\Sigma\vert^{1/2}}\exp\left(-\frac{1}{2}(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)\right)\right)\right)}{\partial\mu_1}\\&amp;=\frac{\partial\left(-\frac{1}{2}\sum_{i=1}^Ny_i(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)\right)}{\partial\mu_1}\\&amp;=\frac{\partial\left(-\frac{1}{2}\sum_{i=1}^Ny_i(x_i^T\Sigma^{-1}x_i-x_i^T\Sigma^{-1}\mu_1-\mu_1^T\Sigma^{-1}x_i+\mu_1^T\Sigma^{-1}\mu_1)\right)}{\partial\mu_1}\\&amp;=\sum_{i=1}^Ny_i\left(\Sigma^{-1}\mu_1-\Sigma^{-1}x_i\right)\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf&quot;&gt;The Matrix Cookbook&lt;/a&gt;:&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial x^Ta}{\partial x}=\frac{\partial a^Tx}{\partial x}=a.&lt;/script&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial x^TBx}{\partial x}=(B+B^T)x.&lt;/script&gt;
&lt;/blockquote&gt;

&lt;p&gt;Solving &lt;script type=&quot;math/tex&quot;&gt;\partial \mathcal{J}(w)/\partial\mu_1=0&lt;/script&gt;, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat \mu_1=\frac{\sum_{i=1}^N y_ix_i}{\sum_{i=1}^N y_i}=\bar x_{c1}.&lt;/script&gt;

&lt;p&gt;Similarly, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat\mu_2=\bar x_{c2}.&lt;/script&gt;

&lt;p&gt;For $\hat\Sigma$, we first consider the following transformation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\sum_{i=1}^N\left(y_i\log \rho(\mu_1,\Sigma)+(1-y_i)\log\rho(\mu_2,\Sigma)\right)&amp;=\sum_{x\in X_{c1}}\log\rho(\mu_1,\Sigma)+\sum_{x\in X_{c2}}\log\rho(\mu_2,\Sigma)\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;p&gt;As $\Sigma$ is shared by both &lt;script type=&quot;math/tex&quot;&gt;\rho(\mu_1,\Sigma)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\rho(\mu_2,\Sigma)&lt;/script&gt;, we consider the expansion of $\rho(\mu_1,\Sigma)$ for example,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\sum_{x\in X_{c1}}\log\rho(\mu_1,\Sigma)&amp;=\sum_{x\in X_{c1}}\log\left(\frac{1}{(2\pi)^{d/2}\vert\Sigma\vert^{1/2}}\exp\left(-\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\right)\right)\\&amp;=\underbrace{\sum_{x\in X_{c1}}-\frac{d}{2}\log2\pi}_{\text{constant }\lambda_1}-\sum_{x\in X_{c1}}\left(\frac{1}{2}\log\vert \Sigma\vert+\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\right)\\&amp;=\lambda_1-\frac{N_1}{2}\log\vert\Sigma\vert-\frac{1}{2}\sum_{x\in X_{c1}}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;p&gt;For the third term, notice that $(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\in\mathbb{R}$, thus&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\sum_{x\in X_{c1}}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)&amp;=\text{tr}\left(\sum_{x\in X_{c1}}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\right)\\&amp;=\text{tr}\left(\sum_{x\in X_{c1}}(x-\mu_1)(x-\mu_1)^T\Sigma^{-1}\right)\\&amp;=N_1\text{tr}\left(S_{c1}\Sigma^{-1}\right)\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf&quot;&gt;The Matrix Cookbook&lt;/a&gt;:&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{tr}(ABC)=\text{tr}(CAB)=\text{tr}(BCA).&lt;/script&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial\text{tr}(AB)}{\partial A}=B^T.&lt;/script&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial \vert A\vert}{\partial A}=\vert A\vert A^{-1}.&lt;/script&gt;
&lt;/blockquote&gt;

&lt;p&gt;Hence,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{x\in X_{c1}}\log\rho(\mu_1,\Sigma)=\lambda_1-\frac{N_1}{2}\log\vert\Sigma\vert-\frac{N_1}{2}\text{tr}\left(S_{c1}\Sigma^{-1}\right).&lt;/script&gt;

&lt;p&gt;Similarly,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{x\in X_{c2}}\log\rho(\mu_2,\Sigma)=\lambda_2-\frac{N_2}{2}\log\vert\Sigma\vert-\frac{N_2}{2}\text{tr}\left(S_{c2}\Sigma^{-1}\right).&lt;/script&gt;

&lt;p&gt;Then it follows that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\frac{\partial\mathcal{J}(w)}{\partial\Sigma}&amp;=\frac{\partial \left(\sum_{i=1}^N\left(y_i\log \rho+(1-y_i)\log\rho\right)\right)}{\partial\Sigma}\\&amp;=\frac{\partial \left(\sum_{x\in X_{c1}}\log\rho+\sum_{x\in X_{c2}}\log\rho\right)}{\partial\Sigma}\\&amp;=-\frac{1}{2}\frac{\partial \left(N\log\vert\Sigma\vert+N_1\text{tr}\left(S_{c1}\Sigma^{-1}\right)+N_2\text{tr}\left(S_{c2}\Sigma^{-1}\right)\right)}{\partial\Sigma}\\&amp;=-\frac{1}{2}\left(N\Sigma^{-1}-N_1S_{c1}\Sigma^{-2}-N_2S_{c2}\Sigma^{-2}\right)\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;p&gt;By setting &lt;script type=&quot;math/tex&quot;&gt;\partial \mathcal{J}(w)/\partial\Sigma=0&lt;/script&gt;, we finally arrive at&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat\Sigma=\frac{N_1S_{c1}+N_2S_{c2}}{N}.&lt;/script&gt;

&lt;h1 id=&quot;5-conclusion&quot;&gt;5. Conclusion&lt;/h1&gt;

&lt;p&gt;In this post, we introduced five linear classifiers. Among these models, $\mathcal{L}(w)$ is to be minimized, while $\mathcal{J}(w)$ is to be maximized. We omitted the prediction part of a classification problem. What we focused is actually how to model these data, especially in those generative cases.&lt;/p&gt;

&lt;p&gt;This post is obviously a long story. Moreover, there are many other things stoped my writing occasionally these days. It definitely has some logical problems, let alone typos, to be fixed. Anyway, I made it. Hope next time I can do better.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Machine Learning - 02 Linear Regression</title>
   <link href="http://localhost:4000/2020/10/07/linear_regression-ml02/"/>
   <updated>2020-10-07T00:00:00+08:00</updated>
   <id>http://localhost:4000/2020/10/07/linear_regression-ml02</id>
   <content type="html">&lt;p&gt;&lt;em&gt;The notes are based on the &lt;a href=&quot;https://github.com/shuhuai007/Machine-Learning-Session&quot;&gt;session&lt;/a&gt;. Many thanks to the great work.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;1-least-squares-method&quot;&gt;1. Least Squares Method&lt;/h1&gt;

&lt;p&gt;Suppose we have the IID data &lt;script type=&quot;math/tex&quot;&gt;\mathcal{D}=\{(x_1,y_1), (x_2,y_2),\dots,(x_N,y_N)\},&lt;/script&gt; where $x_i\in\mathbb{R}^{d\times 1}, y_i\in\mathbb{R}, i=1,2,\dots,N$. One can view $x_i$ as a feature vector and $y_i$ is the corresponding label. Denote&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
X=(x_1,x_2,\dots,x_N)^T=\begin{pmatrix}x_{11}&amp;x_{12}&amp;\dots&amp;x_{1d}\\x_{21}&amp;x_{22}&amp;\dots&amp;x_{2d}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\x_{N1}&amp;x_{12}&amp;\dots&amp;x_{1d}\end{pmatrix}_{N\times d},\quad Y=\begin{pmatrix}y_1\\y_2\\\vdots\\y_N\end{pmatrix}_{N\times 1}. %]]&gt;&lt;/script&gt;

&lt;p&gt;The problem is given the data set, we need to find a function to fit these data while minimizing the sum of squared error. In this case, we only focus on linear function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(w)=w^Tx,&lt;/script&gt;

&lt;p&gt;where $w\in\mathbb{R}^{d\times 1}$ is the unknown weight we need to learn. Here we ignore the bias term as it can be represented by adding a new dimension to the variables. The &lt;em&gt;loss function&lt;/em&gt; of the least squares method is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}(w)=\sum_{i=1}^N||w^Tx_i-y_i||^2.&lt;/script&gt;

&lt;p&gt;In this case, $w^Tx_i\in\mathbb{R}$ and $y_i\in\mathbb{R}$. Thus we can expand the loss function as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\mathcal{L}(w)&amp;=\underbrace{\begin{pmatrix}w^Tx_1-y_1&amp;w^Tx_2-y_2&amp;\dots&amp;w^Tx_N-y_N\end{pmatrix}}_{w^T\begin{pmatrix}x_1&amp;x_2&amp;\dots&amp;x_N\end{pmatrix}-\begin{pmatrix}y_1&amp;y_2&amp;\dots&amp;y_N\end{pmatrix}}\begin{pmatrix}w^Tx_1-y_1\\w^Tx_2-y_2\\\vdots\\w^Tx_N-y_N\end{pmatrix}\\&amp;=(w^TX^T-Y^T)(Xw-Y)\\&amp;=w^TX^TXw-2w^TX^TY+Y^TY\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;p&gt;Such expansion is for the derivative of the loss. Thus we have the optimal weight where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{alignat*}{3}\hat w&amp;=\arg\min_w \mathcal{L}(w)\Rightarrow\frac{\partial \mathcal{L}(w)}{\partial w}&amp;=0\end{alignat*}. %]]&gt;&lt;/script&gt;

&lt;p&gt;Solving the equation, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{w}=(X^TX)^{-1}X^TY.&lt;/script&gt;

&lt;h1 id=&quot;2-least-squares-method---a-matrix-perspective&quot;&gt;2. Least Squares Method - A Matrix Perspective&lt;/h1&gt;

&lt;p&gt;Knowledge about &lt;a href=&quot;http://math.mit.edu/~gs/linearalgebra/&quot;&gt;vectors and space&lt;/a&gt; is required in this section. We consider using a new approximation linear function&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h(w)=Xw,&lt;/script&gt;

&lt;p&gt;where $w\in\mathbb{R}^{d\times 1}$ and it is quite similar to $f(w)$ we defined in section 1. One should keep in mind that a matrix multiple a vector yields a linear combination of the &lt;strong&gt;column vectors&lt;/strong&gt; of the matrix. We will use $X_1,X_2,\dots,X_N$ to represent the column vector of $X$. Ideally, we want to find the $w$ subject to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X w=Y.&lt;/script&gt;

&lt;p&gt;If such $w$ exists, then we can solve it directly and easily. However, in practice error is inevitable and such $w$ often does not exist, which means $Y$ is not a linear combination of $X_i$ and they do not share the same space. What we can do is finding a linear combination of $X_i$ so that it has the least Euclidean distance to $Y$.&lt;/p&gt;

&lt;p&gt;As I am too lazy to depict the picture, I would like to give an imaginable example for that. One can also get the picture easily with these examples. The following discussion is based on a three-dimensional space with $xyz$ axes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ideal case&lt;/strong&gt;: Suppose we have the data &lt;script type=&quot;math/tex&quot;&gt;x_1, x_2, x_3 = \{(1,0,0)^T,(0,1,0)^T,(0,0,1)^T\}, Y=(1,1,1)^T&lt;/script&gt;. Thus&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
X=(x_1,x_2, x_3)^T=\begin{pmatrix}1&amp;0&amp;0\\0&amp;1&amp;0\\0&amp;0&amp;1\end{pmatrix},\quad Y=\begin{pmatrix}1\\1\\1\end{pmatrix}. %]]&gt;&lt;/script&gt;

&lt;p&gt;With the knowledge of vectors, it can be easily found that we can obtain $Y$ by $1\cdot X_1+1\cdot X_2+1\cdot X_3$, which gives the solution for $X\hat{w}=Y$ as $\hat{w}=(1,1,1)^T$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Practical case&lt;/strong&gt;: In practice, things may be different. Suppose we have $x_1, x_2, x_3 = \{(1,0.2,0.5)^T,(0,1,0)^T,(0,0,0)^T\}, Y=(1,1,1)^T$. Thus&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
X=(x_1,x_2, x_3)^T=\begin{pmatrix}1&amp;0.2&amp;0.5\\0&amp;1&amp;0\\0&amp;0&amp;0\end{pmatrix},\quad Y=\begin{pmatrix}1\\1\\1\end{pmatrix}. %]]&gt;&lt;/script&gt;

&lt;p&gt;With a manual drafting, one can find there is no way to get $Y$ by the linear combination of $X_1,X_2$ and $X_3$: all the $X_i$ lies in $xy$ surface while $Y$ exists in $xyz$ space. In such case, what we can do is finding a line on $xy$ surface as close to $Y$ as possible. Obviously, the closest one is the projection of $Y$ on $xy$ surface, which is $(1,1,0)^T$ .&lt;/p&gt;

&lt;p&gt;The key is how to find the projection $X\hat{w}$ numerically rather than intuitively. Consider the vector $Y-X\hat{w}$, which starts from the end of $X\hat{w}$ pointing to the end of $Y$. $X\hat{w}$ is the projection if and only if $Y-X\hat w$ is perpendicular to all the column vectors of $X$. Therefore it follows that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}X^T(Y-X\hat w)&amp;=\mathbf{0}\\X^TX\hat w&amp;=X^TY\\\hat w&amp;=(X^TX)^{-1}X^TY,\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;which is consistent with our conclusion in section 1. Also, plugging the values of the example above, we have $\hat w=(0.64, 1, 0.32)^T$, thus $X\hat w=(1,1,0)^T$, which is consistent with our (imaginary) observation of the projection.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The term $(X^TX)^{-1}X^T$ is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse&quot;&gt;Moore–Penrose inverse&lt;/a&gt; of $X$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;3-least-squares-method---a-probabilistic-perspective&quot;&gt;3. Least Squares Method - A Probabilistic Perspective&lt;/h1&gt;

&lt;p&gt;As we mentioned before, error is inevitable in practice, otherwise there is no need to do such approximation.&lt;/p&gt;

&lt;h2 id=&quot;31-maximum-likelihood-estimation&quot;&gt;3.1 Maximum likelihood estimation&lt;/h2&gt;

&lt;p&gt;We now use Gaussian distribution to reflect such noise as $\varepsilon\sim\mathcal{N}(0,\sigma^2)$. With the definition in section 1, we need to find the $w$ subject to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y=w^Tx+\varepsilon.&lt;/script&gt;

&lt;p&gt;Obviously, we have $y|x;w\sim\mathcal{N}(w^Tx,\sigma^2)$. For $N$ samples, the log-likelihood follows that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\mathcal{L}(w)&amp;=\log P(Y|X;w)\\&amp;=\sum_{i=1}^N\log P(y_i|x_i;w)\\&amp;=\sum_{i=1}^N\left(\log \frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{2\sigma^2}(y_i-w^Tx_i)^2\right)\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;p&gt;Therefore, by &lt;em&gt;maximum likelihood estimation&lt;/em&gt; (MLE) method, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\hat w&amp;=\arg\max_w\mathcal{L}(w)\\&amp;=\arg\max_w -\sum_{i=1}^N\frac{1}{2\sigma^2}(y_i-w^Tx_i)^2\\&amp;=\arg\min_w\sum_{i=1}^N(y_i-w^Tx_i)^2\end{aligned}, %]]&gt;&lt;/script&gt;

&lt;p&gt;which is consistent with our analysis in section 1. In fact, least squares method has an assumption that the noise is subject to Gaussian distribution.&lt;/p&gt;

&lt;h2 id=&quot;32-maximum-a-posteriori&quot;&gt;3.2 Maximum a posteriori&lt;/h2&gt;

&lt;p&gt;As we mentioned in the &lt;a href=&quot;https://19w6.github.io/2020/09/28/intro-ml01/&quot;&gt;previous notes&lt;/a&gt;, in the view of Bayesians, $w$ can also be a random variable. Suppose $w\sim\mathcal{N}(0,\sigma_0^2)$. Still, we have $y\vert x;w\sim\mathcal{N}(w^Tx,\sigma^2)$ (there is a little abuse of notation: $w$ after ‘$|$’ is a sample rather than a random variable). By &lt;em&gt;maximum a posteriori&lt;/em&gt; (MAP) method, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\hat w&amp;=\arg\max_w P(w|Y)\\&amp;=\arg\max_w\log\left(\frac{\prod_{i=1}^NP(y_i|w)\cdot P(w)}{\prod_{i=1}^NP(y_i)}\right)\\&amp;=\arg\max_w\log\left(\prod_{i=1}^NP(y_i|w)\right)+\log P(w)\\&amp;=\arg\max_w\sum_{i=1}^N\log\left(\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}}\right)+\log\left(\frac{1}{\sqrt{2\pi}\sigma_0}e^{-\frac{||w||^2}{2\sigma_0^2}}\right)\\&amp;=\arg\min_w\sum_{i=1}^N\frac{(y_i-w^Tx_i)^2}{2\sigma^2}+\frac{||w||^2}{2\sigma_0^2}\\&amp;=\arg\min_w\underbrace{\sum_{i=1}^N(y_i-w^Tx_i)^2}_\text{square error}+\underbrace{\frac{\sigma^2}{\sigma_0^2}||w||^2}_\text{regularizer}\end{aligned}, %]]&gt;&lt;/script&gt;

&lt;p&gt;which is slightly different from the result of MLE method. However, we will show in next section this is actually equivalent to the &lt;em&gt;regularized least squares method&lt;/em&gt;.&lt;/p&gt;

&lt;h1 id=&quot;4-regularization&quot;&gt;4. Regularization&lt;/h1&gt;

&lt;p&gt;In practice, a common issue is $N\ll d$, which may cause $X^TX$ not invertible and further lead to &lt;em&gt;overfitting&lt;/em&gt;. There are three techniques to avoid overfitting: collecting more data, &lt;em&gt;feature engineering/extracting&lt;/em&gt;, and &lt;em&gt;regularization&lt;/em&gt;. The regularization method is adding a penalty term, and we have the new loss function $\mathcal{L}_r(w)=\mathcal{L}(w)+\lambda P(w)$ where $\lambda$ is a tunable parameter. Depending on what $P(w)$ is, we have different regression.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lasso regression&lt;/strong&gt;: use $L1$ norm as the penalty, which means $P(w)=||w||$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Ridge regression&lt;/strong&gt;: use $L2$ norm as the penalty, which means $P(w)=||w||_2^2=w^Tw.$&lt;/p&gt;

&lt;p&gt;Such regularization is often called &lt;em&gt;weight decay&lt;/em&gt;. We now focus on ridge regression. According the above definition, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\mathcal{L}_r(w)&amp;=\mathcal{L}(w)+\lambda P(w)\\&amp;=\sum_{i=1}^N||w^Tx_i-y_i||^2+\lambda w^Tw\\&amp;=w^TX^TXw-2w^TX^TY+Y^TY+\lambda w^Tw\\&amp;=w^T(X^TX+\lambda\mathbf{I})w-2w^TX^TY+Y^TY\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;p&gt;Therefore, we have the optimal weight where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{alignat*}{3}\hat w&amp;=\arg\min_w \mathcal{L}_r(w)\Rightarrow\frac{\partial \mathcal{L}_r(w)}{\partial w}&amp;=0\end{alignat*}. %]]&gt;&lt;/script&gt;

&lt;p&gt;Solving the equation, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{w}=(X^TX+\lambda\mathbf{I})^{-1}X^TY.&lt;/script&gt;

&lt;p&gt;By introducing the positive definite matrix $\lambda\mathbf{I}$, the problem of the invertible matrix $X^TX$ is avoided.&lt;/p&gt;

&lt;h1 id=&quot;5-conclusion&quot;&gt;5. Conclusion&lt;/h1&gt;

&lt;p&gt;Though linear regression is a naive model of machine learning, the thought of it is inspiring. In this post, we show that least squares is equivalent to MLE method with Gaussian noise in data, while the least squares with $L2$ regularizer is equivalent to MAP method with Gaussian noise in both weight and data.&lt;/p&gt;

&lt;p&gt;Based on the attributes of linear regression, thoughts of many machine learning models can be derived:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Linearity: as its name suggests, the linear regression method exploits linear functions to fit the data. Unsurprisingly, such linearity has a limited performance in general. Hence, there are many models developed from breaking the linearity. Examples:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Polynomial Regression&lt;/strong&gt; is a form of linear regression in which we convert the original features into their higher order terms. For example, transforms $x=(x_1,x_2)$ into $\tilde x=(x_1,x_2,x_1x_2,x_2^3)$.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Logistic Regression&lt;/strong&gt;: introduces a sigmoid function to the linear function, &lt;em&gt;e.g.&lt;/em&gt; $f(x)=\text{sigmoid}(w^Tx)$.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Neural Network&lt;/strong&gt; introduces multiple nonlinear functions and brings the multi-layer structure.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Global Space: the approximation we found by linear regression is applied to the whole space. However, in practice, the data may not be continuous, and we need different approximations for different space.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Decision Tree&lt;/strong&gt; divides the space into smaller sub-spaces depending on the question.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Raw Data: the basic linear regression utilizes all the given data, which incurs the &lt;em&gt;curse of dimensionality&lt;/em&gt;. In this case, dimensionality reduction methods are necessary.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;PCA&lt;/strong&gt; transforms the variables of the higher dimension into a smaller ones that still contain most of the information in the original data set.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Machine Learning - 01 Introduction</title>
   <link href="http://localhost:4000/2020/09/28/intro-ml01/"/>
   <updated>2020-09-28T00:00:00+08:00</updated>
   <id>http://localhost:4000/2020/09/28/intro-ml01</id>
   <content type="html">&lt;p&gt;&lt;em&gt;The notes are based on the &lt;a href=&quot;https://github.com/shuhuai007/Machine-Learning-Session&quot;&gt;session&lt;/a&gt;. Many thanks to the great work.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;0-motivation&quot;&gt;0. Motivation&lt;/h1&gt;

&lt;p&gt;After setting up this blog, I actually had no idea about what should I post. I am definitely not good at writing, but still I enjoy it a lot. As I am weak in machine learning and mathematics, for writing and learning, a ‘&lt;em&gt;translation&lt;/em&gt;’ work of the &lt;a href=&quot;https://github.com/shuhuai007/Machine-Learning-Session&quot;&gt;session&lt;/a&gt; may be a good start. So here we are!&lt;/p&gt;

&lt;h1 id=&quot;1-frequentist-vs-bayesian&quot;&gt;1. Frequentist vs Bayesian&lt;/h1&gt;

&lt;p&gt;Many machine learning methods can be illustrated in a &lt;em&gt;probabilistic&lt;/em&gt; way. We now introduce two mainstream views of interpreting probability: &lt;strong&gt;frequentist&lt;/strong&gt; and &lt;strong&gt;bayesian&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Firstly let us consider a simple example. Suppose we have a data $X=(x_1,x_2,\ldots,x_N)^T$ where $x_i$ are i.i.d. samples of the random variable $x$ and is of $d$ dimensions. Also, we define $\theta$ be the parameter so that $x\sim P(x|\theta)$, and we call $P(X|\theta)$ the &lt;em&gt;likelihood&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Frequentists&lt;/strong&gt;: $\theta$ is an unknown &lt;em&gt;constant&lt;/em&gt;. What they care is how to use the data to infer the value of $\theta$. The best known approach for that is &lt;strong&gt;maximum likelihood estimation&lt;/strong&gt; (MLE):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta}_\text{MLE}=\arg\max_{\theta}P(X|\theta).&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;Many traditional machine learning methods are based on this view. They construct a probabilistic model, then determine a loss function and minimize it by methods like gradient descent.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Bayesians&lt;/strong&gt;: $\theta$ is random variable, following distribution $P(\theta)$, which is called &lt;em&gt;prior distribution&lt;/em&gt;, and $P(\theta|X)$ is called &lt;em&gt;posterior distribution&lt;/em&gt;. Then by &lt;em&gt;Bayes’s theorem&lt;/em&gt;, we have &lt;strong&gt;maximum a posteriori&lt;/strong&gt; (MAP):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta}_\text{MAP}=\arg\max_{\theta}P(\theta|X).&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;In Bayesian inference, the posterior distribution, rather than the maximum, is the key. However, to determine the distribution sometimes is really hard as it may incur complex calculus:&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\theta|X)=\frac{P(X|\theta)\cdot P(\theta)}{\int_{\theta}P(X|\theta)\cdot P(\theta)\text{d}\theta}&lt;/script&gt;

  &lt;p&gt;Many powerful and elegant approaches were proposed for this. For examples, MCMC, probabilistic graphical model, &lt;em&gt;etc&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I’d like to cite &lt;a href=&quot;https://www.probabilisticworld.com/frequentist-bayesian-approaches-inferential-statistics/&quot;&gt;this&lt;/a&gt; to summary the differences.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“In short, according to the frequentist definition of probability, only repeatable random events (like the result of flipping a coin) have probabilities. These probabilities are equal to the long-term frequency of occurrence of the events in question. Frequentists don’t attach probabilities to hypotheses or to any fixed but unknown values in general.”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“In contrast, Bayesians view probabilities as a more general concept. As a Bayesian, you can use probabilities to represent the uncertainty in any event or hypothesis. Here, it’s perfectly acceptable to assign probabilities to non-repeatable events.”&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;2maximum-likelihood-estimation-on-gaussian-distribution&quot;&gt;2.Maximum Likelihood Estimation on Gaussian Distribution&lt;/h1&gt;

&lt;p&gt;In this section, we will give an example of MLE on Gaussian distribution. Just like what we defined in section 1, there is a data $X=(x_1,x_2,\ldots,x_N)^T$ where $x_i$ are i.i.d. samples of the random variable $x$ and is of $d=1$  dimension. Thus we have $X\in\mathbb{R}^{N\times 1}$. We further attach a specific distribution to $x$ as $x\sim\mathcal{N}(\mu,\sigma^2)$. The unknown parameter then becomes $\theta=(\mu,\sigma)$. Now we use MLE method to estimate the parameter:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta}_\text{MLE}=\arg\max_\theta P(X|\theta)=\arg\max_\theta \log P(X|\theta).&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;Given $x\sim\mathcal{N}(\mu,\sigma^2)$, the PDF of $x$ is&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x|\mu,\sigma)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}.&lt;/script&gt;
&lt;/blockquote&gt;

&lt;p&gt;As $x_i,i=1,\ldots,N$ are i.i.d. variables, it follows that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X|\theta)=\prod_{i=1}^NP(x_i|\theta).&lt;/script&gt;

&lt;p&gt;Hence,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\arg\max_\theta\log P(X|\theta)&amp;=\arg\max_\theta\log \prod_{i=1}^N P(x_i|\theta)\\&amp;=\arg\max_\theta\sum_{i=1}^N\log P(x_i|\theta)\\&amp;=\arg\max_\theta\sum_{i=1}^N\log\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\\&amp;=\arg\max_\theta\sum_{i=1}^N\left[\log \frac{1}{\sqrt{2\pi}}+\log\frac{1}{\sigma}-\frac{(x_i-\mu)^2}{2\sigma^2}\right]\\&amp;=\arg\max_\theta\sum_{i=1}^N\left[\log\frac{1}{\sigma}-\frac{(x_i-\mu)^2}{2\sigma^2}\right]\end{aligned}, %]]&gt;&lt;/script&gt;

&lt;p&gt;which is equivalent to a numerical optimization problem. Denote the term $\sum_{i=1}^N\left[\log\frac{1}{\sigma}-\frac{(x_i-\mu)^2}{2\sigma^2}\right]$ as $\mathcal{L}$. The maximum occurs when&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial\mathcal{L}}{\partial\mu}=0,\quad \frac{\partial\mathcal{L}}{\partial\sigma}=0.&lt;/script&gt;

&lt;p&gt;Solving the equations we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_\text{MLE}=\frac{1}{N}\sum_{i=1}^N x_i,\quad \sigma^2_\text{MLE}=\frac{1}{N}\sum_{i=1}^N(x_i-\mu_\text{MLE})^2.&lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;$\mu_\text{MLE}$ is an &lt;em&gt;unbiased estimation&lt;/em&gt; as $\mathbb{E}[\mu_\text{MLE}]=\mu$, while $\sigma_\text{MLE}^2$ is a &lt;em&gt;biased estimation&lt;/em&gt; since $\mathbb{E}[\sigma_\text{MLE}^2]=\frac{N-1}{N}\sigma^2$. The bias is incurred by the exploitation on samples rather than the true distribution. Intuitively, the variance of the samples will never be larger than the true distribution since they are totally generated from that!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;3-gaussian-distribution&quot;&gt;3. Gaussian Distribution&lt;/h1&gt;

&lt;p&gt;Many advanced machine learning techniques, like &lt;em&gt;linear Gaussian model&lt;/em&gt;, &lt;em&gt;Kalman filter&lt;/em&gt;, &lt;em&gt;P-PCA&lt;/em&gt;, &lt;em&gt;etc&lt;/em&gt;, involve Gaussian distribution. Therefore it is quite practical to get familiar with Gaussian distribution. In this section, we are trying to explain why the PDF of bivariate Gaussian distribution is shaped by numerous ellipses.&lt;/p&gt;

&lt;p&gt;Suppose we have a random variable $x\sim \mathcal{N}(\mu,\Sigma)$ of $d$ dimensions. Specifically, the definition follows that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x|\mu,\Sigma)=\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)},&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
x=\begin{pmatrix}x_{1}\\x_{2}\\\vdots\\x_{d}\end{pmatrix}_{d\times1},\quad \mu=\begin{pmatrix}\mu_1\\\mu_2\\\vdots\\\mu_{d}\end{pmatrix}_{d\times1},\quad\Sigma=\begin{pmatrix}\sigma_{11}&amp;\sigma_{12}&amp;\cdots&amp;\sigma_{1d}\\\sigma_{21}&amp;\sigma_{22}&amp;\cdots&amp;\sigma_{2d}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\\sigma_{d1}&amp;\sigma_{d2}&amp;\cdots&amp;\sigma_{dd}\end{pmatrix}_{d\times d}. %]]&gt;&lt;/script&gt;

&lt;p&gt;As it involves matrix, a simplification will be a wise choice. For simplicity,  we use $\Delta(x)$ to represent $(x-\mu)^T\Sigma^{-1}(x-\mu)$ and suppose the covariance matrix $\Sigma$ to be positive definite, which is reasonable. By the eigenvalue decomposition and the property of positive definite matrix, $\Sigma$ can be decomposed as $\Sigma=U\Lambda U^T$, where $U=(u_1,u_2,\ldots,u_d)$ is an orthogonal matrix whose columns are the eigenvectors of $\Sigma$, and $\Lambda=\text{diag}(\lambda_1,\lambda_2,\ldots,\lambda_d)$ is a diagonal matrix whose entries are the eigenvalues of $\Sigma$. Then we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\Sigma^{-1}&amp;=(U\Lambda U^T)^{-1}\\&amp;=(U^T)^{-1}\Lambda^{-1}U^{-1}\\&amp;=U\Lambda^{-1}U^T\\&amp;=(u_1,u_2,\ldots,u_d)\begin{pmatrix}\frac{1}{\lambda_1}&amp;0&amp;\cdots&amp;0\\0&amp;\frac{1}{\lambda_2}&amp;\cdots&amp;0\\\vdots&amp;\vdots&amp;\ddots&amp;0\\0&amp;0&amp;\cdots&amp;\frac{1}{\lambda_d}\end{pmatrix}\begin{pmatrix}u_1\\u_2\\\vdots\\u_d\end{pmatrix}\\&amp;=\begin{pmatrix}\frac{u_1}{\lambda_1},\frac{u_2}{\lambda_2},\ldots,\frac{u_d}{\lambda_d}\end{pmatrix}\begin{pmatrix}u_1\\u_2\\\vdots\\u_d\end{pmatrix}\\&amp;=\sum_{i=1}^d\frac{u_iu_i^T}{\lambda_i}\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;p&gt;Plugging it in $\Delta(x)$, we arrive at&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\Delta(x)&amp;=(x-\mu)^T\left(\sum_{i=1}^d\frac{u_iu_i^T}{\lambda_i}\right)(x-\mu)\\&amp;=\sum_{i=1}^d(x-\mu)^Tu_i\frac{1}{\lambda_i}u_i^T(x-\mu)\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;p&gt;Before moving on, we take a pause to be acquainted with those notations. Specifically, $(x-\mu)\in\mathbb{R}^{d\times 1}$ as $x, \mu\in\mathbb{R}^{d\times 1}$. $u_i$ is the eigenvector of $\Sigma$ so $u_i\in\mathbb{R}^{d\times 1}$, while $\lambda_i$ is the eigenvalue of $\Sigma$ so $\lambda_i\in\mathbb{R}$. Therefore $(x-\mu)^Tu_i\in\mathbb{R}$, $u_i^T(x-\mu)\in\mathbb{R}$ and $\Delta(x)\in\mathbb{R}$. It should not be surprising. Recall that the PDF&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x|\mu,\Sigma)=\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}\Delta(x)}&lt;/script&gt;

&lt;p&gt;represents a probability, which is definitely a real number. Now we introduce $Y=(y_1,y_2,\ldots,y_d)^T$ where $y_i=(x-\mu)^Tu_i$. Hence&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta(x)=\sum_{i=1}^dy_i\frac{1}{\lambda_i}y_i^T=\sum_{i=1}^d\frac{y_i^2}{\lambda_i}.&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;The PDF of $x$ is rewritten as&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x|\mu,\Sigma)=\alpha e^{-\frac{1}{2}\sum_{i=1}^d y_i^2/\lambda_i},&lt;/script&gt;

&lt;p&gt;where $\alpha=\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}$ is a constant. Now suppose we are trying to determine where $P(x|\mu,\Sigma)=\beta$. To make it intuitive, we consider the case $d=2$, then it follows that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{alignat*}{3}&amp;&amp; \alpha e^{-\frac{1}{2}\left(\frac{y_1^2}{\lambda_1}+\frac{y_2^2}{\lambda_2}\right)}&amp;=\beta\\\Rightarrow&amp;&amp;\frac{y_1^2}{\lambda_1}+\frac{y_2^2}{\lambda_2}&amp;=2(\ln\alpha-\ln\beta)\\\Rightarrow&amp;&amp;\frac{y_1^2}{a^2}+\frac{y_2^2}{b^2}&amp;=1\end{alignat*}, %]]&gt;&lt;/script&gt;

&lt;p&gt;where $a^2=2\lambda_1(\ln\alpha-\ln\beta)$ and $b^2=2\lambda_2(\ln\alpha-\ln\beta)$, and we finally get &lt;strong&gt;a standard equation of ellipse&lt;/strong&gt; with respect to $y_1$ and $y_2$! Actually, the transformation $y_i=(x-\mu)^Tu_i$ is the projection of $x$ on $u_i$. Thus in the coordinate system represented by $x_1$ and $x_2$, the ellipse will be changed linearly hence the ellipse will not be so standard. For different $\beta$, we have ellipses with different major and minor axes, and that is why the graph of bivariate Gaussian distribution looks like it is composed of numerous concentration ellipses.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;PS.&lt;/em&gt; The above discussion leaves much correctness proof to be done.&lt;/p&gt;

&lt;h1 id=&quot;4-calculations-on-gaussian&quot;&gt;4. Calculations on Gaussian&lt;/h1&gt;

&lt;p&gt;In this section, all the random variables are Gaussian and we will not touch the complex PDF. Rather, we mainly focus on the statistics, the expectation and the variance, of Gaussian distribution.&lt;/p&gt;

&lt;h2 id=&quot;41-calculations-on-the-union-distribution&quot;&gt;4.1. Calculations on the union distribution&lt;/h2&gt;

&lt;p&gt;Suppose $x\sim\mathcal{N}(\mu, \Sigma)$ is the union distribution of $x_a$ and $x_b$, &lt;em&gt;i.e.&lt;/em&gt; $x=(x_a,x_b)^T$. Then the definition follows that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
x=\begin{pmatrix}x_a\\x_b\end{pmatrix}_{d\times1},\quad \mu=\begin{pmatrix}\mu_a\\\mu_b\end{pmatrix}_{d\times1},\quad\Sigma=\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}_{d\times d}, %]]&gt;&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_a,\mu_a\in\mathbb{R}^{m\times 1},x_b,\mu_b\in\mathbb{R}^{n\times 1}, m+n=d,&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Sigma_{aa}\in\mathbb{R}^{m\times m}, \Sigma_{ab},\Sigma_{ba}^T\in\mathbb{R}^{m\times n}, \Sigma_{bb}\in\mathbb{R}^{n\times n}.&lt;/script&gt;

&lt;p&gt;The problem in this section is how to derive $P(x_a)$ and $P(x_b|x_a)$, or symmetrically, $P(x_b)$ and $P(x_a|x_b)$ given the union distribution.  The derivation of $P(x_a)$ is quite straightforward:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\mathbb{E}[x_a]&amp;=\mathbb{E}\left[\begin{pmatrix}\mathbf{I}_{m\times m}&amp;\mathbf{0}_{m\times n}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}\right]\\&amp;=\begin{pmatrix}\mathbf{I}_{m\times m}&amp;\mathbf{0}_{m\times n}\end{pmatrix}\mathbb{E}[x]\\&amp;=\begin{pmatrix}\mathbf{I}_{m\times m}&amp;\mathbf{0}_{m\times n}\end{pmatrix}\begin{pmatrix}\mu_a\\\mu_b\end{pmatrix}\\&amp;=\mu_a\end{aligned}, %]]&gt;&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}var[x_a]&amp;=var\left[\begin{pmatrix}\mathbf{I}_{m\times m}&amp;\mathbf{0}_{m\times n}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}\right]\\&amp;=\begin{pmatrix}\mathbf{I}_{m\times m}&amp;\mathbf{0}_{m\times n}\end{pmatrix}var[x]\begin{pmatrix}\mathbf{I}_{m\times m}\\\mathbf{0}_{n\times m}\end{pmatrix}\\&amp;=\begin{pmatrix}\mathbf{I}_{m\times m}&amp;\mathbf{0}_{m\times n}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\begin{pmatrix}\mathbf{I}_{m\times m}\\\mathbf{0}_{n\times m}\end{pmatrix}\\&amp;=\Sigma_{aa}\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;p&gt;Thus, we have $x_a\sim\mathcal{N}(\mu_a,\Sigma_{aa})$, and similarly, $x_b\sim\mathcal{N}(\mu_b,\Sigma_{bb})$. For the conditional probabilities $P(x_b|x_a)$, we first define variables&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{b\cdot a}=x_b-\Sigma_{ba}\Sigma^{-1}_{aa}x_a,&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_{b\cdot a}=\mu_b-\Sigma_{ba}\Sigma^{-1}_{aa}\mu_a,&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Sigma_{bb\cdot a}=\Sigma_{bb}-\Sigma_{ba}\Sigma^{-1}_{aa}\Sigma_{ab}.&lt;/script&gt;

&lt;p&gt;The term $\Sigma_{bb\cdot a}$ is also the &lt;em&gt;Schur complement&lt;/em&gt; of $\Sigma_{aa}$ of the matrix $\Sigma$. Then it follows that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\mathbb{E}[x_{b\cdot a}]&amp;=\mathbb{E}\left[\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbf{I}_{n\times n}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}\right]\\&amp;=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbf{I}_{n\times n}\end{pmatrix}\mathbb{E}[x]\\&amp;=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbf{I}_{n\times n}\end{pmatrix}\begin{pmatrix}\mu_a\\\mu_b\end{pmatrix}\\&amp;=\mu_{b\cdot a}\end{aligned}, %]]&gt;&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}var[x_{b\cdot a}]&amp;=var\left[\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbf{I}_{n\times n}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}\right]\\&amp;=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbf{I}_{n\times n}\end{pmatrix}var[x]\begin{pmatrix}-\Sigma_{aa}^{-1}\Sigma_{ba}^T\\\mathbf{I}_{n\times n}\end{pmatrix}\\&amp;=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbf{I}_{n\times n}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\begin{pmatrix}-\Sigma_{aa}^{-1}\Sigma_{ba}^T\\\mathbf{I}_{n\times n}\end{pmatrix}\\&amp;=\Sigma_{bb\cdot a}\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;p&gt;Hence we have $x_{b\cdot a}\sim\mathcal{N}(\mu_{b\cdot a},\Sigma_{bb\cdot a})$. Before showing the relation between $x_{b\cdot a}$ and $x_b|x_a$, we introduce a helpful theorem.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;:&lt;em&gt;If&lt;/em&gt; $x\sim\mathcal{N}(\mu,\Sigma)$, &lt;em&gt;then&lt;/em&gt; $Mx\perp Nx\iff M\Sigma N=\mathbf{0}.$&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Proof:&lt;/em&gt; According to the property of Gaussian, $Mx\sim\mathcal{N}(M\mu,M\Sigma M^T)$ and $Nx\sim\mathcal{N}(N\mu,N\Sigma N^T)$. Further,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}Cov(Mx,Nx)&amp;=\mathbb{E}[(Mx-M\mu)(Nx-N\mu)^T]\\&amp;=M\mathbb{E}[(x-\mu)(x-\mu)^T]N\\&amp;=M\Sigma N^T.\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;For Gaussian distribution, uncorrelation implies independence. Therefore $M\Sigma N^T=\mathbf{0}\iff Mx\perp Nx.\tag*{$\blacksquare$}$&lt;/p&gt;

&lt;p&gt;Back to our case, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
x_{b\cdot a}=\underbrace{\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbf{I}\end{pmatrix}}_M\underbrace{\begin{pmatrix}x_a\\x_b\end{pmatrix}}_x,\qquad x_a=\underbrace{\begin{pmatrix}\mathbf{I}&amp;\mathbf{0}\end{pmatrix}}_{N}\underbrace{\begin{pmatrix}x_a\\x_b\end{pmatrix}}_x. %]]&gt;&lt;/script&gt;

&lt;p&gt;Check $M\Sigma N$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}M\Sigma N&amp;=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbf{I}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\begin{pmatrix}\mathbf{I}\\\mathbf{0}\end{pmatrix}\\&amp;=\begin{pmatrix}\mathbf{0}&amp; \Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}\end{pmatrix}\begin{pmatrix}\mathbf{I}\\\mathbf{0}\end{pmatrix}\\&amp;=\mathbf{0}\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;p&gt;Thus $x_{b\cdot a}\perp x_a$, which means $x_{b\cdot a}|x_a=x_{b\cdot a}$.  According to our definition, we finally arrive at&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}x_b|x_a&amp;=x_{b\cdot a}|x_a+\Sigma_{ba}\Sigma_{aa}^{-1}x_a|x_a\\&amp;=x_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;p&gt;There is a little abuse of notation: the term $x_a$ after $|$ is a sample of $x_a$ rather than a random variable. One should notice that $x_a$ below is constant. Therefore,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}[x_b|x_a]=\mathbb{E}[x_{b\cdot a}]+\Sigma_{ba}\Sigma_{aa}^{-1}x_a=\mu_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a,&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;var[x_b|x_a]=var[x_{b\cdot a}]+0=\Sigma_{bb\cdot a}.&lt;/script&gt;

&lt;p&gt;Removing all the extra variables we defined, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_b|x_a\sim\mathcal{N}(\mu_b+\Sigma_{ba}\Sigma_{aa}^{-1}(x_a-\mu_a), \Sigma_{bb}-\Sigma_{ba}\Sigma^{-1}_{aa}\Sigma_{ab}),&lt;/script&gt;

&lt;p&gt;and the case for $x_a|x_b$ is similar.&lt;/p&gt;

&lt;h2 id=&quot;42-calculations-on-the-marginal-and-conditional-distribution&quot;&gt;4.2 Calculations on the marginal and conditional distribution&lt;/h2&gt;

&lt;p&gt;In this section, the problem is deriving $P(y)$ and $P(x|y)$ given $P(x)$ and $P(y|x)$. For example, let’s say, $x\sim\mathcal{N}(\mu, \Sigma)$ and $y|x\sim\mathcal{N}(Ax+b,L^{-1})$, which is common in &lt;em&gt;linear Gaussian model&lt;/em&gt;. The abuse of notation here is the same as we mentioned above: the $x\ (y)$  in $y|x\ (x|y)$ represents a sample rather than a random variable. Now we define $y=Ax+b+\varepsilon$, where $\varepsilon\sim\mathcal{N}(0,L^{-1})$ is the noise in practice. Then we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}[y]=\mathbb{E}[Ax+b+\varepsilon]=A\mu+b,&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;var[y]=var[Ax+b]+var[\varepsilon]=A\Sigma A^T+L^{-1}.&lt;/script&gt;

&lt;p&gt;Therefore $y\sim\mathcal{N}(A\mu+b, A\Sigma A^T+L^{-1})$. For the distribution of $x|y$, we can refer to the conclusion of section 4.1. Introducing a union variable $z=(x,y)^T$, we obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
z\sim\mathcal{N}\left(\begin{pmatrix}\mu\\A\mu+b\end{pmatrix},\begin{pmatrix}\Sigma&amp;Cov(x,y)\\Cov(y,x)&amp;A\Sigma A^T+L^{-1}\end{pmatrix}\right). %]]&gt;&lt;/script&gt;

&lt;p&gt;By the property of covariance, we have $Cov(y,x)=Cov(x,y)^T$. Further,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}Cov(x,y)&amp;=\mathbb{E}[(x-\mu)(y-A\mu-b)^T]\\&amp;=\mathbb{E}[(x-\mu)(Ax+b+\varepsilon-A\mu-b)^T]\\&amp;=\mathbb{E}[(x-\mu)(Ax-A\mu)^T]+\mathbb{E}[(x-\mu)\varepsilon^T]\\&amp;=\mathbb{E}[(x-\mu)(x-\mu)^T]A^T+0\\&amp;=var(x)A^T\\&amp;=\Sigma A^T\end{aligned}. %]]&gt;&lt;/script&gt;

&lt;p&gt;With the known union distribution $z=(x,y)^T$ and $P(y)$, we can derive $P(x|y)$ by the method mentioned in section 4.1.&lt;/p&gt;

&lt;h1 id=&quot;5-conclusion&quot;&gt;5. Conclusion&lt;/h1&gt;

&lt;p&gt;In this post, we talk a lot about Gaussian distribution as it plays an important role in machine learning. Also, we are trying to get familar with those mathematical things.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Introduction to Reinforcement Learning</title>
   <link href="http://localhost:4000/2020/09/21/intro-rl/"/>
   <updated>2020-09-21T00:00:00+08:00</updated>
   <id>http://localhost:4000/2020/09/21/intro-rl</id>
   <content type="html">&lt;p&gt;The introductory &lt;a href=&quot;https://github.com/19w6/Reinforcement_Learning_Notes&quot;&gt;notes&lt;/a&gt; included &lt;strong&gt;Bandit Algorithms&lt;/strong&gt;, &lt;strong&gt;MDP&lt;/strong&gt;, &lt;strong&gt;Model-free Methods&lt;/strong&gt;, &lt;strong&gt;Value Function Approximation&lt;/strong&gt;, &lt;strong&gt;Policy Optimization&lt;/strong&gt;. For the state-of-the-art advances, one can refer to paper directly and some excellent blogs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://19w6.github.io/assets/rl/Reinforcement Learning Notes.pdf&quot;&gt;Reinforcement Learning Notes (an integration of the following sections)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://19w6.github.io/assets/rl/Section 1 Introduction.pdf&quot;&gt;Section 1 Introduction&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://19w6.github.io/assets/rl/Section 2 Probability.pdf&quot;&gt;Section 2 Probability&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://19w6.github.io/assets/rl/Section 3 Bandit Algorithms.pdf&quot;&gt;Section 3 Bandit Algorithms&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://19w6.github.io/assets/rl/Section 4 Markov Chains.pdf&quot;&gt;Section 4 Markov Chains&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://19w6.github.io/assets/rl/Section 5 Markov Decision Process.pdf&quot;&gt;Section 5 Markov Decision Process&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://19w6.github.io/assets/rl/Section 6 Model-Free Prediction.pdf&quot;&gt;Section 6 Model-Free Prediction&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://19w6.github.io/assets/rl/Section 7 Model-Free Control.pdf&quot;&gt;Section 7 Model-Free Control&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://19w6.github.io/assets/rl/Section 8 Value Function Approximation.pdf&quot;&gt;Section 8 Value Function Approximation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://19w6.github.io/assets/rl/Section 9 Policy Gradient.pdf&quot;&gt;Section 9 Policy Gradient&lt;/a&gt;&lt;/p&gt;

</content>
 </entry>
 

</feed>
