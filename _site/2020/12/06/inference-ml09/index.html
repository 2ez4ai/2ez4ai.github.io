<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-51FFV4BDWW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-51FFV4BDWW');
</script>
  
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    Machine Learning - 09 Exact Inference of Graphical Models &middot; Jingye Wang
  </title>

  
  <link rel="canonical" href="http://localhost:4000/2020/12/06/inference-ml09/">
  

  <link rel="stylesheet" href="http://localhost:4000/public/css/poole.css">
  <link rel="stylesheet" href="http://localhost:4000/public/css/syntax.css">
  <link rel="stylesheet" href="http://localhost:4000/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://localhost:4000/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="http://localhost:4000/public/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="http://localhost:4000/atom.xml">

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>"You can’t connect the dots looking forward; you can only connect them looking backward."</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="http://localhost:4000/">Home</a>

    

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="http://localhost:4000/about/">About</a>
        
      
    
      
    
      
        
      
    
    <span class="sidebar-nav-item">Currently v0.0.1</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2020. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">Jingye Wang</a>
            <small>A personal blog.</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">Machine Learning - 09 Exact Inference of Graphical Models </h1>
  <span class="post-date">06 Dec 2020</span>
  <p><em>The notes are based on the <a href="https://github.com/shuhuai007/Machine-Learning-Session">session</a>, <a href="https://ermongroup.github.io/cs228-notes/">CS228-notes</a> and <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">PRML</a>. For the fundamental of probability, one can refer to <a href="https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view">Introduction to Probability</a>. Many thanks to these great works.</em></p>

<ul id="markdown-toc">
  <li><a href="#0-introduction" id="markdown-toc-0-introduction">0. Introduction</a></li>
  <li><a href="#1-variable-elimination" id="markdown-toc-1-variable-elimination">1. Variable Elimination</a></li>
  <li><a href="#2-belief-propagation" id="markdown-toc-2-belief-propagation">2. Belief Propagation</a>    <ul>
      <li><a href="#21-message" id="markdown-toc-21-message">2.1. Message</a></li>
      <li><a href="#22-sum-product" id="markdown-toc-22-sum-product">2.2. Sum-Product</a></li>
      <li><a href="#23-max-product" id="markdown-toc-23-max-product">2.3 Max-Product</a></li>
    </ul>
  </li>
  <li><a href="#3-conclusion" id="markdown-toc-3-conclusion">3. Conclusion</a></li>
</ul>
<h1 id="0-introduction">0. Introduction</h1>

<p>In the last <a href="https://19w6.github.io/2020/11/29/probabilistic_graphical_models-ml08/">post</a>, we introduce graphical models which is capable of representing random variables and the conditional independences among them. We now consider the problem of inference in graphical models. Particularly, we wish to compute posterior distributions of one or more nodes contioned on some other known (observed) nodes, and the techniques we shall talk in this post are for <em>exact inference</em>.</p>

<h1 id="1-variable-elimination">1. Variable Elimination</h1>

<p>We first consider the marginal inference in a <em>chain</em> Bayesian network which has the following joint distribution,</p>

<script type="math/tex; mode=display">P(x_1,x_2,\dots,x_n)=P(x_1)\prod_{i=2}^nP(x_i\vert x_{i-1}).</script>

<p>Suppose we want to infer the marginal distribution $P(x_n)$. A naive way to do that is marginalizing $x_1,x_2,\dots,x_{n-1}$:</p>

<script type="math/tex; mode=display">P(x_n)=\sum_{x_1}\sum_{x_2}\dots\sum_{x_{n-1}}P(x_1,x_2,\dots,x_{n-1}).</script>

<p>However, as there are $n-1$ variables each with $k$ states, the computation needs to sum the probability over $k^{n-1}$ values and would scale exponentially with the length of the chain. To simplify the computation, we can leverage <em>variable elimination</em>.</p>

<p>The key of <em>variable elimination</em> is in <em>reagrranging the order</em> of the summations and the multiplications. Specifically, the marginal distribution follows that</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}P(x_n)&=\sum_{x_1}\sum_{x_2}\dots\sum_{x_{n-1}}P(x_1)\prod_{i=2}^nP(x_i\vert x_{i-1})\\&=\sum_{x_{n-1}}P(x_n\vert x_{n-1})\cdot \sum_{x_{n-2}}P(x_{n-1}\vert x_{n-2})\cdot\dots\cdot \sum_{x_{1}}P(x_{2}\vert x_{1})P(x_1).\end{aligned} %]]></script>

<p>Such a rearrangement works because multiplication is distributive over addition,</p>

<script type="math/tex; mode=display">ab+ac=a(b+c),</script>

<p>where the number of arithmetic operations are reduced from three (the left-hand side) to two (the right-hand side). Before we move on, we generalize the new expression to a Markov network since every Bayesian network can be tranformed into a Markov network. For the chain Bayesian network we considered, we can remove all the arrows of the graph to obtain a Markov network. The obtained Markov network would have maximum cliques <script type="math/tex">\{x_1,x_2\}</script>, …, <script type="math/tex">\{x_{n-2},x_{n-1}\}</script> and <script type="math/tex">\{x_{n-1},x_{n}\}</script> and the corresponding potentials are</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\psi_{1,2}(x_1,x_2)&=P(x_2\vert x_1)P(x_1),\\&\vdots\\\psi_{x_{n-2},x_{n-1}}(x_{n-2},x_{n-1})&=P(x_{n-2}\vert x_{n-1}),\\\psi_{n-1,n}(x_{n-1},x_n)&=P(x_{n-1}\vert x_n).\end{aligned} %]]></script>

<p>The rearrangement for the Markov network then follows that</p>

<script type="math/tex; mode=display">P(x_n)=\sum_{x_{n-1}}\psi_{n-1,n}(x_{n-1}, x_{n})\cdot \sum_{x_{n-2}}\psi_{n-2,n-1}(x_{n-2}, x_{n-1})\cdot\dots\cdot \sum_{x_{1}}\psi_{1,2}(x_1,x_2).</script>

<p>Now the computation composes of $n-1$ summations. More importantly, unlike the previous one sums over $k^{n-1}$ values, this exression allows each term only need to sum over $k\times k$ values. Specifically, the sum</p>

<script type="math/tex; mode=display">\sum_{x_i}\psi_{x_i,x_{i+1}}(x_{i},x_{i+1})</script>

<p>only involves two variables and thus the summation is over $k\times k$ values. Then the overall computation is of $O(nk^2)$ complexity, which is much better than the naive $O(k^n)$ method.</p>

<p>However, the variable elimination (VE) method requires an ordering over the variables. In fact, the running time of VE on different orderings would vary greatly, while to find the best ordering is still an NP-hard problem. Moreover, VE method for $P(x_n)$ can be hard to be generalized to other marginal distribution as it does not store the intermediate results.</p>

<h1 id="2-belief-propagation">2. Belief Propagation</h1>

<p>For convenience, we consider undirected graphs with tree structure, where the optimal variable elimination ordering for node $x_i$ is the post-order iteration of the subtree rooted at $x_i$. The relationship between any two directly connected nodes is decided by which node the tree is rooted at and how far the two nodes are away from the root: the close one is the parent of the farther one.</p>

<h2 id="21-message">2.1. Message</h2>

<p>For a tree graph, its maximum cliques contains only two nodes. By VE algorithm, to compute the marginal $P(x_i)$, we need to eliminate all nodes that are in the subtree of $x_i$. For node $x_j$, the elimination involves computing $\sum_{x_j}\psi_{x_j,x_k}(x_j,x_k)m_{j,k}$ where $x_k$ is the parent of $x_j$ in the tree. The term $m_{j,k}$ can be thought of a <em>message</em> that $x_j$ sends to $x_k$ about the subtree rooted at $x_j$. Similarly, the computing result can be viewed as</p>

<script type="math/tex; mode=display">m_{k,l}=\sum_{x_j}\psi_{x_j,x_k}(x_j,x_k)m_{j,k}</script>

<p>that contains the information for $x_l$, the parent of $x_k$, about the subtree rooted at $x_k$. By doing so, at the end of VE, $x_i$ would receive messages from all of its immediate children and then marginalize them out to yield the final marginal.</p>

<p>Suppose that after computing $P(x_i)$, we are interested in computing $P(x_k)$ as well. If we use VE algorithm again, we can find that the computation also involves the messages $m_{j,k}$ as node $x_k$ is still the parent of node $x_j$. Moreover, such a message is exactly the same as the one used in computing $P(x_i)$ since the graph structure does not change. Therefore, it is easy to find that if we store the intermediary messages of VE, we can obtain other marginals quickly.</p>

<h2 id="22-sum-product">2.2. Sum-Product</h2>

<p>Belief propagation can be viewed as a combination of VE and <em>caching</em>. For each edge between $x_i$ and $x_j$, the messages passing on it are $m_{i,j}$ and $m_{j,i}$, which depends on the marginal we want to determine. After computing all these messages, one can compute any marginals with these messages.</p>

<p>Belief propagation:</p>

<ul>
  <li>Set a node, for example, node $x_i$, as the root;</li>
  <li>For each $x_j$ in $N(x_i)$, <em>i.e.,</em> the neighborhood of $x_i$, collect the messages sent to $x_i$:</li>
</ul>

<script type="math/tex; mode=display">m_{j,i}=\sum_{x_j}\psi_{x_i,x_j}(x_i,x_j)\psi_{x_j}(x_j)\prod_{k\in N(x_j)\setminus i}m_{k,j};</script>

<ul>
  <li>For each $x_j$ in $N(x_i)$, collect the messages sent from $x_i$:</li>
</ul>

<script type="math/tex; mode=display">m_{i,j}=\sum_{x_i}\psi_{x_j,x_i}(x_j,x_i)\psi_{x_i}(x_i)\prod_{k\in N(x_i)\setminus j}m_{k,i}.</script>

<p>By doing so, we can obtain all the messages with <script type="math/tex">2\vert E\vert</script> steps, where $E$ is the set of edges. Then for any marginal we have</p>

<script type="math/tex; mode=display">P(x_i)=\psi_i(x_i)\prod_{k\in N(x_i)}m_{k,i}.</script>

<h2 id="23-max-product">2.3 Max-Product</h2>

<p>We now consider a problem of finding the set of values that have the largest probability so that</p>

<script type="math/tex; mode=display">\hat{\text{x}}=\arg\max_{\text{x}} P(\text{x}).</script>

<p>Notice that</p>

<script type="math/tex; mode=display">\max_{\text{x}} P(\text{x})=\max_{\text{x}_1}\dots \max_{\text{x}_n}P(\text{x}).</script>

<p>By <em>sum-product</em>, we have</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\max_{\text{x}} P(\text{x})&=\max_{x_1}\dots \max_{x_n}\psi_i(x_i)\prod_{k\in N(x_i)}m_{k,i}\\&=\max_{x_n}\max_{x_n-1{}}\psi_{x_n,x_{n-1}}(x_n,x_{n-1})\max_{x_{n-2}}\psi_{x_{n-1},x_{n-2}}(x_{n-1},x_{n-2})\dots\max_{x_1}\psi_{x_2,x_1}(x_2,x_1)\psi_{x_1}(x_1).\end{aligned} %]]></script>

<p>Such a method for maximizing <em>max-product</em> is known as <em>max-product</em> algorithm.</p>

<h1 id="3-conclusion">3. Conclusion</h1>

<p>In this post, we briefly introduced two algorithms for <em>exact inference</em> in graphical models. Given a proper order of nodes, <em>variable elimination</em> algorithm is efficient. However, the finding of the proper order is an NP-hard problem. Besides, each query of marginals needs running the algorithm, during which the computation can be highly redundant. To improve computing efficiency, <em>belif propagation</em> stores the intermediate results as messages. After that, one can get any marginal by the messages. Moreover, we can also exploit those messages to determine the values of random varaibles with the largest probability, which is known as <em>max-product</em>.</p>


</div>


<div class="related">
  <h2>Related posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2020/11/29/probabilistic_graphical_models-ml08/">
            Machine Learning - 08 Probabilistic Graphical Models
            <small>29 Nov 2020</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2020/11/12/exponential_family-ml07/">
            Machine Learning - 07 Exponential Family
            <small>12 Nov 2020</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2020/11/05/kernel_method-ml06/">
            Machine Learning - 06 Kernel Method
            <small>05 Nov 2020</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>


<!-- Comments -->
<!--

-->
<!-- Comments Form -->
<!--
  <form method="POST" action="">
    <input name="options[redirect]" type="hidden" value="https://example.com">
    <input name="options[slug]" type="hidden" value="inference-ml09">
      <label>*Name</label>
      <input name="fields[name]" type="text">
      <label>E-mail</label>
      <input name="fields[email]" type="email"><br>
      <label>Message:</label>
      <textarea style="width:100%" name="fields[message]" rows="12"></textarea>
      <button type="submit">Submit comment</button>
      <small>Comments will appear after moderation.</small>
  </form>
-->
      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>