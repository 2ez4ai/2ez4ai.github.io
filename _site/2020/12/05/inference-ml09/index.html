<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-51FFV4BDWW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-51FFV4BDWW');
</script>
  
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    Machine Learning - 09 Exact Inference of Graphical Models &middot; Jingye Wang
  </title>

  
  <link rel="canonical" href="/2020/12/05/inference-ml09/">
  

  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>"You can’t connect the dots looking forward; you can only connect them looking backward."</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/">Home</a>

    

    
    
      
        
      
    
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/categories/">Archive</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/about/">About</a>
        
      
    
    <span class="sidebar-nav-item">Currently v0.2.2</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2021. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">Jingye Wang</a>
            <small>A personal blog.</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <style>
  .button {
    border: 2px solid;
    border-radius: 7.5px;
    color: white;
    padding: 2.5px 7.5px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    transition-duration: 0.1s;
    cursor: pointer;
  }

  .buttonC {
    background-color: white; 
    color: black; 
    border: 2px solid #2e3131;
  }

  .buttonC:hover {
    background-color: #2e3131;
    color: white;
  }
  a:hover{
    text-decoration: none;
  }
</style>
<div class="post">
  <h1 class="post-title">Machine Learning - 09 Exact Inference of Graphical Models </h1>
  <span class="post-date">
    05 Dec 2020
    <p></p>
  <div class="post-categories">
      
      
      <a class="button buttonC" href="/categories/#machine-learning">machine-learning</a>&nbsp;
      
      
    </div>
  </span>
  <p><em>The notes are based on the <a href="https://github.com/shuhuai007/Machine-Learning-Session">session</a>, <a href="https://ermongroup.github.io/cs228-notes/">CS228-notes</a> and <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">PRML</a>. For the fundamental of probability, one can refer to <a href="https://drive.google.com/file/d/1VmkAAGOYCTORq1wxSQqy255qLJjTNvBI/view">Introduction to Probability</a>. Many thanks to these great works.</em></p>

<ul id="markdown-toc">
  <li><a href="#0-introduction" id="markdown-toc-0-introduction">0. Introduction</a></li>
  <li><a href="#1-variable-elimination" id="markdown-toc-1-variable-elimination">1. Variable Elimination</a></li>
  <li><a href="#2-belief-propagation" id="markdown-toc-2-belief-propagation">2. Belief Propagation</a>    <ul>
      <li><a href="#21-message" id="markdown-toc-21-message">2.1. Message</a></li>
      <li><a href="#22-sum-product" id="markdown-toc-22-sum-product">2.2. Sum-Product</a></li>
      <li><a href="#23-max-product" id="markdown-toc-23-max-product">2.3 Max-Product</a></li>
    </ul>
  </li>
  <li><a href="#3-conclusion" id="markdown-toc-3-conclusion">3. Conclusion</a></li>
</ul>
<h1 id="0-introduction">0. Introduction</h1>

<p>In the last <a href="https://2ez4ai.github.io/2020/11/29/probabilistic_graphical_models-ml08/">post</a>, we introduce graphical models which is capable of representing random variables and the conditional independences among them. We now consider the problem of inference in graphical models. Particularly, we wish to compute posterior distributions of one or more nodes conditioned on some other known (observed) nodes, and the techniques we shall talk in this post are for <em>exact inference</em>.</p>

<h1 id="1-variable-elimination">1. Variable Elimination</h1>

<p>We first consider the marginal inference in a <em>chain</em> Bayesian network which has the following joint distribution,</p>

\[P(x_1,x_2,\dots,x_n)=P(x_1)\prod_{i=2}^nP(x_i\vert x_{i-1}).\]

<p>Suppose we want to infer the marginal distribution $P(x_n)$. A naive way to do that is marginalizing $x_1,x_2,\dots,x_{n-1}$:</p>

\[P(x_n)=\sum_{x_1}\sum_{x_2}\dots\sum_{x_{n-1}}P(x_1,x_2,\dots,x_{n-1}).\]

<p>However, as there are $n-1$ variables each with $k$ states, the computation needs to sum the probability over $k^{n-1}$ values and would scale exponentially with the length of the chain. To simplify the computation, we can leverage <em>variable elimination</em>.</p>

<p>The key of <em>variable elimination</em> is in <em>rearranging the order</em> of the summations and the multiplications. Specifically, the marginal distribution follows that</p>

\[\begin{aligned}P(x_n)&amp;=\sum_{x_1}\sum_{x_2}\dots\sum_{x_{n-1}}P(x_1)\prod_{i=2}^nP(x_i\vert x_{i-1})\\&amp;=\sum_{x_{n-1}}P(x_n\vert x_{n-1})\cdot \sum_{x_{n-2}}P(x_{n-1}\vert x_{n-2})\cdot\dots\cdot \sum_{x_{1}}P(x_{2}\vert x_{1})P(x_1).\end{aligned}\]

<p>Such a rearrangement works because multiplication is distributive over addition,</p>

\[ab+ac=a(b+c),\]

<p>where the number of arithmetic operations are reduced from three (the left-hand side) to two (the right-hand side). Before we move on, we generalize the new expression to a Markov network since every Bayesian network can be transformed into a Markov network. For the chain Bayesian network we considered, we can remove all the arrows of the graph to obtain a Markov network. The obtained Markov network would have maximum cliques \(\{x_1,x_2\}\), …, \(\{x_{n-2},x_{n-1}\}\) and \(\{x_{n-1},x_{n}\}\) and the corresponding potentials are</p>

\[\begin{aligned}\psi_{1,2}(x_1,x_2)&amp;=P(x_2\vert x_1)P(x_1),\\&amp;\vdots\\\psi_{x_{n-2},x_{n-1}}(x_{n-2},x_{n-1})&amp;=P(x_{n-2}\vert x_{n-1}),\\\psi_{n-1,n}(x_{n-1},x_n)&amp;=P(x_{n-1}\vert x_n).\end{aligned}\]

<p>The rearrangement for the Markov network then follows that</p>

\[P(x_n)=\sum_{x_{n-1}}\psi_{n-1,n}(x_{n-1}, x_{n})\cdot \sum_{x_{n-2}}\psi_{n-2,n-1}(x_{n-2}, x_{n-1})\cdot\dots\cdot \sum_{x_{1}}\psi_{1,2}(x_1,x_2).\]

<p>Now the computation composes of $n-1$ summations. More importantly, unlike the previous one sums over $k^{n-1}$ values, this expression allows each term only need to sum over $k\times k$ values. Specifically, the sum</p>

\[\sum_{x_i}\psi_{x_i,x_{i+1}}(x_{i},x_{i+1})\]

<p>only involves two variables and thus the summation is over $k\times k$ values. Then the overall computation is of $O(nk^2)$ complexity, which is much better than the naive $O(k^n)$ method.</p>

<p>However, the variable elimination (VE) method requires an ordering over the variables. In fact, the running time of VE on different orderings would vary greatly, while to find the best ordering is still an NP-hard problem. Moreover, VE method for $P(x_n)$ can be hard to be generalized to other marginal distribution as it does not store the intermediate results.</p>

<h1 id="2-belief-propagation">2. Belief Propagation</h1>

<p>For convenience, we consider undirected graphs with tree structure, where the optimal variable elimination ordering for node $x_i$ is the post-order iteration of the subtree rooted at $x_i$. The relationship between any two directly connected nodes is decided by which node the tree is rooted at and how far the two nodes are away from the root: the close one is the parent of the farther one.</p>

<h2 id="21-message">2.1. Message</h2>

<p>For a tree graph, its maximum cliques contains only two nodes. By VE algorithm, to compute the marginal $P(x_i)$, we need to eliminate all nodes that are in the subtree of $x_i$. For node $x_j$, the elimination involves computing $\sum_{x_j}\psi_{x_j,x_k}(x_j,x_k)m_{j,k}$ where $x_k$ is the parent of $x_j$ in the tree. The term $m_{j,k}$ can be thought of a <em>message</em> that $x_j$ sends to $x_k$ about the subtree rooted at $x_j$. Similarly, the computing result can be viewed as</p>

\[m_{k,l}=\sum_{x_j}\psi_{x_j,x_k}(x_j,x_k)m_{j,k}\]

<p>that contains the information for $x_l$, the parent of $x_k$, about the subtree rooted at $x_k$. By doing so, at the end of VE, $x_i$ would receive messages from all of its immediate children and then marginalize them out to yield the final marginal.</p>

<p>Suppose that after computing $P(x_i)$, we are interested in computing $P(x_k)$ as well. If we use VE algorithm again, we can find that the computation also involves the messages $m_{j,k}$ as node $x_k$ is still the parent of node $x_j$. Moreover, such a message is exactly the same as the one used in computing $P(x_i)$ since the graph structure does not change. Therefore, it is easy to find that if we store the intermediary messages of VE, we can obtain other marginals quickly.</p>

<h2 id="22-sum-product">2.2. Sum-Product</h2>

<p>Belief propagation can be viewed as a combination of VE and <em>caching</em>. For each edge between $x_i$ and $x_j$, the messages passing on it are $m_{i,j}$ and $m_{j,i}$, which depends on the marginal we want to determine. After computing all these messages, one can compute any marginals with these messages.</p>

<p>Belief propagation:</p>

<ul>
  <li>Set a node, for example, node $x_i$, as the root;</li>
  <li>For each $x_j$ in $N(x_i)$, <em>i.e.,</em> the neighborhood of $x_i$, collect the messages sent to $x_i$:</li>
</ul>

\[m_{j,i}=\sum_{x_j}\psi_{x_i,x_j}(x_i,x_j)\psi_{x_j}(x_j)\prod_{k\in N(x_j)\setminus i}m_{k,j};\]

<ul>
  <li>For each $x_j$ in $N(x_i)$, collect the messages sent from $x_i$:</li>
</ul>

\[m_{i,j}=\sum_{x_i}\psi_{x_j,x_i}(x_j,x_i)\psi_{x_i}(x_i)\prod_{k\in N(x_i)\setminus j}m_{k,i}.\]

<p>By doing so, we can obtain all the messages with \(2\vert E\vert\) steps, where $E$ is the set of edges. Then for any marginal we have</p>

\[P(x_i)=\psi_i(x_i)\prod_{k\in N(x_i)}m_{k,i}.\]

<h2 id="23-max-product">2.3 Max-Product</h2>

<p>We now consider a problem of finding the set of values that have the largest probability so that</p>

\[\hat{\text{x}}=\arg\max_{\text{x}} P(\text{x}).\]

<p>Notice that</p>

\[\max_{\text{x}} P(\text{x})=\max_{\text{x}_1}\dots \max_{\text{x}_n}P(\text{x}).\]

<p>By <em>sum-product</em>, we have</p>

\[\begin{aligned}\max_{\text{x}} P(\text{x})&amp;=\max_{x_1}\dots \max_{x_n}\psi_i(x_i)\prod_{k\in N(x_i)}m_{k,i}\\&amp;=\max_{x_n}\max_{x_n-1{}}\psi_{x_n,x_{n-1}}(x_n,x_{n-1})\max_{x_{n-2}}\psi_{x_{n-1},x_{n-2}}(x_{n-1},x_{n-2})\\&amp;\quad\ \dots\max_{x_1}\psi_{x_2,x_1}(x_2,x_1)\psi_{x_1}(x_1).\end{aligned}\]

<p>Such a method for maximizing <em>max-product</em> is known as <em>max-product</em> algorithm.</p>

<h1 id="3-conclusion">3. Conclusion</h1>

<p>In this post, we briefly introduced two algorithms for <em>exact inference</em> in graphical models. Given a proper order of nodes, <em>variable elimination</em> algorithm is efficient. However, the finding of the proper order is an NP-hard problem. Besides, each query of marginals needs running the algorithm, during which the computation can be highly redundant. To improve computing efficiency, <em>belief propagation</em> stores the intermediate results as messages. After that, one can get any marginal by the messages. Moreover, we can also exploit those messages to determine the values of random variables with the largest probability, which is known as <em>max-product</em>.</p>


</div>

 <!--https://blog.webjeda.com/jekyll-related-posts/-->
<div class="related">
  <h2>Related posts</h2>
  
  
  

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    
      <div>
      <h5><a href="/2020/11/29/probabilistic_graphical_models-ml08/">Machine Learning - 08 Probabilistic Graphical Models <span class="label label-default">machine-learning</span> </a></h5>
      </div>
      
      
    

  

    
    

    

    
      <div>
      <h5><a href="/2020/11/12/exponential_family-ml07/">Machine Learning - 07 Exponential Family <span class="label label-default">machine-learning</span> </a></h5>
      </div>
      
      
    

  

    
    

    

    
      <div>
      <h5><a href="/2020/11/05/kernel_method-ml06/">Machine Learning - 06 Kernel Method <span class="label label-default">machine-learning</span> </a></h5>
      </div>
      
      
        
</div>



<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://2ez4ai-github-io.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>