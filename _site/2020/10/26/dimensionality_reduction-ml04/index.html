<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-51FFV4BDWW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-51FFV4BDWW');
</script>
  
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    Machine Learning - 04 Dimensionality Reduction &middot; Jingye Wang
  </title>

  
  <link rel="canonical" href="http://localhost:4000/2020/10/26/dimensionality_reduction-ml04/">
  

  <link rel="stylesheet" href="http://localhost:4000/public/css/poole.css">
  <link rel="stylesheet" href="http://localhost:4000/public/css/syntax.css">
  <link rel="stylesheet" href="http://localhost:4000/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://localhost:4000/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="http://localhost:4000/public/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="http://localhost:4000/atom.xml">

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>"You canâ€™t connect the dots looking forward; you can only connect them looking backward."</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="http://localhost:4000/">Home</a>

    

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="http://localhost:4000/about/">About</a>
        
      
    
      
    
      
        
      
    
    <span class="sidebar-nav-item">Currently v0.0.1</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2020. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">Jingye Wang</a>
            <small>A personal blog.</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">Machine Learning - 04 Dimensionality Reduction </h1>
  <span class="post-date">26 Oct 2020</span>
  <p><em>The notes are based on the <a href="https://github.com/shuhuai007/Machine-Learning-Session">session</a>. For the fundamental of linear algebra, one can always refer to <a href="http://math.mit.edu/~gs/linearalgebra/">Introduction to Linear Algebra</a> and <a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">The Matrix Cookbook</a> for more details. Many thanks to these great works.</em></p>

<h1 id="0-curse-of-dimensionality">0. Curse of Dimensionality</h1>

<p><em>The following introduction is derived from <a href="http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf">Pattern Recognition and Machine Learning</a>.</em></p>

<p>High dimensionality often incurs not only calculation issue but also <em>overfitting</em>. The increasing in dimension can point to the methods becoming rapidly unwieldy and of limited practical utility.</p>

<p>Our geometrical intuitions, formed through a life spent in a space of three-dimension, can fail badly when we consider spaces of higher dimensionality. As a simple example, consider a sphere of radius $r = 1$ in a space of $d$ dimensions, and ask what is the fraction of the volume of the sphere that lies between radius $r=1-\varepsilon$ and $r=1$. We can evaluate this fraction by noting that the volume of a sphere of radius $r$ in $d$ dimensions must scale as $r^d$, and so we write</p>

<script type="math/tex; mode=display">V_d(r)=K_dr^d,</script>

<p>where the constant $K_d$ depends only on $d$. Thus the required fraction is given by</p>

<script type="math/tex; mode=display">\frac{V_d(1)-V_d(1-\varepsilon)}{V_d(1)}=\frac{K_dr^d-K_d(1-\varepsilon)^d}{K_dr^d}=1-(1-\varepsilon)^d.</script>

<p>Obviously, for large $d$, this fraction tends to 1 even for small values of $\varepsilon$. Thus, in spaces of high dimensionality, most of the volume of a sphere is concentrated in a thin shell near the surface!</p>

<p>Luckily, for those data of high dimensionality, we can refer to <em>dimensionality reduction</em> algorithms.</p>

<h1 id="1-sample-mean-and-variance">1. Sample Mean and Variance</h1>

<p>Before we move on, we give the following definitions. Suppose we have data <script type="math/tex">X=(x_1,x_2,\dots,x_N)^T\in\mathbb{R}^{N\times d}</script>, where $x_i\in\mathbb{R}^{d\times 1}$ is a sample with $d$ features. Specifically,</p>

<script type="math/tex; mode=display">% <![CDATA[
X=(x_1,x_2,\dots,x_N)^T=\begin{pmatrix}x_{1}^T\\x_{2}^T\\\vdots\\x_{N}^T\end{pmatrix}=\begin{pmatrix}x_{11}&x_{12}&\cdots&x_{1d}\\x_{21}&x_{22}&\cdots&x_{2d}\\\vdots&\vdots&\ddots&\vdots\\x_{N1}&x_{N2}&\cdots&x_{Nd}\end{pmatrix}_{N\times d}. %]]></script>

<p>Then we have the <strong>sample mean</strong></p>

<script type="math/tex; mode=display">\bar X=\frac{1}{N}\sum_{i=1}^Nx_i,</script>

<p>and the <strong>sample covariance</strong></p>

<script type="math/tex; mode=display">S=\frac{1}{N}\sum_{i=1}^N\left(x_i-\bar X\right)\left(x_i-\bar X\right)^T.</script>

<p>However, the <em>sum</em> operation is inconvenient in calculating. Thus we further transform them into</p>

<script type="math/tex; mode=display">\bar X=\frac{1}{N}(x_1,x_2,\dots,x_N)\begin{pmatrix}1\\1\\\vdots\\1\end{pmatrix}=\frac{1}{N}X^T\mathbf{1}_{N\times 1},</script>

<p>and</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}S&=\frac{1}{N}\underbrace{(x_1-\bar X,x_2-\bar X,\dots,x_N-\bar X)}_{X^T-\bar X(1,1,\dots,1)}\begin{pmatrix}(x_1-\bar X)^T\\(x_2-\bar X)^T\\\vdots\\(x_N-\bar X)^T\end{pmatrix}\\&=\frac{1}{N}\left(X^T-\bar X\mathbf{1}_{1\times N}\right)\left(X^T-\bar X\mathbf{1}_{1\times N}\right)^T\\&=\frac{1}{N}\left(X^T-\frac{1}{N}X^T\mathbf{1}_{N\times 1}\mathbf{1}_{1\times N}\right)\left(X^T-\frac{1}{N}X^T\mathbf{1}_{N\times 1}\mathbf{1}_{1\times N}\right)^T\\&=\frac{1}{N}X^T\underbrace{\left(\mathbf{I}_{N}-\frac{1}{N}\mathbf{1}_{N\times N}\right)}_{H}\left(\mathbf{I}_{N}-\frac{1}{N}\mathbf{1}_{N\times N}\right)^TX\\&=\frac{1}{N}X^THH^TX\\&=\frac{1}{N}X^THX,\end{aligned} %]]></script>

<p>where $H$ is <em>centering matrix</em>, and it can be shown that $H^T=H,H^n=H$.</p>

<h1 id="2-principal-component-analysis">2. Principal Component Analysis</h1>

<p>Principal component analysis (PCA) is defined as an <a href="https://en.wikipedia.org/wiki/Orthogonal_transformation">orthogonal</a> <a href="https://en.wikipedia.org/wiki/Linear_transformation">linear transformation</a> that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.</p>

<p>Recall that in <a href="https://19w6.github.io/2020/10/14/linear_classification-ml03/">the previous post</a>, we introduce <em>linear discriminant analysis</em> (LDA), which requires a projection that maximizes the distance between different classes. In PCA, instead of classes, we need to find a projection that maximizes the distance (variance) of all the data so that we can reduce the dimensionality (as a projection involves one dimensionality reduction) while losing the least information (as the greatest variance implies most information). Now we give the mathematically formulation.</p>

<p>The notation in section 1 will be used in this section as well. Since PCA is sensitive to the variance of the data, it is critical to normalize the variables over all dimensions, which yields</p>

<p>$z_i=x_i-\bar X.$</p>

<p>In many case, the values of $z_i\in\mathbb{R}^{d\times 1}$ should be scaled in $[0,1]$ but here we just consider the simple case. We then denote the transformation vector as $W=(w_1,w_2,\dots,w_d)\in\mathbb{R}^{d\times d}$ which is the <em>orthogonal basis</em> of the new coordinate system. Then the new data $\tilde z_i$, <em>i.e.</em>, the projection of $z_i$ on the new coordinate system is</p>

<script type="math/tex; mode=display">\tilde z_i=z_i^TW=(z_i^Tw_1,z_i^Tw_2,\dots,z_i^Tw_d),</script>

<p>where $z_i^Tw_j\in\mathbb{R}$ is the value of $j$-th dimension of the new data. Now suppose we want reduce the dimension from $d$ to $k$, <em>i.e.,</em> we want to preserve the first $k$ dimensions of $\tilde z_i$ while maximizing the variance, which gives us the objective function</p>

<script type="math/tex; mode=display">\mathcal{J}(w)=\sum_{j=1}^k\sum_{i=1}^N\frac{1}{N}\left(z_i^Tw_j-\overline{z^Tw_j}\right)^2,</script>

<p>where <script type="math/tex">\sum_{i=1}^N\frac{1}{N}\left(z_i^Tw_j-\overline{z^Tw_j}\right)^2</script> is the variance of the new data in $j$-th dimension. <script type="math/tex">\overline{z^Tw_j}</script> is the mean of the new data in $j$-th dimension, which is</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\overline{z^Tw_j}&=\frac{1}{N}\sum_{i=1}^Nz_i^Tw_j\\&=\frac{1}{N}\sum_{i=1}^N(x_i-\bar X)^Tw_j\\&=\frac{1}{N}\left(\sum_{i=1}^Nx_i^Tw_j-N\bar X^Tw_j\right)\\&=\frac{1}{N}\left(\sum_{i=1}^Nx_i^Tw_j-\sum_{i=1}^Nx_i^Tw_j\right)\\&=0\end{aligned}. %]]></script>

<p>Therefore the objective function follows that</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\mathcal{J}(w)&=\sum_{j=1}^k\sum_{i=1}^N\frac{1}{N}\left(z_i^Tw_j\right)^2\\&=\sum_{j=1}^k\sum_{i=1}^N\frac{1}{N}\left((x_i-\bar X)^Tw_j\right)^2\\&=\sum_{j=1}^k\sum_{i=1}^N\frac{1}{N}w_j^T(x_i-\bar X)(x_i-\bar X)^Tw_j\\&=\sum_{j=1}^kw_j^TSw_j\end{aligned}, %]]></script>

<p>where each $w_j^TSw_j$ can be maximized independently since $w_j$ are orthogonal. Combined with the constraint that $w_j$ is a unit vector, the problem is then equivalent to</p>

<script type="math/tex; mode=display">\hat w_j=\arg\max_{w_j} w_j^TSw_j\quad \text{s.t. }w_j^Tw_j=1.</script>

<p>The problem can be solved by <a href="https://en.wikipedia.org/wiki/Lagrange_multiplier">the method of Lagrange multipliers</a> as follows,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\mathcal{L}(w_j,\lambda_j)&=w_j^TSw_j+\lambda_j(1-w_j^Tw_j),\\\frac{\partial\mathcal{L}(w_j,\lambda_j)}{\partial w_j}&=2Sw_j-2\lambda_j w_j.\end{aligned} %]]></script>

<p>Setting the second equation to be zero, we have the standard eigenvalue problem</p>

<script type="math/tex; mode=display">S\hat{w}_j=\lambda_j\hat{w}_j\implies SW=\text{diag}(\lambda)W ,</script>

<p><em>i.e.,</em> $\hat w_j$ is actually the eigenvalue of $S$. Plugging the above equation to the objective function, we have</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\max\mathcal{J(\hat{w})}&=\max\sum_{j=1}^k\hat{w}_j^TS\hat{w}_j\\&=\max\sum_{j=1}^k\hat{w}_j^T\lambda_j\hat{w}_j\\&=\max\sum_{j=1}^k\lambda_j\end{aligned}. %]]></script>

<p>Therefore, to reduce the data from $d$ to $k$ dimension, one should select the $k$ eigenvectors of $S$ corresponding to the $k$ largest eigenvalues to construct the $W\in\mathbb{R}^{d\times k}$, which is</p>

<script type="math/tex; mode=display">W=(w_{(1)},w_{(2)},\dots,w_{(k)}),</script>

<p>where $w_{(i)}$ is the eigenvector corresponding to the $i$-th largest eigenvalue.  Then the data in the new coordinate system is</p>

<script type="math/tex; mode=display">X^\text{new}=(x_1-\bar X,x_2-\bar X,\dots, x_N-\bar X)^TW.</script>

<h1 id="3-principal-component-analysis---an-svd-perspective">3. Principal Component Analysis - An SVD Perspective</h1>

<p>Now we consider the <em>singular vector decomposition</em> (SVD) of the centralized data,</p>

<script type="math/tex; mode=display">HX=U\Sigma V^T,</script>

<p>where $U=(u_1,u_2,\dots,u_N)\in\mathbb{R}^{N\times N},\Sigma\in\mathbb{R}^{N\times d},V=(v_1,v_2,\dots,v_N)\in\mathbb{R}^{d\times d}$, and they have the following properties,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}UU^T&=U^TU=\mathbf{I}_{N},\\VV^T&=V^TV=\mathbf{I}_{d},\\\Sigma_{ij}=0,\quad i&=0,1,â€¦,N,j=0,1,â€¦,d,i\ne j.\end{aligned} %]]></script>

<p>We further represent $\Sigma$ as $\lambda(\sigma_1,\sigma_2,\dots,\sigma_d)$. Then according to the analysis in section 1, the covariance of the data is</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}S&=\frac{1}{N}X^THX\\&=\frac{1}{N}X^TH^THX\\&=\frac{1}{N}(HX)^THX\\&=\frac{1}{N}V\Sigma^TU^TU\Sigma V^T\\&=V\text{diag}\left(\frac{\sigma_1^2}{N},\frac{\sigma_2^2}{N},\dots,\frac{\sigma_d^2}{N}\right)V^T\end{aligned}. %]]></script>

<p>By multiplying $V$ on both sides, it follows that</p>

<script type="math/tex; mode=display">SV=V\text{diag}\left(\frac{\sigma_1^2}{N},\frac{\sigma_2^2}{N},\dots,\frac{\sigma_d^2}{N}\right)\implies Sv_i=\frac{\sigma_i^2}{N}v_i,</script>

<p>which is consistent with the conclusion of PCA. Therefore, instead of decomposing the covariance matrix $S$, we can conduct an SVD on the centralized data, which gives the transformation matrix that allows us to obtain the new data</p>

<script type="math/tex; mode=display">Z=HX\cdot V.</script>

<p>By selecting $k$ vectors of $V$ according to the single values, we can reduce the original data matrix from $d$ to $k$ dimensions. Such decomposition may take advantage when $N\ll d$. Intuitively, $HX\in\mathbb{R}^{N\times d}$ and $S\in\mathbb{R}^{d\times d}$. When $N\ll d$, decomposing $HX$ should be more efficient than decomposing $S$.</p>

<h1 id="4-principal-coordinates-analysis">4. Principal Coordinates Analysis</h1>

<p><em>Principal coordinates analysis</em> (PCoA) is a well known technique in many fields. It actually can be derived from PCA. Specifically, we consider a matrix</p>

<script type="math/tex; mode=display">T=HXX^TH^T.</script>

<p>By SVD, we have</p>

<script type="math/tex; mode=display">T=U\Sigma V^TV\Sigma^TU^T=U\Sigma\Sigma^T U^T,</script>

<p>which is similar to the decomposition of $S$ in section 3. Further, by multiplying $U\Sigma$ on the both sides, we have</p>

<script type="math/tex; mode=display">TU\Sigma=U\Sigma\Sigma^TU^TU\Sigma=U\Sigma\text{diag}\left(\sigma_1^2,\sigma_2^2,\dots,\sigma_d^2\right)\implies T(U\Sigma)_i=\sigma^2_i(U\Sigma)_i.</script>

<p>Therefore, $U\Sigma$ is actually composed of $d$  eigenvalues of $T$. Recall that in section 3, we have the new data as</p>

<script type="math/tex; mode=display">Z=HX\cdot V=U\Sigma V^T\cdot V=U\Sigma,</script>

<p>which implies that by the eigenvalue decomposition of $T$, we can get the new data directly. Such a dimensionality reduction technique with a different perspective is PCoA. Notice $T\in\mathbb{R}^{N\times N}$, thus the complexity of PCoA is $O(N^2)$.</p>

<h1 id="5-probabilistic-principal-component-analysis">5. Probabilistic Principal Component Analysis</h1>

<p>Just like the notations we used in previous sections, we define the new data we want to transform $X$ into is $Z=(z_1,z_2,\dots,z_N)$ where $z_i\in\mathbb{R}^{k\times 1}$. However, in <em>probabilistic principal component analysis</em> (PPCA), we further introduce randomness as</p>

<script type="math/tex; mode=display">z_i\sim\mathcal{N}(\mathbf{0}_{k\times 1}, \mathbf{I}_{k\times k}),</script>

<script type="math/tex; mode=display">x_i=Wz_i+\mu+\varepsilon,</script>

<script type="math/tex; mode=display">\varepsilon\sim\mathcal{N}(\mathbf{0}_{d\times 1},\sigma^2\mathbf{I}_{d\times d}),</script>

<p>where $W\in\mathbb{R}^{d\times k}, \mu\in\mathbb{R}^{d\times 1}$ and $\sigma^2$ are the parameters to be learned ($\mu$ can be viewed as the bias term of many machine learning model). Such randomization can generalize the model to the unseen data. One can also refer such a model to <em>linear Gaussian model</em>. In particular,  there are two phases. The first is learning:</p>

<script type="math/tex; mode=display">\hat W,\hat \mu, \hat \sigma^2=\arg\max_{W,\mu,\sigma^2}P(X\vert Z;W,\mu,\sigma^2).</script>

<p>The second is inference (dimensionality reduction):</p>

<script type="math/tex; mode=display">Z=\arg\max_{\tilde Z} P(\tilde Z\vert X).</script>

<p>The learning part can be dealt with <em>expectationâ€“maximization algorithm</em>. The following is the details of the inference procedure. According to the definition, we have</p>

<script type="math/tex; mode=display">x_i\vert z_i\sim\mathcal{N}(Wz_i+\mu,\sigma^2\mathbf{I}_{d\times d}),</script>

<p>where $z_i$ is a sample rather than a random variable. Also, we have</p>

<script type="math/tex; mode=display">\mathbb{E}[x_i]=\mathbb{E}[Wz_i+\mu+\varepsilon]=\mu,</script>

<script type="math/tex; mode=display">var[x_i]=var[Wz_i+\mu+\varepsilon]=var[Wz_i]+var[\varepsilon]=WW^T+\sigma^2\mathbf{1}_{d\times d},</script>

<p>and</p>

<script type="math/tex; mode=display">x_i\sim\mathcal{N}(\mu, WW^T+\sigma^2\mathbf{1}_{d\times d}).</script>

<p>Then we consider the covariance $Cov(x_i,z_i)$,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}Cov(x_i,z_i)&=\mathbb{E}[(x_i-\mu)(z_i-\mathbf{0}_{k\times 1})^T]\\&=\mathbb{E}[(x_i-\mu)z_i^T]\\&=\mathbb{E}[(Wz_i+\varepsilon)z_i^T]\\&=\mathbb{E}[Wz_iz_i^T]+\mathbb{E}[\varepsilon z_i^T]\\&=Wvar[z_i]+\mathbb{E}[\varepsilon]\mathbb{E}[z_i^T]\\&=W\end{aligned}. %]]></script>

<p>Hence the union distribution of $(x_i,z_i)^T$ is</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{pmatrix}x_i\\z_i\end{pmatrix}\sim\mathcal{N}\left(\begin{pmatrix}\mu\\\mathbf{0}_{k\times1} \end{pmatrix},\begin{pmatrix}WW^T+\sigma^2\mathbf{1}_{d\times d}&W\\W^T&\mathbf{1}_{k\times k}\end{pmatrix}\right). %]]></script>

<p>By the formula derived in <a href="https://19w6.github.io/2020/09/28/intro-ml01/">session 01</a>, it can be shown that</p>

<script type="math/tex; mode=display">z_i\vert x_i\sim\mathcal{N}(W^T(WW^T+\sigma^2\mathbf{1}_{d\times d})^{-1}(x_i-\mu),\mathbf{1}_{k\times k}-W^T(WW^T+\sigma^2\mathbf{1}_{d\times d})^{-1}W).</script>

<h1 id="6-conclusion">6. Conclusion</h1>

<p>In this post, we introduced the naive <em>principal component analysis</em> (PCA) model. Then we conducted <em>singular vector decomposition</em> on the centralized data, which gives us the same conclusion as that of PCA. Such conclusion can also be derived from <em>Principal coordinates analysis</em> (PCoA) model. Further, by introducing parameters, <em>probabilistic principal component analysis</em> (PPCA) can generalize PCA to handle unseen data.</p>


</div>


<div class="related">
  <h2>Related posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2020/12/06/inference-ml09/">
            Machine Learning - 09 Exact Inference of Graphical Models
            <small>06 Dec 2020</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2020/11/29/probabilistic_graphical_models-ml08/">
            Machine Learning - 08 Probabilistic Graphical Models
            <small>29 Nov 2020</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2020/11/12/exponential_family-ml07/">
            Machine Learning - 07 Exponential Family
            <small>12 Nov 2020</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>


<!-- Comments -->
<!--

-->
<!-- Comments Form -->
<!--
  <form method="POST" action="">
    <input name="options[redirect]" type="hidden" value="https://example.com">
    <input name="options[slug]" type="hidden" value="dimensionality_reduction-ml04">
      <label>*Name</label>
      <input name="fields[name]" type="text">
      <label>E-mail</label>
      <input name="fields[email]" type="email"><br>
      <label>Message:</label>
      <textarea style="width:100%" name="fields[message]" rows="12"></textarea>
      <button type="submit">Submit comment</button>
      <small>Comments will appear after moderation.</small>
  </form>
-->
      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>