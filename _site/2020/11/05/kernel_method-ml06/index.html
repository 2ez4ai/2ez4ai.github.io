<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-51FFV4BDWW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-51FFV4BDWW');
</script>
  
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    Machine Learning - 06 Kernel Method &middot; Jingye Wang
  </title>

  
  <link rel="canonical" href="/2020/11/05/kernel_method-ml06/">
  

  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>"You can’t connect the dots looking forward; you can only connect them looking backward."</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/">Home</a>

    

    
    
      
        
      
    
      
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/categories/">Archive</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/about/">About</a>
        
      
    
    <span class="sidebar-nav-item">Currently v0.1.2</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2021. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">Jingye Wang</a>
            <small>A personal blog.</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <style>
  .button {
    border: 2px solid;
    border-radius: 7.5px;
    color: white;
    padding: 2.5px 7.5px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    transition-duration: 0.1s;
    cursor: pointer;
  }

  .buttonC {
    background-color: white; 
    color: black; 
    border: 2px solid #2e3131;
  }

  .buttonC:hover {
    background-color: #2e3131;
    color: white;
  }
  a:hover{
    text-decoration: none;
  }
</style>
<div class="post">
  <h1 class="post-title">Machine Learning - 06 Kernel Method </h1>
  <span class="post-date">
    05 Nov 2020
    <p></p>
  <div class="post-categories">
      
      
      <a class="button buttonC" href="/categories/#machine-learning">machine-learning</a>&nbsp;
      
      
    </div>
  </span>
  <p><em>The notes are based on the <a href="https://github.com/shuhuai007/Machine-Learning-Session">session</a>. For the fundamental of linear algebra, one can always refer to <a href="http://math.mit.edu/~gs/linearalgebra/">Introduction to Linear Algebra</a> and <a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">The Matrix Cookbook</a> for more details. Many thanks to these great works.</em></p>

<ul id="markdown-toc">
  <li><a href="#0-introduction" id="markdown-toc-0-introduction">0. Introduction</a></li>
  <li><a href="#1-kernel-method" id="markdown-toc-1-kernel-method">1. Kernel method</a></li>
  <li><a href="#2-kernel-function" id="markdown-toc-2-kernel-function">2. Kernel function</a></li>
  <li><a href="#3-conclusion" id="markdown-toc-3-conclusion">3. Conclusion</a></li>
</ul>
<h1 id="0-introduction">0. Introduction</h1>

<p>Kernel methods are a class of algorithms for pattern analysis. The name of kernel methods comes from the use of <em>kernel function</em>, which enable operations in a high-dimensional and implicit space. Specifically, by <a href="https://en.wikipedia.org/wiki/Cover%27s_theorem">Cover’s theorem</a>, given a set of training data that is not <em>linearly separable</em>, one can with high probability transform it into a training set that is linearly separable by projecting it into a higher-dimensional space via some <em>non-linear transformation</em>. With the help of kernel function, the operation, <em>i.e.,</em> inner product, it involves after transforming can be often computationally cheaper than the explicit computation. Such an approach is called the <em>kernel trick</em>. In this post, we will focus on the application of kernel method to SVM.</p>

<h1 id="1-kernel-method">1. Kernel method</h1>

<p>Define the data set as \(\mathcal{D}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}, X=\{x_1,x_2,\dots,x_N\}\) and \(Y=\{y_1,y_2,\dots,y_N\}\) where $x_i\in\mathbb{R}^{d\times 1}$ and \(y_i\in\{-1,1\}\). We further assume that the data set is non-linearly separable. Kernel method supposes that there is a non-linear transformation $\phi(x):\mathbb{R}^{d\times 1}\to\mathbb{R}^{p\times 1},d&lt;p,$  such that \(\mathcal{D}_p=\{(\phi(x_1),y_1),(\phi(x_2),y_2),\dots,(\phi(x_N),y_N)\}\) are linearly separable. For such a linearly separable data set, recalling the problem we formulated in section 1.3 of <a href="https://2ez4ai.github.io/2020/10/28/support_vector_machine-ml05/">SVM</a>, we have the duality problem</p>

\[\begin{aligned}\min_\lambda&amp;\quad \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^N\left(\lambda_i\lambda_jy_iy_j\phi^T(x_i)\phi(x_j)\right)-\sum_{i=1}^N\lambda_i\\\text{s.t.}&amp;\quad \lambda_i\ge0,i=1,2,\dots,N\\&amp;\quad \sum_{i=1}^N\lambda_iy_i=0.\end{aligned}\]

<p>However, after transforming, the inner product $\phi(x_i)^T\phi(x_j)=\langle\phi(x_i),\phi(x_j)\rangle$ could be hard to obtain (consider the case that $\phi(\cdot)$ has infinite dimensions), which requires the aid of <em>kernel function</em>.</p>

<h1 id="2-kernel-function">2. Kernel function</h1>

<p>A kernel function is defined as $K:\mathbb{R}^{d\times 1}\times\mathbb{R}^{d\times 1}\to\mathbb{R}$. Specifically, for non-linear transformation $\phi(\cdot)\in\mathcal{H}\text{ (Hilbert space) }:\mathbb{R}^{d\times 1}\to\mathbb{R}^{p\times 1}$ and any $x_i,x_j\in\mathbb{R}^{d\times 1}$, we call $K(x_i,x_j)=\langle\phi(x_i),\phi(x_j)\rangle$ a kernel function. Such a kernel function is regarded <em>positive definite</em>, which satisfies</p>

\[K(x_i,x_j)=K(x_j,x_i),\]

<p>and for \(x_{1},x_{2},\dots,x_{N}\in\mathbb{R}^{d\times 1},\)</p>

\[\mathcal{K}=[K(x_{i},x_{j})]_{N\times N}\text{ is a positive semi-definite (PSD) matrix},\]

<p>where \(\mathcal{K}\) is called <em>Gram matrix</em> of $K$ over set \(\{x_{1},x_{2},\dots,x_{N}\}\). When the explicit expression of $\phi(\cdot)$ is hard to be determined, it quite often to show the positive definiteness of a kernel function via its corresponding Gram matrix.</p>

<p><strong>(Properties)</strong> We now show two <em>properties</em> of kernel functions.</p>

<ul>
  <li>Let $K$ be kernel function such that \(K:\mathbb{R}^{d\times 1}\times\mathbb{R}^{d\times 1}\to\mathbb{R}\), then we define its Gram matrix $\mathcal{K}\in\mathbb{R}^{N\times N}$ over \(\{x_1,x_2,\dots,x_N\}\) where $x_i\in\mathbb{R}^{d\times 1}$. Considering the mapping function $\phi(\cdot):\mathbb{R}^{d\times 1}\to\mathbb{R}^{p\times 1}$, we have</li>
</ul>

\[K(x_i,x_j)=\langle\phi(x_i),\phi(x_j)\rangle, \phi(\cdot)\in\mathcal{H}\implies \begin{cases}K(x_i,x_j)=K(x_j,x_i)\\ \mathcal{K}\text{ is a PSD matrix}\end{cases}.\]

<p><em>Proof</em>:</p>

<p>By the definition of $K(x_i,x_j)$, we have</p>

\[K(x_i,x_j)=\langle \phi(x_i),\phi(x_j)\rangle,\quad K(x_j,x_i)=\langle \phi(x_j),\phi(x_i)\rangle.\]

<p>By the symmetry of inner product, we have \(\langle \phi(x_i),\phi(x_j)\rangle=\langle \phi(x_j),\phi(x_i)\rangle\). It then follows that</p>

\[K(x_i,x_j)=K(x_j,x_i).\]

<p>Therefore the Gramian matrix $\mathcal{K}=[K(x_{i},x_{j})]_{N\times N}$ is symmetric real matrix. Now we show that \(\forall\alpha\in\mathbb{R}^{R\times 1}, \alpha^T\mathcal{K}\alpha\ge 0.\) The notation is given by</p>

\[\begin{aligned}\alpha^T\mathcal{K}\alpha=(\alpha_1,\alpha_2,\dots,\alpha_N)\begin{bmatrix}K_{11}&amp;K_{12}&amp;\dots&amp;K_{1N}\\K_{21}&amp;K_{22}&amp;\dots&amp;K_{2N}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\K_{N1}&amp;K_{N2}&amp;\dots&amp;K_{NN}\end{bmatrix}\begin{pmatrix}\alpha_1\\\alpha_2\\\vdots\\\alpha_N\end{pmatrix}\end{aligned},\]

<p>where $K_{ij}=K(x_{ri},x_{rj})$. We then have</p>

\[\begin{aligned}\alpha^T\mathcal{K}\alpha&amp;=\sum_{i=1}^R\sum_{j=1}^R \alpha_i\alpha_jK_{ij}\\&amp;=\sum_{i=1}^R\sum_{j=1}^R \alpha_i\alpha_j\phi^T(x_{ri})\phi(x_{rj})\\&amp;=\sum_{i=1}^R \alpha_i\phi^T(x_{ri})\sum_{j=1}^R\alpha_j\phi(x_{rj})\\&amp;=\left(\sum_{i=1}^R \alpha_i\phi(x_{ri})\right)^T\left(\sum_{j=1}^R\alpha_j\phi(x_{rj})\right)\\&amp;=\left\langle\left(\sum_{i=1}^R \alpha_i\phi(x_{ri})\right), \left(\sum_{j=1}^R \alpha_i\phi(x_{rj})\right)\right\rangle\\&amp;=\left\vert\left\vert\sum_{i=1}^R \alpha_i\phi(x_{ri})\right\vert\right\vert^2,\end{aligned}\]

<p>therefore, $\alpha^T\mathcal{K}\alpha\ge 0$ and $\mathcal{K}$ is a PSD matrix.$\tag*{$\blacksquare$}$</p>

<ul>
  <li>Let $\mathcal{K}\in\mathbb{R}^{d\times d}$ be a symmetric PSD matrix, then for \(\{x_1,x_2,\dots,x_N\}\) where $x_i\in\mathbb{R}^{d\times 1}$, we have kernel function $K(x_i,x_j)=x_i^T\mathcal{K}x_j$.</li>
</ul>

<p><em>Proof</em>:</p>

<p>Consider the <em>diagonalisation</em> of $\mathcal{K}=Q^T\Lambda Q$ by an orthogonal matrix $Q$, where $\Lambda$ is a diagnoal matrix containing the non-negative eigenvalues of $\mathcal{K}$. Let $\sqrt{\Lambda}$ be the diagonal matrix with the square roots of the eigenvalues and set $A=\sqrt{\Lambda}Q$.  Then for \(\{x_1,x_2,\dots,x_N\}\) where $x_i\in\mathbb{R}^{d\times 1}$, we have</p>

\[x_i^T\mathcal{K}x_j=x_i^TQ^T\Lambda Qx_j=x_i^TA^TA x_j=\langle A x_i,Ax_j\rangle.\]

<p>Therefore we have kernel function $K(x_i,x_j)=x_i^T\mathcal{K}x_j=\langle Ax_i,Ax_j\rangle$ with linear transformation $\phi(\cdot)=A\cdot. \tag*{$\blacksquare$}$</p>

<h1 id="3-conclusion">3. Conclusion</h1>

<p>In this post, we introduced <em>kernel method</em> for classification problem. Given <em>Cover’s theorem</em>, we can project non-linear data into high-dimensional space and obtain linearly separable data. To simplify the computation incurred by the duality problem, we can leverage <em>kernel function</em> to avoid the computing labor.</p>

<p>This is definitely not a good introduction to kernel methods. For more details of kernel method, I would recommend <a href="https://people.eecs.berkeley.edu/~jordan/kernels/0521813972c03_p47-84.pdf">Kernel methods: an overview</a>.</p>

</div>

 <!--https://blog.webjeda.com/jekyll-related-posts/-->
<div class="related">
  <h2>Related posts</h2>
  
  
  

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    
      <div>
      <h5><a href="/2020/12/05/inference-ml09/">Machine Learning - 09 Exact Inference of Graphical Models <span class="label label-default">machine-learning</span> </a></h5>
      </div>
      
      
    

  

    
    

    

    
      <div>
      <h5><a href="/2020/11/29/probabilistic_graphical_models-ml08/">Machine Learning - 08 Probabilistic Graphical Models <span class="label label-default">machine-learning</span> </a></h5>
      </div>
      
      
    

  

    
    

    

    
      <div>
      <h5><a href="/2020/11/12/exponential_family-ml07/">Machine Learning - 07 Exponential Family <span class="label label-default">machine-learning</span> </a></h5>
      </div>
      
      
        
</div>



<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://2ez4ai-github-io.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>