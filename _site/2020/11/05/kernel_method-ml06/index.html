<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-51FFV4BDWW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-51FFV4BDWW');
</script>
  
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    Machine Learning - 06 Kernel Method &middot; Jingye Wang
  </title>

  
  <link rel="canonical" href="http://localhost:4000/2020/11/05/kernel_method-ml06/">
  

  <link rel="stylesheet" href="http://localhost:4000/public/css/poole.css">
  <link rel="stylesheet" href="http://localhost:4000/public/css/syntax.css">
  <link rel="stylesheet" href="http://localhost:4000/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://localhost:4000/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="http://localhost:4000/public/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="http://localhost:4000/atom.xml">

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>"You can’t connect the dots looking forward; you can only connect them looking backward."</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="http://localhost:4000/">Home</a>

    

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="http://localhost:4000/about/">About</a>
        
      
    
      
    
      
        
      
    
    <span class="sidebar-nav-item">Currently v0.0.1</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2020. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">Jingye Wang</a>
            <small>A personal blog.</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">Machine Learning - 06 Kernel Method </h1>
  <span class="post-date">05 Nov 2020</span>
  <p><em>The notes are based on the <a href="https://github.com/shuhuai007/Machine-Learning-Session">session</a>. For the fundamental of linear algebra, one can always refer to <a href="http://math.mit.edu/~gs/linearalgebra/">Introduction to Linear Algebra</a> and <a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">The Matrix Cookbook</a> for more details. Many thanks to these great works.</em></p>

<h1 id="0-introduction">0. Introduction</h1>

<p>Kernel methods are a class of algorithms for pattern analysis. The name of kernel methods comes from the use of <em>kernel function</em>, which enable operations in a high-dimensional and implicit space. Specifically, by <a href="https://en.wikipedia.org/wiki/Cover%27s_theorem">Cover’s theorem</a>, given a set of training data that is not <em>linearly separable</em>, one can with high probability transform it into a training set that is linearly separable by projecting it into a higher-dimensional space via some <em>non-linear transformation</em>. With the help of kernel function, the operation, <em>i.e.,</em> inner product, it involves after transforming can be often computationally cheaper than the explicit computation. Such an approach is called the <em>kernel trick</em>. In this post, we will focus on the application of kernel method to SVM.</p>

<h2 id="1-kernel-method">1. Kernel method</h2>

<p>Define the data set as <script type="math/tex">\mathcal{D}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}, X=\{x_1,x_2,\dots,x_N\}</script> and <script type="math/tex">Y=\{y_1,y_2,\dots,y_N\}</script> where $x_i\in\mathbb{R}^{d\times 1}$ and <script type="math/tex">y_i\in\{-1,1\}</script>. We further assume that the data set is non-linearly separable. Kernel method supposes that there is a non-linear transformation $\phi(x):\mathbb{R}^{d\times 1}\to\mathbb{R}^{p\times 1},d&lt;p,$  such that <script type="math/tex">\mathcal{D}_p=\{(\phi(x_1),y_1),(\phi(x_2),y_2),\dots,(\phi(x_N),y_N)\}</script> are linearly separable. For such a linearly separable data set, recalling the problem we formulated in section 1.3 of <a href="https://19w6.github.io/2020/10/28/support_vector_machine-ml05/">SVM</a>, we have the duality problem</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\min_\lambda&\quad \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^N\left(\lambda_i\lambda_jy_iy_j\phi^T(x_i)\phi(x_j)\right)-\sum_{i=1}^N\lambda_i\\\text{s.t.}&\quad \lambda_i\ge0,i=1,2,\dots,N\\&\quad \sum_{i=1}^N\lambda_iy_i=0.\end{aligned} %]]></script>

<p>However, after transforming, the inner product $\phi(x_i)^T\phi(x_j)=\langle\phi(x_i),\phi(x_j)\rangle$ could be hard to obtain (consider the case that $\phi(\cdot)$ has infinite dimensions), which requires the aid of <em>kernel function</em>.</p>

<h2 id="2-kernel-function">2. Kernel function</h2>

<p>A kernel function is defined as $K:\mathbb{R}^{d\times 1}\times\mathbb{R}^{d\times 1}\to\mathbb{R}$. Specifically, for non-linear transformation $\phi(\cdot)\in\mathcal{H}\text{ (Hilbert space) }:\mathbb{R}^{d\times 1}\to\mathbb{R}^{p\times 1}$ and any $x_i,x_j\in\mathbb{R}^{d\times 1}$, we call $K(x_i,x_j)=\langle\phi(x_i),\phi(x_j)\rangle$ a kernel function. Such a kernel function is regarded <em>positive definite</em>, which satisfies</p>

<script type="math/tex; mode=display">K(x_i,x_j)=K(x_j,x_i),</script>

<p>and for <script type="math/tex">x_{1},x_{2},\dots,x_{N}\in\mathbb{R}^{d\times 1},</script></p>

<script type="math/tex; mode=display">\mathcal{K}=[K(x_{i},x_{j})]_{N\times N}\text{ is a positive semi-definite (PSD) matrix},</script>

<p>where <script type="math/tex">\mathcal{K}</script> is called <em>Gram matrix</em> of $K$ over set <script type="math/tex">\{x_{1},x_{2},\dots,x_{N}\}</script>. When the explicit expression of $\phi(\cdot)$ is hard to be determined, it quite often to show the positive definiteness of a kernel function via its corresponding Gram matrix.</p>

<p><strong>(Properties)</strong> We now show two <em>properties</em> of kernel functions.</p>

<ul>
  <li>Let $K$ be kernel function such that <script type="math/tex">K:\mathbb{R}^{d\times 1}\times\mathbb{R}^{d\times 1}\to\mathbb{R}</script>, then we define its Gram matrix $\mathcal{K}\in\mathbb{R}^{N\times N}$ over <script type="math/tex">\{x_1,x_2,\dots,x_N\}</script> where $x_i\in\mathbb{R}^{d\times 1}$. Considering the mapping function $\phi(\cdot):\mathbb{R}^{d\times 1}\to\mathbb{R}^{p\times 1}$, we have</li>
</ul>

<script type="math/tex; mode=display">K(x_i,x_j)=\langle\phi(x_i),\phi(x_j)\rangle, \phi(\cdot)\in\mathcal{H}\implies \begin{cases}K(x_i,x_j)=K(x_j,x_i)\\ \mathcal{K}\text{ is a PSD matrix}\end{cases}.</script>

<p><em>Proof</em>:</p>

<p>By the definition of $K(x_i,x_j)$, we have</p>

<script type="math/tex; mode=display">K(x_i,x_j)=\langle \phi(x_i),\phi(x_j)\rangle,\quad K(x_j,x_i)=\langle \phi(x_j),\phi(x_i)\rangle.</script>

<p>By the symmetry of inner product, we have <script type="math/tex">\langle \phi(x_i),\phi(x_j)\rangle=\langle \phi(x_j),\phi(x_i)\rangle</script>. It then follows that</p>

<script type="math/tex; mode=display">K(x_i,x_j)=K(x_j,x_i).</script>

<p>Therefore the Gramian matrix $\mathcal{K}=[K(x_{i},x_{j})]_{N\times N}$ is symmetric real matrix. Now we show that <script type="math/tex">\forall\alpha\in\mathbb{R}^{R\times 1}, \alpha^T\mathcal{K}\alpha\ge 0.</script> The notation is given by</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\alpha^T\mathcal{K}\alpha=(\alpha_1,\alpha_2,\dots,\alpha_N)\begin{bmatrix}K_{11}&K_{12}&\dots&K_{1N}\\K_{21}&K_{22}&\dots&K_{2N}\\\vdots&\vdots&\ddots&\vdots\\K_{N1}&K_{N2}&\dots&K_{NN}\end{bmatrix}\begin{pmatrix}\alpha_1\\\alpha_2\\\vdots\\\alpha_N\end{pmatrix}\end{aligned}, %]]></script>

<p>where $K_{ij}=K(x_{ri},x_{rj})$. We then have</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\alpha^T\mathcal{K}\alpha&=\sum_{i=1}^R\sum_{j=1}^R \alpha_i\alpha_jK_{ij}\\&=\sum_{i=1}^R\sum_{j=1}^R \alpha_i\alpha_j\phi^T(x_{ri})\phi(x_{rj})\\&=\sum_{i=1}^R \alpha_i\phi^T(x_{ri})\sum_{j=1}^R\alpha_j\phi(x_{rj})\\&=\left(\sum_{i=1}^R \alpha_i\phi(x_{ri})\right)^T\left(\sum_{j=1}^R\alpha_j\phi(x_{rj})\right)\\&=\left\langle\left(\sum_{i=1}^R \alpha_i\phi(x_{ri})\right), \left(\sum_{j=1}^R \alpha_i\phi(x_{rj})\right)\right\rangle\\&=\left\vert\left\vert\sum_{i=1}^R \alpha_i\phi(x_{ri})\right\vert\right\vert^2,\end{aligned} %]]></script>

<p>therefore, $\alpha^T\mathcal{K}\alpha\ge 0$ and $\mathcal{K}$ is a PSD matrix.$\tag*{$\blacksquare$}$</p>

<ul>
  <li>Let $\mathcal{K}\in\mathbb{R}^{d\times d}$ be a symmetric PSD matrix, then for <script type="math/tex">\{x_1,x_2,\dots,x_N\}</script> where $x_i\in\mathbb{R}^{d\times 1}$, we have kernel function $K(x_i,x_j)=x_i^T\mathcal{K}x_j$.</li>
</ul>

<p><em>Proof</em>:</p>

<p>Consider the <em>diagonalisation</em> of $\mathcal{K}=Q^T\Lambda Q$ by an orthogonal matrix $Q$, where $\Lambda$ is a diagnoal matrix containing the non-negative eigenvalues of $\mathcal{K}$. Let $\sqrt{\Lambda}$ be the diagonal matrix with the square roots of the eigenvalues and set $A=\sqrt{\Lambda}Q$.  Then for <script type="math/tex">\{x_1,x_2,\dots,x_N\}</script> where $x_i\in\mathbb{R}^{d\times 1}$, we have</p>

<script type="math/tex; mode=display">x_i^T\mathcal{K}x_j=x_i^TQ^T\Lambda Qx_j=x_i^TA^TA x_j=\langle A x_i,Ax_j\rangle.</script>

<p>Therefore we have kernel function $K(x_i,x_j)=x_i^T\mathcal{K}x_j=\langle Ax_i,Ax_j\rangle$ with linear transformation $\phi(\cdot)=A\cdot. \tag*{$\blacksquare$}$</p>

<h1 id="3-conclusion">3. Conclusion</h1>

<p>In this post, we introduced <em>kernel method</em> for classification problem. Given <em>Cover’s theorem</em>, we can project non-linear data into high-dimensional space and obtain linearly separable data. To simplify the computation incurred by the duality problem, we can leverage <em>kernel function</em> to avoid the computing labor.</p>

<p>This is definitely not a good introduction to kernel methods. For more details of kernel method, I would recommend <a href="https://people.eecs.berkeley.edu/~jordan/kernels/0521813972c03_p47-84.pdf">Kernel methods: an overview</a>.</p>

</div>


<div class="related">
  <h2>Related posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2020/12/06/inference-ml09/">
            Machine Learning - 09 Exact Inference of Graphical Models
            <small>06 Dec 2020</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2020/11/29/probabilistic_graphical_models-ml08/">
            Machine Learning - 08 Probabilistic Graphical Models
            <small>29 Nov 2020</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2020/11/12/exponential_family-ml07/">
            Machine Learning - 07 Exponential Family
            <small>12 Nov 2020</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>


<!-- Comments -->
<!--

-->
<!-- Comments Form -->
<!--
  <form method="POST" action="">
    <input name="options[redirect]" type="hidden" value="https://example.com">
    <input name="options[slug]" type="hidden" value="kernel_method-ml06">
      <label>*Name</label>
      <input name="fields[name]" type="text">
      <label>E-mail</label>
      <input name="fields[email]" type="email"><br>
      <label>Message:</label>
      <textarea style="width:100%" name="fields[message]" rows="12"></textarea>
      <button type="submit">Submit comment</button>
      <small>Comments will appear after moderation.</small>
  </form>
-->
      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>