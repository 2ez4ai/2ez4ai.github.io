<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-51FFV4BDWW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-51FFV4BDWW');
</script>
  
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    Machine Learning - 07 Exponential Family &middot; Jingye Wang
  </title>

  
  <link rel="canonical" href="/2020/11/12/exponential_family-ml07/">
  

  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>"You can’t connect the dots looking forward; you can only connect them looking backward."</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/">Home</a>

    

    
    
      
        
      
    
      
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/categories/">Archive</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/about/">About</a>
        
      
    
    <span class="sidebar-nav-item">Currently v0.1.2</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2021. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">Jingye Wang</a>
            <small>A personal blog.</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <style>
  .button {
    border: 2px solid;
    border-radius: 7.5px;
    color: white;
    padding: 2.5px 7.5px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    transition-duration: 0.1s;
    cursor: pointer;
  }

  .buttonC {
    background-color: white; 
    color: black; 
    border: 2px solid #2e3131;
  }

  .buttonC:hover {
    background-color: #2e3131;
    color: white;
  }
  a:hover{
    text-decoration: none;
  }
</style>
<div class="post">
  <h1 class="post-title">Machine Learning - 07 Exponential Family </h1>
  <span class="post-date">
    12 Nov 2020
    <p></p>
  <div class="post-categories">
      
      
      <a class="button buttonC" href="/categories/#machine-learning">machine-learning</a>&nbsp;
      
      
    </div>
  </span>
  <p><em>The notes are based on the <a href="https://github.com/shuhuai007/Machine-Learning-Session">session</a> and the <a href="https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter8.pdf">material</a>. For the fundamental of linear algebra, one can always refer to <a href="http://math.mit.edu/~gs/linearalgebra/">Introduction to Linear Algebra</a> and <a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">The Matrix Cookbook</a> for more details. Many thanks to these great works.</em></p>

<ul id="markdown-toc">
  <li><a href="#0-introduction" id="markdown-toc-0-introduction">0. Introduction</a></li>
  <li><a href="#1-exponential-family" id="markdown-toc-1-exponential-family">1. Exponential Family</a></li>
  <li><a href="#2-sufficient-statistic" id="markdown-toc-2-sufficient-statistic">2. Sufficient Statistic</a></li>
  <li><a href="#3-log-partition-function" id="markdown-toc-3-log-partition-function">3. Log-partition Function</a></li>
  <li><a href="#4-maximum-entropy" id="markdown-toc-4-maximum-entropy">4. Maximum Entropy</a></li>
  <li><a href="#5-gaussian-distribution" id="markdown-toc-5-gaussian-distribution">5. Gaussian Distribution</a></li>
  <li><a href="#6-conclusion" id="markdown-toc-6-conclusion">6. Conclusion</a></li>
</ul>
<h1 id="0-introduction">0. Introduction</h1>

<p>An exponential family is a family of distributions which share some properties in common. The real message of this note is the simplicity and elegance of the exponential family. Once the ideas are mastered, it is often easier to work within the general exponential family framework than with specific instances.</p>

<h1 id="1-exponential-family">1. Exponential Family</h1>

<p>Given one real-vector parameter \(\mathbf{\theta}=[\theta_1,\theta_2,\dots,\theta_d]^T\), we define an <em>exponential family</em> of probability distributions as those distributions whose density have the following general form:</p>

\[f_X(x\vert\eta)=h(x)\exp\left(\eta^T T(x)-A(\eta)\right).\]

<p><strong>Canonical parameter</strong>: $\eta$ is called <em>canonical</em>, or <em>natural parameter</em> (function), which can be viewed as a transformation of $\mathcal{\theta}$. The set of values of $\eta$ is always convex.</p>

<p><strong>Sufficient statistic</strong>: Given a data set sampled from \(f_X(x\vert\eta)\), the sufficient statistic $T(x)$ is a function of the data that holds all information the data set provides with regard to the unknown parameter $\mathbf{\theta}$.</p>

<p><strong>Log-partition function</strong>: $A(\eta)$ is the <em>log-partition function</em> to normalize \(f_X(x\vert \eta)\) to be a probability distribution,</p>

\[A(\eta)=\log\left(\int_{X}h(x)\exp(\eta^T T(x))\text{d}x\right).\]

<p>In the following sections, we will discuss them detailedly.</p>

<h1 id="2-sufficient-statistic">2. Sufficient Statistic</h1>

<p>Consider the problem of estimating the unknown parameters by <em>maximum likelihood estimation</em> (MLE) in exponential family cases. Specifically, for an <em>i.i.d.</em> data set \(\mathcal{D}=\{x_1,x_2,\dots,x_N\}\), we have the log likelihood</p>

\[\mathcal{L}(\eta\vert\mathcal{D})=\log\left(\prod_{i=1}^Nh(x_i)\right)+\eta^T\left(\sum_{i=1}^NT(x_i)\right)-NA(\eta).\]

<p>By <em>MLE</em>, we have the estimation $\hat\eta$ when its gradient with respect to $\eta$ is zero:</p>

\[\mathcal{L}’(\eta\vert\mathcal{D})=\sum_{i=1}^NT(x_i)-NA’(\eta)=0.\]

<p>Solving the equation, we have</p>

\[A’(\hat\eta)=\frac{1}{N}\sum_{i=1}^NT(x_i),\]

<p>which is the general formula of MLE for the parameters in the exponential family. Further, notice that our formula involves the data only via the sufficient statistic $T(x_i)$. This gives the operational meaning to <em>sufficiency</em>—for the purpose of estimating parameters we retain only the sufficient statistic.</p>

<h1 id="3-log-partition-function">3. Log-partition Function</h1>

<p>As we mentioned in section 1, $A(\eta)$ can be viewed as a normalization factor. In fact, $A(\eta)$ is not a degree of freedom in the specification of an exponential family density; it is determined once $T(x)$ and $h(x)$ are determined. The relation between $A(\eta)$ and $T(x)$ can be further characterized by</p>

\[\begin{aligned}A’(\eta)&amp;=\frac{\text{d}\log\left(\int_{X}h(x)\exp(\eta^T T(x))\text{d}x\right)}{\text{d}\eta}\\&amp;=\frac{\int_{X}h(x)\exp(\eta^T T(x))\cdot  T(x)\text{d}x}{\int_{X}h(x)\exp(\eta^T T(x))\text{d}x}\\&amp;=\frac{\int_{X}h(x)\exp(\eta^T T(x))\cdot T(x)\text{d}x}{\exp({A(\mathbf{\theta})})}\\&amp;=\int_{X}\underbrace{h(x)\exp(\eta^T T(x)-A(\eta))}_{f_X(x\vert \eta)}\cdot T(x)\text{d}x\\&amp;=\mathbb{E}_{f_X(x\vert\eta)}[T(x)].\end{aligned}\]

<p>Further, we have</p>

\[\begin{aligned}A’’(\eta)&amp;=\int_{X}f_X(x\vert \eta)\cdot(T(x)-A’(\eta)) T(x)\text{d}x\\&amp;=\int_{X}f_X(x\vert \eta)\cdot(T(x))^2\text{d}x-A’(\eta)\int_{X}f_X(x\vert \mathbf{\eta})\cdot T(x)\text{d}x\\&amp;=\mathbb{E}_{f_X(x\vert\eta)}[(T(x))^2]-\left(\mathbb{E}_{f_X(x\vert\eta)}[T(x)]\right)^2\\&amp;=var[T(x)],\end{aligned}\]

<p>which also shows that $A(\eta)$ is convex as $var[T(x)]\ge 0$.</p>

<h1 id="4-maximum-entropy">4. Maximum Entropy</h1>

<p>The entropy of $P$ with distribution $p(x)$ supported on $X$ is</p>

\[H(P)=\mathbb{E}_{P}[-\log p(x)].\]

<p>The <em>maximum entropy</em> principle is that: given some constraints (prior information) about the distribution $P$, we consider all probability distributions satisfying said constraints such that the constraints are being utilized as <em>objective</em> as possible, <em>i.e.,</em> be as uncertain as possible.</p>

<p>For example, consider the case where the very constraint is $\sum_Xp(x)=1$, which formulates</p>

\[\begin{aligned}\max&amp;\quad H(P)\\\text{s.t.}&amp;\quad \sum_{X}p(x)=1.\end{aligned}\]

<p>By the definition we have</p>

\[H(P)=-\sum_{i=1}^{\vert X\vert}p(x_i)\log p(x_i).\]

<p>Then the <em>Lagrangian</em> for the optimization problem is</p>

\[L(P,\lambda)=\sum_{i=1}^{\vert X\vert}p(x_i)\log p(x_i)+\lambda\left(1-\sum_{i=1}^{\vert X\vert}p(x_i)\right).\]

<p>Setting the first derivation of the Lagrangian to be zero yields</p>

\[\frac{\partial L}{\partial p(x_i)}=0\implies \hat{p}(x_i)=\exp(\lambda-1),\]

<p>which gives that</p>

\[\hat{p}(x_1)=\hat{p}(x_2)=\dots=\hat{p}(x_{\vert X\vert})=\frac{1}{\vert X\vert},\]

<p><em>i.e.,</em> the distribution with maximum entropy is <em>uniform distribution</em>.</p>

<p>We now consider a general case where $p(x)$ is continuous with a general constraint $\mathbb{E}_P[\Phi(x)]=\alpha$, where $\Phi(x)=[\phi_1(x),\phi_2(x),\dots,\phi_d(x)]\in\mathbb{R}^d$ and $\alpha=[\alpha_1,\alpha_2,\dots,\alpha_d]\in\mathbb{R}^d$, which formulates</p>

\[\begin{aligned}\max&amp;\quad H(P)\\\text{s.t.}&amp;\quad \mathbb{E}_P[\Phi(x)]=\alpha\\\implies\min&amp;\quad \int_Xp(x)\log p(x)\text{d}x\\\text{s.t.}&amp;\quad \int_X p(x)\phi_i(x)\text{d}x=\alpha_i,\ i=1,2,\dots, d,\\&amp;\quad \int_X p(x)\text{d}x=1.\end{aligned}\]

<p>Similarly, we obtain the Lagrangian as</p>

\[L(P,\theta,\lambda)=\int_X p(x)\log p(x)\text{d}x+\sum_{i=1}^d\theta_i\left(\alpha_i-\int_X p(x)\phi_i(x)\text{d}x\right)+\lambda\left(\int_X p(x)\text{d}x-1\right).\]

<p>By treating the density $P=[p(x)]_{x\in X}$ as a finite vector such that $\int_X p(x)\text{d}x$ is similar to $\sum_X p(x)$, we have</p>

\[\begin{aligned}\frac{\partial L}{\partial p(x)}&amp;=\frac{\partial }{\partial p(x)}\left(\sum_X p(x)\log p(x)-\sum_{i=1}^d\theta_i\sum_X p(x)\phi_i(x)+\lambda\sum_X p(x)\right)\\&amp;=1+\log p(x)-\sum_{i=1}^d\theta_i\phi_i(x)+\lambda\\&amp;=1+\log p(x)-\theta^T\Phi(x)+\lambda.\end{aligned}\]

<p>Setting the derivation to be zero for all $x$, we have</p>

\[p(x)=\exp\left\{\theta^T\Phi(x)-(\lambda+1)\right\},\]

<p>which is in the exponential family form with</p>

\[\begin{aligned}\eta&amp;=\theta,\\T(x)&amp;=\Phi(x),\\A(\eta)&amp;=\lambda+1,\\h(x)&amp;=1.\end{aligned}\]

<h1 id="5-gaussian-distribution">5. Gaussian Distribution</h1>

<p>In this section, we consider an example, Gaussian distribution, which is of the exponential family and exemplifies the properties we mentioned above.</p>

<p>We first rewritten the PDF of one-dimension Gaussian distribution to show it is in the exponential family .</p>

<p><em>Proof:</em> Given unknown parameter \(\mathbf{\theta}=[\mu,\sigma^2]\), the Gaussian density can be written as follows,</p>

\[\begin{aligned}f_X(x\vert \mathbf{\theta})&amp;=\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{-\frac{1}{2\sigma^2}(x-\mu)^2\right\}\\&amp;=\frac{1}{\sqrt{2\pi}}\exp\left\{\frac{\mu}{\sigma^2}x-\frac{1}{2\sigma^2}x^2-\frac{1}{2\sigma^2}\mu^2-\log\sigma\right\}\\&amp;=\frac{1}{\sqrt{2\pi}}\exp\left\{\begin{bmatrix}\frac{\mu}{\sigma^2}&amp;-\frac{1}{2}\sigma^2\end{bmatrix}\begin{bmatrix}x\\x^2\end{bmatrix}-\left(\frac{\mu^2}{2\sigma^2}+\log\sigma\right)\right\},\end{aligned}\]

<p>which is in the exponential family form with</p>

\[\begin{aligned}\eta&amp;=\begin{bmatrix}\frac{\mu}{\sigma^2}&amp;-\frac{1}{2\sigma^2}\end{bmatrix}^T,\\T(x)&amp;=\begin{bmatrix}x&amp;x^2\end{bmatrix}^T,\\A(\eta)&amp;=\frac{\mu^2}{2\sigma^2}+\log\sigma=-\frac{\eta_1^2}{4\eta_2}-\frac{1}{2}\log(-2\eta_2),\\h(x)&amp;=\frac{1}{\sqrt{2\pi}}.\end{aligned}\]

\[\tag*{$\blacksquare$}\]

<p>Then we verify the relation between the sufficient statistic and MLE method.</p>

<p><em>Proof:</em> Given a data set \(\mathcal{D}=\{x_1,x_2,\dots,x_N\}\), as we mentioned in section 2, we can derive the parameters via the sufficient statistic as follows,</p>

\[\begin{cases}A’(\eta)=\frac{1}{N}\sum_{i=1}^NT(x)\implies\begin{cases}A’(\hat\eta_1)=\frac{1}{N}\sum_{i=1}^N x_i\\A’(\hat\eta_2)=\frac{1}{N}\sum_{i=1}^Nx_i^2\end{cases}\\A(\eta)=-\frac{\eta_1^2}{4\eta_2}-\frac{1}{2}\log(-2\eta_2)\implies\begin{cases}A’(\hat\eta_1)=-\frac{\eta_1}{2\eta_2}=\hat\mu\\A’(\hat\eta_2)=\frac{\eta_1^2}{4\eta_2^2}-\frac{1}{2\eta_2}=\hat\sigma^2+\hat\mu^2\end{cases}\end{cases}\]

<p>Solving the equations we have</p>

\[\hat\mu=\frac{1}{N}\sum_{i=1}^Nx_i,\quad \hat\sigma=\frac{1}{N}\sum_{i=1}^Nx_i^2-\hat\mu^2,\]

<p>which is consistent with the result in the <a href="https://2ez4ai.github.io/2020/09/28/intro-ml01/">post</a>. $\tag*{$\blacksquare$}$</p>

<p>Now we show that</p>

\[A’’(\hat\eta)=var[T(x)].\]

<p><em>Proof</em>: Firstly, we have</p>

\[\begin{cases}A’’(\hat\eta_1)=-\frac{1}{2\eta_2}=\sigma^2\\A’’(\hat\eta_2)=-\frac{\eta_1^2}{2\eta_2^3}+\frac{1}{2\eta_2^2}=4\sigma^2\mu^2+2\sigma^4\end{cases}\]

<p>For \(T(x)=\begin{bmatrix}x&amp;x^2\end{bmatrix}^T\), we have</p>

\[var[x]=\sigma^2, \text{ as }x\sim\mathcal{N}(\mu,\sigma^2),\]

<p>and</p>

\[var[x^2]=\mathbb{E}[x^4]-\left(\mathbb{E}[x^2]\right)^2.\]

<p>For $\mathbb{E}[x^2]$, it follows that</p>

\[\mathbb{E}[x^2]=var[x]+(\mathbb{E}[x])^2=\sigma^2+\mu^2.\]

<p>For $\mathbb{E}[x^4]$, to compute it we leverage <em>moment generating functions</em> which follows that</p>

\[M_X(t)=e^{\mu t+\frac{1}{2}\sigma^2t^2},\quad \mathbb{E}[x^4]=M^{(4)}_X(0).\]

<p>After a laborious computing, we have</p>

\[\begin{aligned}var[x^2]&amp;=\mathbb{E}[x^4]-\left(\mathbb{E}[x^2]\right)^2\\&amp;=3\sigma^4+6\sigma^2\mu^2+\mu^4-\sigma^4-2\sigma^2-\mu^4\\&amp;=4\sigma^2\mu^2+2\sigma^4.\end{aligned}\]

<p>Therefore, we have</p>

\[A’’(\hat\eta)=\begin{bmatrix}var[x]\\var[x^2]\end{bmatrix}.\tag*{$\blacksquare$}\]

<p>Finally, we show that $X\sim\mathcal{N}(\mu,\sigma^2)$ is the distribution that maximizes the entropy over all distributions $P$ satisfying</p>

\[\mathbb{E}_P\left[\left(\frac{X-\mu}{\sigma}\right)^2\right]=1.\]

<p><em>Proof:</em> Consider the expression we formulated in section 4,</p>

\[p(x)=\exp\left\{\theta^T\Phi(x)-(\lambda+1)\right\},\]

<p>which maximizes the entropy while satisfying $\mathbb{E}_P[\Phi(x)]=\alpha$. Now letting</p>

\[\begin{aligned}\alpha&amp;=1,\\\Phi(x)&amp;=\frac{(x-\mu)^2}{\sigma^2},\\\theta&amp;=-\frac{1}{2},\\\exp\{-\lambda-1\}&amp;=\frac{1}{\sqrt{2\pi}\sigma}.\end{aligned}\]

<p>Therefore we have</p>

\[p(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{-\frac{1}{2\sigma^2}(x-\mu)^2\right\}.\tag*{$\blacksquare$}\]

<h1 id="6-conclusion">6. Conclusion</h1>

<p>In this post, we briefly introduced the basic form of the exponential family. Then we discussed its properties from three perspectives: sufficient statistic, log-partition function and maximum entropy. Moreover, with one-dimension Gaussian distribution, we exemplified the properties.</p>


</div>

 <!--https://blog.webjeda.com/jekyll-related-posts/-->
<div class="related">
  <h2>Related posts</h2>
  
  
  

  

    
    

    

    

  

    
    

    

    

  

    
    

    

    
      <div>
      <h5><a href="/2020/12/05/inference-ml09/">Machine Learning - 09 Exact Inference of Graphical Models <span class="label label-default">machine-learning</span> </a></h5>
      </div>
      
      
    

  

    
    

    

    
      <div>
      <h5><a href="/2020/11/29/probabilistic_graphical_models-ml08/">Machine Learning - 08 Probabilistic Graphical Models <span class="label label-default">machine-learning</span> </a></h5>
      </div>
      
      
    

  

    
    

    

    

  

    
    

    

    
      <div>
      <h5><a href="/2020/11/05/kernel_method-ml06/">Machine Learning - 06 Kernel Method <span class="label label-default">machine-learning</span> </a></h5>
      </div>
      
      
        
</div>



<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://2ez4ai-github-io.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>