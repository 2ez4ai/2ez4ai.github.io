<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-51FFV4BDWW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-51FFV4BDWW');
</script>
  
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    Machine Learning - 01 Introduction &middot; Jingye Wang
  </title>

  
  <link rel="canonical" href="http://localhost:4000/2020/09/28/intro-ml01/">
  

  <link rel="stylesheet" href="http://localhost:4000/public/css/poole.css">
  <link rel="stylesheet" href="http://localhost:4000/public/css/syntax.css">
  <link rel="stylesheet" href="http://localhost:4000/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://localhost:4000/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="http://localhost:4000/public/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="http://localhost:4000/atom.xml">

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>"You can’t connect the dots looking forward; you can only connect them looking backward."</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="http://localhost:4000/">Home</a>

    

    
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="http://localhost:4000/about/">About</a>
        
      
    
      
    
      
        
      
    
    <span class="sidebar-nav-item">Currently v0.0.1</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2020. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">Jingye Wang</a>
            <small>A personal blog.</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">Machine Learning - 01 Introduction </h1>
  <span class="post-date">28 Sep 2020</span>
  <p><em>The notes are based on the <a href="https://github.com/shuhuai007/Machine-Learning-Session">session</a>. Many thanks to the great work.</em></p>

<h1 id="0-motivation">0. Motivation</h1>

<p>After setting up this blog, I actually had no idea about what should I post. I am definitely not good at writing, but still I enjoy it a lot. As I am weak in machine learning and mathematics, for writing and learning, a ‘<em>translation</em>’ work of the <a href="https://github.com/shuhuai007/Machine-Learning-Session">session</a> may be a good start. So here we are!</p>

<h1 id="1-frequentist-vs-bayesian">1. Frequentist vs Bayesian</h1>

<p>Many machine learning methods can be illustrated in a <em>probabilistic</em> way. We now introduce two mainstream views of interpreting probability: <strong>frequentist</strong> and <strong>bayesian</strong>.</p>

<p>Firstly let us consider a simple example. Suppose we have a data $X=(x_1,x_2,\ldots,x_N)^T$ where $x_i$ are i.i.d. samples of the random variable $x$ and is of $d$ dimensions. Also, we define $\theta$ be the parameter so that $x\sim P(x|\theta)$, and we call $P(X|\theta)$ the <em>likelihood</em>.</p>

<p><strong>Frequentists</strong>: $\theta$ is an unknown <em>constant</em>. What they care is how to use the data to infer the value of $\theta$. The best known approach for that is <strong>maximum likelihood estimation</strong> (MLE):</p>

<script type="math/tex; mode=display">\hat{\theta}_\text{MLE}=\arg\max_{\theta}P(X|\theta).</script>

<blockquote>
  <p>Many traditional machine learning methods are based on this view. They construct a probabilistic model, then determine a loss function and minimize it by methods like gradient descent.</p>
</blockquote>

<p><strong>Bayesians</strong>: $\theta$ is random variable, following distribution $P(\theta)$, which is called <em>prior distribution</em>, and $P(\theta|X)$ is called <em>posterior distribution</em>. Then by <em>Bayes’s theorem</em>, we have <strong>maximum a posteriori</strong> (MAP):</p>

<script type="math/tex; mode=display">\hat{\theta}_\text{MAP}=\arg\max_{\theta}P(\theta|X).</script>

<blockquote>
  <p>In Bayesian inference, the posterior distribution, rather than the maximum, is the key. However, to determine the distribution sometimes is really hard as it may incur complex calculus:</p>

  <script type="math/tex; mode=display">P(\theta|X)=\frac{P(X|\theta)\cdot P(\theta)}{\int_{\theta}P(X|\theta)\cdot P(\theta)\text{d}\theta}</script>

  <p>Many powerful and elegant approaches were proposed for this. For examples, MCMC, probabilistic graphical model, <em>etc</em>.</p>
</blockquote>

<p>I’d like to cite <a href="https://www.probabilisticworld.com/frequentist-bayesian-approaches-inferential-statistics/">this</a> to summary the differences.</p>

<p><em>“In short, according to the frequentist definition of probability, only repeatable random events (like the result of flipping a coin) have probabilities. These probabilities are equal to the long-term frequency of occurrence of the events in question. Frequentists don’t attach probabilities to hypotheses or to any fixed but unknown values in general.”</em></p>

<p><em>“In contrast, Bayesians view probabilities as a more general concept. As a Bayesian, you can use probabilities to represent the uncertainty in any event or hypothesis. Here, it’s perfectly acceptable to assign probabilities to non-repeatable events.”</em></p>

<h1 id="2maximum-likelihood-estimation-on-gaussian-distribution">2.Maximum Likelihood Estimation on Gaussian Distribution</h1>

<p>In this section, we will give an example of MLE on Gaussian distribution. Just like what we defined in section 1, there is a data $X=(x_1,x_2,\ldots,x_N)^T$ where $x_i$ are i.i.d. samples of the random variable $x$ and is of $d=1$  dimension. Thus we have $X\in\mathbb{R}^{N\times 1}$. We further attach a specific distribution to $x$ as $x\sim\mathcal{N}(\mu,\sigma^2)$. The unknown parameter then becomes $\theta=(\mu,\sigma)$. Now we use MLE method to estimate the parameter:</p>

<script type="math/tex; mode=display">\hat{\theta}_\text{MLE}=\arg\max_\theta P(X|\theta)=\arg\max_\theta \log P(X|\theta).</script>

<blockquote>
  <p>Given $x\sim\mathcal{N}(\mu,\sigma^2)$, the PDF of $x$ is</p>

  <script type="math/tex; mode=display">p(x|\mu,\sigma)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}.</script>
</blockquote>

<p>As $x_i,i=1,\ldots,N$ are i.i.d. variables, it follows that</p>

<script type="math/tex; mode=display">P(X|\theta)=\prod_{i=1}^NP(x_i|\theta).</script>

<p>Hence,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\arg\max_\theta\log P(X|\theta)&=\arg\max_\theta\log \prod_{i=1}^N P(x_i|\theta)\\&=\arg\max_\theta\sum_{i=1}^N\log P(x_i|\theta)\\&=\arg\max_\theta\sum_{i=1}^N\log\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\\&=\arg\max_\theta\sum_{i=1}^N\left[\log \frac{1}{\sqrt{2\pi}}+\log\frac{1}{\sigma}-\frac{(x_i-\mu)^2}{2\sigma^2}\right]\\&=\arg\max_\theta\sum_{i=1}^N\left[\log\frac{1}{\sigma}-\frac{(x_i-\mu)^2}{2\sigma^2}\right]\end{aligned}, %]]></script>

<p>which is equivalent to a numerical optimization problem. Denote the term $\sum_{i=1}^N\left[\log\frac{1}{\sigma}-\frac{(x_i-\mu)^2}{2\sigma^2}\right]$ as $\mathcal{L}$. The maximum occurs when</p>

<script type="math/tex; mode=display">\frac{\partial\mathcal{L}}{\partial\mu}=0,\quad \frac{\partial\mathcal{L}}{\partial\sigma}=0.</script>

<p>Solving the equations we have</p>

<script type="math/tex; mode=display">\mu_\text{MLE}=\frac{1}{N}\sum_{i=1}^N x_i,\quad \sigma^2_\text{MLE}=\frac{1}{N}\sum_{i=1}^N(x_i-\mu_\text{MLE})^2.</script>

<blockquote>
  <p>$\mu_\text{MLE}$ is an <em>unbiased estimation</em> as $\mathbb{E}[\mu_\text{MLE}]=\mu$, while $\sigma_\text{MLE}^2$ is a <em>biased estimation</em> since $\mathbb{E}[\sigma_\text{MLE}^2]=\frac{N-1}{N}\sigma^2$. The bias is incurred by the exploitation on samples rather than the true distribution. Intuitively, the variance of the samples will never be larger than the true distribution since they are totally generated from that!</p>
</blockquote>

<h1 id="3-gaussian-distribution">3. Gaussian Distribution</h1>

<p>Many advanced machine learning techniques, like <em>linear Gaussian model</em>, <em>Kalman filter</em>, <em>P-PCA</em>, <em>etc</em>, involve Gaussian distribution. Therefore it is quite practical to get familiar with Gaussian distribution. In this section, we are trying to explain why the PDF of bivariate Gaussian distribution is shaped by numerous ellipses.</p>

<p>Suppose we have a random variable $x\sim \mathcal{N}(\mu,\Sigma)$ of $d$ dimensions. Specifically, the definition follows that</p>

<script type="math/tex; mode=display">P(x|\mu,\Sigma)=\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)},</script>

<p>and</p>

<script type="math/tex; mode=display">% <![CDATA[
x=\begin{pmatrix}x_{1}\\x_{2}\\\vdots\\x_{d}\end{pmatrix}_{d\times1},\quad \mu=\begin{pmatrix}\mu_1\\\mu_2\\\vdots\\\mu_{d}\end{pmatrix}_{d\times1},\quad\Sigma=\begin{pmatrix}\sigma_{11}&\sigma_{12}&\cdots&\sigma_{1d}\\\sigma_{21}&\sigma_{22}&\cdots&\sigma_{2d}\\\vdots&\vdots&\ddots&\vdots\\\sigma_{d1}&\sigma_{d2}&\cdots&\sigma_{dd}\end{pmatrix}_{d\times d}. %]]></script>

<p>As it involves matrix, a simplification will be a wise choice. For simplicity,  we use $\Delta(x)$ to represent $(x-\mu)^T\Sigma^{-1}(x-\mu)$ and suppose the covariance matrix $\Sigma$ to be positive definite, which is reasonable. By the eigenvalue decomposition and the property of positive definite matrix, $\Sigma$ can be decomposed as $\Sigma=U\Lambda U^T$, where $U=(u_1,u_2,\ldots,u_d)$ is an orthogonal matrix whose columns are the eigenvectors of $\Sigma$, and $\Lambda=\text{diag}(\lambda_1,\lambda_2,\ldots,\lambda_d)$ is a diagonal matrix whose entries are the eigenvalues of $\Sigma$. Then we have</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\Sigma^{-1}&=(U\Lambda U^T)^{-1}\\&=(U^T)^{-1}\Lambda^{-1}U^{-1}\\&=U\Lambda^{-1}U^T\\&=(u_1,u_2,\ldots,u_d)\begin{pmatrix}\frac{1}{\lambda_1}&0&\cdots&0\\0&\frac{1}{\lambda_2}&\cdots&0\\\vdots&\vdots&\ddots&0\\0&0&\cdots&\frac{1}{\lambda_d}\end{pmatrix}\begin{pmatrix}u_1\\u_2\\\vdots\\u_d\end{pmatrix}\\&=\begin{pmatrix}\frac{u_1}{\lambda_1},\frac{u_2}{\lambda_2},\ldots,\frac{u_d}{\lambda_d}\end{pmatrix}\begin{pmatrix}u_1\\u_2\\\vdots\\u_d\end{pmatrix}\\&=\sum_{i=1}^d\frac{u_iu_i^T}{\lambda_i}\end{aligned}. %]]></script>

<p>Plugging it in $\Delta(x)$, we arrive at</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\Delta(x)&=(x-\mu)^T\left(\sum_{i=1}^d\frac{u_iu_i^T}{\lambda_i}\right)(x-\mu)\\&=\sum_{i=1}^d(x-\mu)^Tu_i\frac{1}{\lambda_i}u_i^T(x-\mu)\end{aligned}. %]]></script>

<p>Before moving on, we take a pause to be acquainted with those notations. Specifically, $(x-\mu)\in\mathbb{R}^{d\times 1}$ as $x, \mu\in\mathbb{R}^{d\times 1}$. $u_i$ is the eigenvector of $\Sigma$ so $u_i\in\mathbb{R}^{d\times 1}$, while $\lambda_i$ is the eigenvalue of $\Sigma$ so $\lambda_i\in\mathbb{R}$. Therefore $(x-\mu)^Tu_i\in\mathbb{R}$, $u_i^T(x-\mu)\in\mathbb{R}$ and $\Delta(x)\in\mathbb{R}$. It should not be surprising. Recall that the PDF</p>

<script type="math/tex; mode=display">P(x|\mu,\Sigma)=\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}\Delta(x)}</script>

<p>represents a probability, which is definitely a real number. Now we introduce $Y=(y_1,y_2,\ldots,y_d)^T$ where $y_i=(x-\mu)^Tu_i$. Hence</p>

<script type="math/tex; mode=display">\Delta(x)=\sum_{i=1}^dy_i\frac{1}{\lambda_i}y_i^T=\sum_{i=1}^d\frac{y_i^2}{\lambda_i}.</script>

<p><strong>The PDF of $x$ is rewritten as</strong></p>

<script type="math/tex; mode=display">P(x|\mu,\Sigma)=\alpha e^{-\frac{1}{2}\sum_{i=1}^d y_i^2/\lambda_i},</script>

<p>where $\alpha=\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}$ is a constant. Now suppose we are trying to determine where $P(x|\mu,\Sigma)=\beta$. To make it intuitive, we consider the case $d=2$, then it follows that</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{alignat*}{3}&& \alpha e^{-\frac{1}{2}\left(\frac{y_1^2}{\lambda_1}+\frac{y_2^2}{\lambda_2}\right)}&=\beta\\\Rightarrow&&\frac{y_1^2}{\lambda_1}+\frac{y_2^2}{\lambda_2}&=2(\ln\alpha-\ln\beta)\\\Rightarrow&&\frac{y_1^2}{a^2}+\frac{y_2^2}{b^2}&=1\end{alignat*}, %]]></script>

<p>where $a^2=2\lambda_1(\ln\alpha-\ln\beta)$ and $b^2=2\lambda_2(\ln\alpha-\ln\beta)$, and we finally get <strong>a standard equation of ellipse</strong> with respect to $y_1$ and $y_2$! Actually, the transformation $y_i=(x-\mu)^Tu_i$ is the projection of $x$ on $u_i$. Thus in the coordinate system represented by $x_1$ and $x_2$, the ellipse will be changed linearly hence the ellipse will not be so standard. For different $\beta$, we have ellipses with different major and minor axes, and that is why the graph of bivariate Gaussian distribution looks like it is composed of numerous concentration ellipses.</p>

<p><em>PS.</em> The above discussion leaves much correctness proof to be done.</p>

<h1 id="4-calculations-on-gaussian">4. Calculations on Gaussian</h1>

<p>In this section, all the random variables are Gaussian and we will not touch the complex PDF. Rather, we mainly focus on the statistics, the expectation and the variance, of Gaussian distribution.</p>

<h2 id="41-calculations-on-the-union-distribution">4.1. Calculations on the union distribution</h2>

<p>Suppose $x\sim\mathcal{N}(\mu, \Sigma)$ is the union distribution of $x_a$ and $x_b$, <em>i.e.</em> $x=(x_a,x_b)^T$. Then the definition follows that</p>

<script type="math/tex; mode=display">% <![CDATA[
x=\begin{pmatrix}x_a\\x_b\end{pmatrix}_{d\times1},\quad \mu=\begin{pmatrix}\mu_a\\\mu_b\end{pmatrix}_{d\times1},\quad\Sigma=\begin{pmatrix}\Sigma_{aa}&\Sigma_{ab}\\\Sigma_{ba}&\Sigma_{bb}\end{pmatrix}_{d\times d}, %]]></script>

<p>where</p>

<script type="math/tex; mode=display">x_a,\mu_a\in\mathbb{R}^{m\times 1},x_b,\mu_b\in\mathbb{R}^{n\times 1}, m+n=d,</script>

<script type="math/tex; mode=display">\Sigma_{aa}\in\mathbb{R}^{m\times m}, \Sigma_{ab},\Sigma_{ba}^T\in\mathbb{R}^{m\times n}, \Sigma_{bb}\in\mathbb{R}^{n\times n}.</script>

<p>The problem in this section is how to derive $P(x_a)$ and $P(x_b|x_a)$, or symmetrically, $P(x_b)$ and $P(x_a|x_b)$ given the union distribution.  The derivation of $P(x_a)$ is quite straightforward:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\mathbb{E}[x_a]&=\mathbb{E}\left[\begin{pmatrix}\mathbf{I}_{m\times m}&\mathbf{0}_{m\times n}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}\right]\\&=\begin{pmatrix}\mathbf{I}_{m\times m}&\mathbf{0}_{m\times n}\end{pmatrix}\mathbb{E}[x]\\&=\begin{pmatrix}\mathbf{I}_{m\times m}&\mathbf{0}_{m\times n}\end{pmatrix}\begin{pmatrix}\mu_a\\\mu_b\end{pmatrix}\\&=\mu_a\end{aligned}, %]]></script>

<p>and</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}var[x_a]&=var\left[\begin{pmatrix}\mathbf{I}_{m\times m}&\mathbf{0}_{m\times n}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}\right]\\&=\begin{pmatrix}\mathbf{I}_{m\times m}&\mathbf{0}_{m\times n}\end{pmatrix}var[x]\begin{pmatrix}\mathbf{I}_{m\times m}\\\mathbf{0}_{n\times m}\end{pmatrix}\\&=\begin{pmatrix}\mathbf{I}_{m\times m}&\mathbf{0}_{m\times n}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&\Sigma_{ab}\\\Sigma_{ba}&\Sigma_{bb}\end{pmatrix}\begin{pmatrix}\mathbf{I}_{m\times m}\\\mathbf{0}_{n\times m}\end{pmatrix}\\&=\Sigma_{aa}\end{aligned}. %]]></script>

<p>Thus, we have $x_a\sim\mathcal{N}(\mu_a,\Sigma_{aa})$, and similarly, $x_b\sim\mathcal{N}(\mu_b,\Sigma_{bb})$. For the conditional probabilities $P(x_b|x_a)$, we first define variables</p>

<script type="math/tex; mode=display">x_{b\cdot a}=x_b-\Sigma_{ba}\Sigma^{-1}_{aa}x_a,</script>

<script type="math/tex; mode=display">\mu_{b\cdot a}=\mu_b-\Sigma_{ba}\Sigma^{-1}_{aa}\mu_a,</script>

<p>and</p>

<script type="math/tex; mode=display">\Sigma_{bb\cdot a}=\Sigma_{bb}-\Sigma_{ba}\Sigma^{-1}_{aa}\Sigma_{ab}.</script>

<p>The term $\Sigma_{bb\cdot a}$ is also the <em>Schur complement</em> of $\Sigma_{aa}$ of the matrix $\Sigma$. Then it follows that</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\mathbb{E}[x_{b\cdot a}]&=\mathbb{E}\left[\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&\mathbf{I}_{n\times n}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}\right]\\&=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&\mathbf{I}_{n\times n}\end{pmatrix}\mathbb{E}[x]\\&=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&\mathbf{I}_{n\times n}\end{pmatrix}\begin{pmatrix}\mu_a\\\mu_b\end{pmatrix}\\&=\mu_{b\cdot a}\end{aligned}, %]]></script>

<p>and</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}var[x_{b\cdot a}]&=var\left[\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&\mathbf{I}_{n\times n}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}\right]\\&=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&\mathbf{I}_{n\times n}\end{pmatrix}var[x]\begin{pmatrix}-\Sigma_{aa}^{-1}\Sigma_{ba}^T\\\mathbf{I}_{n\times n}\end{pmatrix}\\&=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&\mathbf{I}_{n\times n}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&\Sigma_{ab}\\\Sigma_{ba}&\Sigma_{bb}\end{pmatrix}\begin{pmatrix}-\Sigma_{aa}^{-1}\Sigma_{ba}^T\\\mathbf{I}_{n\times n}\end{pmatrix}\\&=\Sigma_{bb\cdot a}\end{aligned}. %]]></script>

<p>Hence we have $x_{b\cdot a}\sim\mathcal{N}(\mu_{b\cdot a},\Sigma_{bb\cdot a})$. Before showing the relation between $x_{b\cdot a}$ and $x_b|x_a$, we introduce a helpful theorem.</p>

<p><strong>Theorem</strong>:<em>If</em> $x\sim\mathcal{N}(\mu,\Sigma)$, <em>then</em> $Mx\perp Nx\iff M\Sigma N=\mathbf{0}.$</p>

<p><em>Proof:</em> According to the property of Gaussian, $Mx\sim\mathcal{N}(M\mu,M\Sigma M^T)$ and $Nx\sim\mathcal{N}(N\mu,N\Sigma N^T)$. Further,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}Cov(Mx,Nx)&=\mathbb{E}[(Mx-M\mu)(Nx-N\mu)^T]\\&=M\mathbb{E}[(x-\mu)(x-\mu)^T]N\\&=M\Sigma N^T.\end{aligned} %]]></script>

<p>For Gaussian distribution, uncorrelation implies independence. Therefore $M\Sigma N^T=\mathbf{0}\iff Mx\perp Nx.\tag*{$\blacksquare$}$</p>

<p>Back to our case, we have</p>

<script type="math/tex; mode=display">% <![CDATA[
x_{b\cdot a}=\underbrace{\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&\mathbf{I}\end{pmatrix}}_M\underbrace{\begin{pmatrix}x_a\\x_b\end{pmatrix}}_x,\qquad x_a=\underbrace{\begin{pmatrix}\mathbf{I}&\mathbf{0}\end{pmatrix}}_{N}\underbrace{\begin{pmatrix}x_a\\x_b\end{pmatrix}}_x. %]]></script>

<p>Check $M\Sigma N$:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}M\Sigma N&=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&\mathbf{I}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&\Sigma_{ab}\\\Sigma_{ba}&\Sigma_{bb}\end{pmatrix}\begin{pmatrix}\mathbf{I}\\\mathbf{0}\end{pmatrix}\\&=\begin{pmatrix}\mathbf{0}& \Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}\end{pmatrix}\begin{pmatrix}\mathbf{I}\\\mathbf{0}\end{pmatrix}\\&=\mathbf{0}\end{aligned}. %]]></script>

<p>Thus $x_{b\cdot a}\perp x_a$, which means $x_{b\cdot a}|x_a=x_{b\cdot a}$.  According to our definition, we finally arrive at</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}x_b|x_a&=x_{b\cdot a}|x_a+\Sigma_{ba}\Sigma_{aa}^{-1}x_a|x_a\\&=x_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a\end{aligned}. %]]></script>

<p>There is a little abuse of notation: the term $x_a$ after $|$ is a sample of $x_a$ rather than a random variable. One should notice that $x_a$ below is constant. Therefore,</p>

<script type="math/tex; mode=display">\mathbb{E}[x_b|x_a]=\mathbb{E}[x_{b\cdot a}]+\Sigma_{ba}\Sigma_{aa}^{-1}x_a=\mu_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a,</script>

<p>and</p>

<script type="math/tex; mode=display">var[x_b|x_a]=var[x_{b\cdot a}]+0=\Sigma_{bb\cdot a}.</script>

<p>Removing all the extra variables we defined, we have</p>

<script type="math/tex; mode=display">x_b|x_a\sim\mathcal{N}(\mu_b+\Sigma_{ba}\Sigma_{aa}^{-1}(x_a-\mu_a), \Sigma_{bb}-\Sigma_{ba}\Sigma^{-1}_{aa}\Sigma_{ab}),</script>

<p>and the case for $x_a|x_b$ is similar.</p>

<h2 id="42-calculations-on-the-marginal-and-conditional-distribution">4.2 Calculations on the marginal and conditional distribution</h2>

<p>In this section, the problem is deriving $P(y)$ and $P(x|y)$ given $P(x)$ and $P(y|x)$. For example, let’s say, $x\sim\mathcal{N}(\mu, \Sigma)$ and $y|x\sim\mathcal{N}(Ax+b,L^{-1})$, which is common in <em>linear Gaussian model</em>. The abuse of notation here is the same as we mentioned above: the $x\ (y)$  in $y|x\ (x|y)$ represents a sample rather than a random variable. Now we define $y=Ax+b+\varepsilon$, where $\varepsilon\sim\mathcal{N}(0,L^{-1})$ is the noise in practice. Then we have</p>

<script type="math/tex; mode=display">\mathbb{E}[y]=\mathbb{E}[Ax+b+\varepsilon]=A\mu+b,</script>

<p>and</p>

<script type="math/tex; mode=display">var[y]=var[Ax+b]+var[\varepsilon]=A\Sigma A^T+L^{-1}.</script>

<p>Therefore $y\sim\mathcal{N}(A\mu+b, A\Sigma A^T+L^{-1})$. For the distribution of $x|y$, we can refer to the conclusion of section 4.1. Introducing a union variable $z=(x,y)^T$, we obtain</p>

<script type="math/tex; mode=display">% <![CDATA[
z\sim\mathcal{N}\left(\begin{pmatrix}\mu\\A\mu+b\end{pmatrix},\begin{pmatrix}\Sigma&Cov(x,y)\\Cov(y,x)&A\Sigma A^T+L^{-1}\end{pmatrix}\right). %]]></script>

<p>By the property of covariance, we have $Cov(y,x)=Cov(x,y)^T$. Further,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}Cov(x,y)&=\mathbb{E}[(x-\mu)(y-A\mu-b)^T]\\&=\mathbb{E}[(x-\mu)(Ax+b+\varepsilon-A\mu-b)^T]\\&=\mathbb{E}[(x-\mu)(Ax-A\mu)^T]+\mathbb{E}[(x-\mu)\varepsilon^T]\\&=\mathbb{E}[(x-\mu)(x-\mu)^T]A^T+0\\&=var(x)A^T\\&=\Sigma A^T\end{aligned}. %]]></script>

<p>With the known union distribution $z=(x,y)^T$ and $P(y)$, we can derive $P(x|y)$ by the method mentioned in section 4.1.</p>

<h1 id="5-conclusion">5. Conclusion</h1>

<p>In this post, we talk a lot about Gaussian distribution as it plays an important role in machine learning. Also, we are trying to get familar with those mathematical things.</p>

</div>


<div class="related">
  <h2>Related posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2020/12/06/inference-ml09/">
            Machine Learning - 09 Exact Inference of Graphical Models
            <small>06 Dec 2020</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2020/11/29/probabilistic_graphical_models-ml08/">
            Machine Learning - 08 Probabilistic Graphical Models
            <small>29 Nov 2020</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2020/11/12/exponential_family-ml07/">
            Machine Learning - 07 Exponential Family
            <small>12 Nov 2020</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>


<!-- Comments -->
<!--

-->
<!-- Comments Form -->
<!--
  <form method="POST" action="">
    <input name="options[redirect]" type="hidden" value="https://example.com">
    <input name="options[slug]" type="hidden" value="intro-ml01">
      <label>*Name</label>
      <input name="fields[name]" type="text">
      <label>E-mail</label>
      <input name="fields[email]" type="email"><br>
      <label>Message:</label>
      <textarea style="width:100%" name="fields[message]" rows="12"></textarea>
      <button type="submit">Submit comment</button>
      <small>Comments will appear after moderation.</small>
  </form>
-->
      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>