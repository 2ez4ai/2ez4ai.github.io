<!DOCTYPE html>
<html lang="en-us">

  <head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-51FFV4BDWW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-51FFV4BDWW');
</script>
  
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    Machine Learning - 01 Introduction &middot; Jingye Wang
  </title>

  
  <link rel="canonical" href="/2020/09/28/intro-ml01/">
  

  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[", "\\]"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    //,
    //displayAlign: "left",
    //displayIndent: "2em"
  });
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>"You can’t connect the dots looking forward; you can only connect them looking backward."</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/">Home</a>

    

    
    
      
        
      
    
      
    
      
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/categories/">Archive</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/about/">About</a>
        
      
    
    <span class="sidebar-nav-item">Currently v0.1.2</span>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2021. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">Jingye Wang</a>
            <small>A personal blog.</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <style>
  .button {
    border: 2px solid;
    border-radius: 7.5px;
    color: white;
    padding: 2.5px 7.5px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    transition-duration: 0.1s;
    cursor: pointer;
  }

  .buttonC {
    background-color: white; 
    color: black; 
    border: 2px solid #2e3131;
  }

  .buttonC:hover {
    background-color: #2e3131;
    color: white;
  }
  a:hover{
    text-decoration: none;
  }
</style>
<div class="post">
  <h1 class="post-title">Machine Learning - 01 Introduction </h1>
  <span class="post-date">
    28 Sep 2020
    <p></p>
  <div class="post-categories">
      
      
      <a class="button buttonC" href="/categories/#machine-learning">machine-learning</a>&nbsp;
      
      
    </div>
  </span>
  <p><em>The notes are based on the <a href="https://github.com/shuhuai007/Machine-Learning-Session">session</a>. Many thanks to the great work.</em></p>

<ul id="markdown-toc">
  <li><a href="#0-motivation" id="markdown-toc-0-motivation">0. Motivation</a></li>
  <li><a href="#1-frequentist-vs-bayesian" id="markdown-toc-1-frequentist-vs-bayesian">1. Frequentist vs Bayesian</a></li>
  <li><a href="#2maximum-likelihood-estimation-on-gaussian-distribution" id="markdown-toc-2maximum-likelihood-estimation-on-gaussian-distribution">2.Maximum Likelihood Estimation on Gaussian Distribution</a></li>
  <li><a href="#3-gaussian-distribution" id="markdown-toc-3-gaussian-distribution">3. Gaussian Distribution</a></li>
  <li><a href="#4-calculations-on-gaussian" id="markdown-toc-4-calculations-on-gaussian">4. Calculations on Gaussian</a>    <ul>
      <li><a href="#41-calculations-on-union-distribution" id="markdown-toc-41-calculations-on-union-distribution">4.1. Calculations on Union Distribution</a></li>
      <li><a href="#42-calculations-on-marginal-and-conditional-distribution" id="markdown-toc-42-calculations-on-marginal-and-conditional-distribution">4.2 Calculations on Marginal and Conditional Distribution</a></li>
    </ul>
  </li>
  <li><a href="#5-conclusion" id="markdown-toc-5-conclusion">5. Conclusion</a></li>
</ul>
<h1 id="0-motivation">0. Motivation</h1>

<p>After setting up this blog, I actually had no idea about what should I post. I am definitely not good at writing, but still I enjoy it a lot. As I am weak in machine learning and mathematics, for writing and learning, a ‘<em>translation</em>’ work of the <a href="https://github.com/shuhuai007/Machine-Learning-Session">session</a> may be a good start. So here we are!</p>

<h1 id="1-frequentist-vs-bayesian">1. Frequentist vs Bayesian</h1>

<p>Many machine learning methods can be illustrated in a <em>probabilistic</em> way. We now introduce two mainstream views of interpreting probability: <strong>frequentist</strong> and <strong>bayesian</strong>.</p>

<p>Firstly let us consider a simple example. Suppose we have a data $X=(x_1,x_2,\ldots,x_N)^T$ where $x_i$ are i.i.d. samples of the random variable $x$ and is of $d$ dimensions. Also, we define $\theta$ be the parameter so that $x\sim P(x|\theta)$, and we call $P(X|\theta)$ the <em>likelihood</em>.</p>

<p><strong>Frequentists</strong>: $\theta$ is an unknown <em>constant</em>. What they care is how to use the data to infer the value of $\theta$. The best known approach for that is <strong>maximum likelihood estimation</strong> (MLE):</p>

\[\hat{\theta}_\text{MLE}=\arg\max_{\theta}P(X|\theta).\]

<blockquote>
  <p>Many traditional machine learning methods are based on this view. They construct a probabilistic model, then determine a loss function and minimize it by methods like gradient descent.</p>
</blockquote>

<p><strong>Bayesians</strong>: $\theta$ is random variable, following distribution $P(\theta)$, which is called <em>prior distribution</em>, and $P(\theta|X)$ is called <em>posterior distribution</em>. Then by <em>Bayes’s theorem</em>, we have <strong>maximum a posteriori</strong> (MAP):</p>

\[\hat{\theta}_\text{MAP}=\arg\max_{\theta}P(\theta|X).\]

<blockquote>
  <p>In Bayesian inference, the posterior distribution, rather than the maximum, is the key. However, to determine the distribution sometimes is really hard as it may incur complex calculus:</p>

\[P(\theta|X)=\frac{P(X|\theta)\cdot P(\theta)}{\int_{\theta}P(X|\theta)\cdot P(\theta)\text{d}\theta}\]

  <p>Many powerful and elegant approaches were proposed for this. For examples, MCMC, probabilistic graphical model, <em>etc</em>.</p>
</blockquote>

<p>I’d like to cite <a href="https://www.probabilisticworld.com/frequentist-bayesian-approaches-inferential-statistics/">this</a> to summary the differences.</p>

<p><em>“In short, according to the frequentist definition of probability, only repeatable random events (like the result of flipping a coin) have probabilities. These probabilities are equal to the long-term frequency of occurrence of the events in question. Frequentists don’t attach probabilities to hypotheses or to any fixed but unknown values in general.”</em></p>

<p><em>“In contrast, Bayesians view probabilities as a more general concept. As a Bayesian, you can use probabilities to represent the uncertainty in any event or hypothesis. Here, it’s perfectly acceptable to assign probabilities to non-repeatable events.”</em></p>

<h1 id="2maximum-likelihood-estimation-on-gaussian-distribution">2.Maximum Likelihood Estimation on Gaussian Distribution</h1>

<p>In this section, we will give an example of MLE on Gaussian distribution. Just like what we defined in section 1, there is a data $X=(x_1,x_2,\ldots,x_N)^T$ where $x_i$ are i.i.d. samples of the random variable $x$ and is of $d=1$  dimension. Thus we have $X\in\mathbb{R}^{N\times 1}$. We further attach a specific distribution to $x$ as $x\sim\mathcal{N}(\mu,\sigma^2)$. The unknown parameter then becomes $\theta=(\mu,\sigma)$. Now we use MLE method to estimate the parameter:</p>

\[\hat{\theta}_\text{MLE}=\arg\max_\theta P(X|\theta)=\arg\max_\theta \log P(X|\theta).\]

<blockquote>
  <p>Given $x\sim\mathcal{N}(\mu,\sigma^2)$, the PDF of $x$ is</p>

\[p(x|\mu,\sigma)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}.\]
</blockquote>

<p>As $x_i,i=1,\ldots,N$ are i.i.d. variables, it follows that</p>

\[P(X|\theta)=\prod_{i=1}^NP(x_i|\theta).\]

<p>Hence,</p>

\[\begin{aligned}\arg\max_\theta\log P(X|\theta)&amp;=\arg\max_\theta\log \prod_{i=1}^N P(x_i|\theta)\\&amp;=\arg\max_\theta\sum_{i=1}^N\log P(x_i|\theta)\\&amp;=\arg\max_\theta\sum_{i=1}^N\log\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\\&amp;=\arg\max_\theta\sum_{i=1}^N\left[\log \frac{1}{\sqrt{2\pi}}+\log\frac{1}{\sigma}-\frac{(x_i-\mu)^2}{2\sigma^2}\right]\\&amp;=\arg\max_\theta\sum_{i=1}^N\left[\log\frac{1}{\sigma}-\frac{(x_i-\mu)^2}{2\sigma^2}\right]\end{aligned},\]

<p>which is equivalent to a numerical optimization problem. Denote the term $\sum_{i=1}^N\left[\log\frac{1}{\sigma}-\frac{(x_i-\mu)^2}{2\sigma^2}\right]$ as $\mathcal{L}$. The maximum occurs when</p>

\[\frac{\partial\mathcal{L}}{\partial\mu}=0,\quad \frac{\partial\mathcal{L}}{\partial\sigma}=0.\]

<p>Solving the equations we have</p>

\[\mu_\text{MLE}=\frac{1}{N}\sum_{i=1}^N x_i,\quad \sigma^2_\text{MLE}=\frac{1}{N}\sum_{i=1}^N(x_i-\mu_\text{MLE})^2.\]

<blockquote>
  <p>$\mu_\text{MLE}$ is an <em>unbiased estimation</em> as $\mathbb{E}[\mu_\text{MLE}]=\mu$, while $\sigma_\text{MLE}^2$ is a <em>biased estimation</em> since $\mathbb{E}[\sigma_\text{MLE}^2]=\frac{N-1}{N}\sigma^2$. The bias is incurred by the exploitation on samples rather than the true distribution. Intuitively, the variance of the samples will never be larger than the true distribution since they are totally generated from that!</p>
</blockquote>

<h1 id="3-gaussian-distribution">3. Gaussian Distribution</h1>

<p>Many advanced machine learning techniques, like <em>linear Gaussian model</em>, <em>Kalman filter</em>, <em>P-PCA</em>, <em>etc</em>, involve Gaussian distribution. Therefore it is quite practical to get familiar with Gaussian distribution. In this section, we are trying to explain why the PDF of bivariate Gaussian distribution is shaped by numerous ellipses.</p>

<p>Suppose we have a random variable $x\sim \mathcal{N}(\mu,\Sigma)$ of $d$ dimensions. Specifically, the definition follows that</p>

\[P(x|\mu,\Sigma)=\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)},\]

<p>and</p>

\[x=\begin{pmatrix}x_{1}\\x_{2}\\\vdots\\x_{d}\end{pmatrix}_{d\times1},\quad \mu=\begin{pmatrix}\mu_1\\\mu_2\\\vdots\\\mu_{d}\end{pmatrix}_{d\times1},\quad\Sigma=\begin{pmatrix}\sigma_{11}&amp;\sigma_{12}&amp;\cdots&amp;\sigma_{1d}\\\sigma_{21}&amp;\sigma_{22}&amp;\cdots&amp;\sigma_{2d}\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\\sigma_{d1}&amp;\sigma_{d2}&amp;\cdots&amp;\sigma_{dd}\end{pmatrix}_{d\times d}.\]

<p>As it involves matrix, a simplification will be a wise choice. For simplicity,  we use $\Delta(x)$ to represent $(x-\mu)^T\Sigma^{-1}(x-\mu)$ and suppose the covariance matrix $\Sigma$ to be positive definite, which is reasonable. By the eigenvalue decomposition and the property of positive definite matrix, $\Sigma$ can be decomposed as $\Sigma=U\Lambda U^T$, where $U=(u_1,u_2,\ldots,u_d)$ is an orthogonal matrix whose columns are the eigenvectors of $\Sigma$, and $\Lambda=\text{diag}(\lambda_1,\lambda_2,\ldots,\lambda_d)$ is a diagonal matrix whose entries are the eigenvalues of $\Sigma$. Then we have</p>

\[\begin{aligned}\Sigma^{-1}&amp;=(U\Lambda U^T)^{-1}\\&amp;=(U^T)^{-1}\Lambda^{-1}U^{-1}\\&amp;=U\Lambda^{-1}U^T\\&amp;=(u_1,u_2,\ldots,u_d)\begin{pmatrix}\frac{1}{\lambda_1}&amp;0&amp;\cdots&amp;0\\0&amp;\frac{1}{\lambda_2}&amp;\cdots&amp;0\\\vdots&amp;\vdots&amp;\ddots&amp;0\\0&amp;0&amp;\cdots&amp;\frac{1}{\lambda_d}\end{pmatrix}\begin{pmatrix}u_1\\u_2\\\vdots\\u_d\end{pmatrix}\\&amp;=\begin{pmatrix}\frac{u_1}{\lambda_1},\frac{u_2}{\lambda_2},\ldots,\frac{u_d}{\lambda_d}\end{pmatrix}\begin{pmatrix}u_1\\u_2\\\vdots\\u_d\end{pmatrix}\\&amp;=\sum_{i=1}^d\frac{u_iu_i^T}{\lambda_i}\end{aligned}.\]

<p>Plugging it in $\Delta(x)$, we arrive at</p>

\[\begin{aligned}\Delta(x)&amp;=(x-\mu)^T\left(\sum_{i=1}^d\frac{u_iu_i^T}{\lambda_i}\right)(x-\mu)\\&amp;=\sum_{i=1}^d(x-\mu)^Tu_i\frac{1}{\lambda_i}u_i^T(x-\mu)\end{aligned}.\]

<p>Before moving on, we take a pause to be acquainted with those notations. Specifically, $(x-\mu)\in\mathbb{R}^{d\times 1}$ as $x, \mu\in\mathbb{R}^{d\times 1}$. $u_i$ is the eigenvector of $\Sigma$ so $u_i\in\mathbb{R}^{d\times 1}$, while $\lambda_i$ is the eigenvalue of $\Sigma$ so $\lambda_i\in\mathbb{R}$. Therefore $(x-\mu)^Tu_i\in\mathbb{R}$, $u_i^T(x-\mu)\in\mathbb{R}$ and $\Delta(x)\in\mathbb{R}$. It should not be surprising. Recall that the PDF</p>

\[P(x|\mu,\Sigma)=\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}\Delta(x)}\]

<p>represents a probability, which is definitely a real number. Now we introduce $Y=(y_1,y_2,\ldots,y_d)^T$ where $y_i=(x-\mu)^Tu_i$. Hence</p>

\[\Delta(x)=\sum_{i=1}^dy_i\frac{1}{\lambda_i}y_i^T=\sum_{i=1}^d\frac{y_i^2}{\lambda_i}.\]

<p><strong>The PDF of $x$ is rewritten as</strong></p>

\[P(x|\mu,\Sigma)=\alpha e^{-\frac{1}{2}\sum_{i=1}^d y_i^2/\lambda_i},\]

<p>where $\alpha=\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}$ is a constant. Now suppose we are trying to determine where $P(x|\mu,\Sigma)=\beta$. To make it intuitive, we consider the case $d=2$, then it follows that</p>

\[\begin{alignat*}{3}&amp;&amp; \alpha e^{-\frac{1}{2}\left(\frac{y_1^2}{\lambda_1}+\frac{y_2^2}{\lambda_2}\right)}&amp;=\beta\\\Rightarrow&amp;&amp;\frac{y_1^2}{\lambda_1}+\frac{y_2^2}{\lambda_2}&amp;=2(\ln\alpha-\ln\beta)\\\Rightarrow&amp;&amp;\frac{y_1^2}{a^2}+\frac{y_2^2}{b^2}&amp;=1\end{alignat*},\]

<p>where $a^2=2\lambda_1(\ln\alpha-\ln\beta)$ and $b^2=2\lambda_2(\ln\alpha-\ln\beta)$, and we finally get <strong>a standard equation of ellipse</strong> with respect to $y_1$ and $y_2$! Actually, the transformation $y_i=(x-\mu)^Tu_i$ is the projection of $x$ on $u_i$. Thus in the coordinate system represented by $x_1$ and $x_2$, the ellipse will be changed linearly hence the ellipse will not be so standard. For different $\beta$, we have ellipses with different major and minor axes, and that is why the graph of bivariate Gaussian distribution looks like it is composed of numerous concentration ellipses.</p>

<p><em>PS.</em> The above discussion leaves much correctness proof to be done.</p>

<h1 id="4-calculations-on-gaussian">4. Calculations on Gaussian</h1>

<p>In this section, all the random variables are Gaussian and we will not touch the complex PDF. Rather, we mainly focus on the statistics, the expectation and the variance, of Gaussian distribution.</p>

<h2 id="41-calculations-on-union-distribution">4.1. Calculations on Union Distribution</h2>

<p>Suppose $x\sim\mathcal{N}(\mu, \Sigma)$ is the union distribution of $x_a$ and $x_b$, <em>i.e.</em> $x=(x_a,x_b)^T$. Then the definition follows that</p>

\[x=\begin{pmatrix}x_a\\x_b\end{pmatrix}_{d\times1},\quad \mu=\begin{pmatrix}\mu_a\\\mu_b\end{pmatrix}_{d\times1},\quad\Sigma=\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}_{d\times d},\]

<p>where</p>

\[x_a,\mu_a\in\mathbb{R}^{m\times 1},x_b,\mu_b\in\mathbb{R}^{n\times 1}, m+n=d,\]

\[\Sigma_{aa}\in\mathbb{R}^{m\times m}, \Sigma_{ab},\Sigma_{ba}^T\in\mathbb{R}^{m\times n}, \Sigma_{bb}\in\mathbb{R}^{n\times n}.\]

<p>The problem in this section is how to derive $P(x_a)$ and $P(x_b|x_a)$, or symmetrically, $P(x_b)$ and $P(x_a|x_b)$ given the union distribution.  The derivation of $P(x_a)$ is quite straightforward:</p>

\[\begin{aligned}\mathbb{E}[x_a]&amp;=\mathbb{E}\left[\begin{pmatrix}\mathbf{I}_{m\times m}&amp;\mathbf{0}_{m\times n}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}\right]\\&amp;=\begin{pmatrix}\mathbf{I}_{m\times m}&amp;\mathbf{0}_{m\times n}\end{pmatrix}\mathbb{E}[x]\\&amp;=\begin{pmatrix}\mathbf{I}_{m\times m}&amp;\mathbf{0}_{m\times n}\end{pmatrix}\begin{pmatrix}\mu_a\\\mu_b\end{pmatrix}\\&amp;=\mu_a\end{aligned},\]

<p>and</p>

\[\begin{aligned}var[x_a]&amp;=var\left[\begin{pmatrix}\mathbf{I}_{m\times m}&amp;\mathbf{0}_{m\times n}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}\right]\\&amp;=\begin{pmatrix}\mathbf{I}_{m\times m}&amp;\mathbf{0}_{m\times n}\end{pmatrix}var[x]\begin{pmatrix}\mathbf{I}_{m\times m}\\\mathbf{0}_{n\times m}\end{pmatrix}\\&amp;=\begin{pmatrix}\mathbf{I}_{m\times m}&amp;\mathbf{0}_{m\times n}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\begin{pmatrix}\mathbf{I}_{m\times m}\\\mathbf{0}_{n\times m}\end{pmatrix}\\&amp;=\Sigma_{aa}\end{aligned}.\]

<p>Thus, we have $x_a\sim\mathcal{N}(\mu_a,\Sigma_{aa})$, and similarly, $x_b\sim\mathcal{N}(\mu_b,\Sigma_{bb})$. For the conditional probabilities $P(x_b|x_a)$, we first define variables</p>

\[x_{b\cdot a}=x_b-\Sigma_{ba}\Sigma^{-1}_{aa}x_a,\]

\[\mu_{b\cdot a}=\mu_b-\Sigma_{ba}\Sigma^{-1}_{aa}\mu_a,\]

<p>and</p>

\[\Sigma_{bb\cdot a}=\Sigma_{bb}-\Sigma_{ba}\Sigma^{-1}_{aa}\Sigma_{ab}.\]

<p>The term $\Sigma_{bb\cdot a}$ is also the <em>Schur complement</em> of $\Sigma_{aa}$ of the matrix $\Sigma$. Then it follows that</p>

\[\begin{aligned}\mathbb{E}[x_{b\cdot a}]&amp;=\mathbb{E}\left[\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbf{I}_{n\times n}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}\right]\\&amp;=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbf{I}_{n\times n}\end{pmatrix}\mathbb{E}[x]\\&amp;=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbf{I}_{n\times n}\end{pmatrix}\begin{pmatrix}\mu_a\\\mu_b\end{pmatrix}\\&amp;=\mu_{b\cdot a}\end{aligned},\]

<p>and</p>

\[\begin{aligned}var[x_{b\cdot a}]&amp;=var\left[\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbf{I}_{n\times n}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}\right]\\&amp;=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbf{I}_{n\times n}\end{pmatrix}var[x]\begin{pmatrix}-\Sigma_{aa}^{-1}\Sigma_{ba}^T\\\mathbf{I}_{n\times n}\end{pmatrix}\\&amp;=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbf{I}_{n\times n}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\begin{pmatrix}-\Sigma_{aa}^{-1}\Sigma_{ba}^T\\\mathbf{I}_{n\times n}\end{pmatrix}\\&amp;=\Sigma_{bb\cdot a}\end{aligned}.\]

<p>Hence we have $x_{b\cdot a}\sim\mathcal{N}(\mu_{b\cdot a},\Sigma_{bb\cdot a})$. Before showing the relation between $x_{b\cdot a}$ and $x_b|x_a$, we introduce a helpful theorem.</p>

<p><strong>Theorem</strong>:<em>If</em> $x\sim\mathcal{N}(\mu,\Sigma)$, <em>then</em> $Mx\perp Nx\iff M\Sigma N=\mathbf{0}.$</p>

<p><em>Proof:</em> According to the property of Gaussian, $Mx\sim\mathcal{N}(M\mu,M\Sigma M^T)$ and $Nx\sim\mathcal{N}(N\mu,N\Sigma N^T)$. Further,</p>

\[\begin{aligned}Cov(Mx,Nx)&amp;=\mathbb{E}[(Mx-M\mu)(Nx-N\mu)^T]\\&amp;=M\mathbb{E}[(x-\mu)(x-\mu)^T]N\\&amp;=M\Sigma N^T.\end{aligned}\]

<p>For Gaussian distribution, uncorrelation implies independence. Therefore $M\Sigma N^T=\mathbf{0}\iff Mx\perp Nx.\tag*{$\blacksquare$}$</p>

<p>Back to our case, we have</p>

\[x_{b\cdot a}=\underbrace{\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbf{I}\end{pmatrix}}_M\underbrace{\begin{pmatrix}x_a\\x_b\end{pmatrix}}_x,\qquad x_a=\underbrace{\begin{pmatrix}\mathbf{I}&amp;\mathbf{0}\end{pmatrix}}_{N}\underbrace{\begin{pmatrix}x_a\\x_b\end{pmatrix}}_x.\]

<p>Check $M\Sigma N$:</p>

\[\begin{aligned}M\Sigma N&amp;=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbf{I}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\begin{pmatrix}\mathbf{I}\\\mathbf{0}\end{pmatrix}\\&amp;=\begin{pmatrix}\mathbf{0}&amp; \Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}\end{pmatrix}\begin{pmatrix}\mathbf{I}\\\mathbf{0}\end{pmatrix}\\&amp;=\mathbf{0}\end{aligned}.\]

<p>Thus $x_{b\cdot a}\perp x_a$, which means $x_{b\cdot a}|x_a=x_{b\cdot a}$.  According to our definition, we finally arrive at</p>

\[\begin{aligned}x_b|x_a&amp;=x_{b\cdot a}|x_a+\Sigma_{ba}\Sigma_{aa}^{-1}x_a|x_a\\&amp;=x_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a\end{aligned}.\]

<p>There is a little abuse of notation: the term $x_a$ after $|$ is a sample of $x_a$ rather than a random variable. One should notice that $x_a$ below is constant. Therefore,</p>

\[\mathbb{E}[x_b|x_a]=\mathbb{E}[x_{b\cdot a}]+\Sigma_{ba}\Sigma_{aa}^{-1}x_a=\mu_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a,\]

<p>and</p>

\[var[x_b|x_a]=var[x_{b\cdot a}]+0=\Sigma_{bb\cdot a}.\]

<p>Removing all the extra variables we defined, we have</p>

\[x_b|x_a\sim\mathcal{N}(\mu_b+\Sigma_{ba}\Sigma_{aa}^{-1}(x_a-\mu_a), \Sigma_{bb}-\Sigma_{ba}\Sigma^{-1}_{aa}\Sigma_{ab}),\]

<p>and the case for $x_a|x_b$ is similar.</p>

<h2 id="42-calculations-on-marginal-and-conditional-distribution">4.2 Calculations on Marginal and Conditional Distribution</h2>

<p>In this section, the problem is deriving $P(y)$ and $P(x|y)$ given $P(x)$ and $P(y|x)$. For example, let’s say, $x\sim\mathcal{N}(\mu, \Sigma)$ and $y|x\sim\mathcal{N}(Ax+b,L^{-1})$, which is common in <em>linear Gaussian model</em>. The abuse of notation here is the same as we mentioned above: the $x\ (y)$  in $y|x\ (x|y)$ represents a sample rather than a random variable. Now we define $y=Ax+b+\varepsilon$, where $\varepsilon\sim\mathcal{N}(0,L^{-1})$ is the noise in practice. Then we have</p>

\[\mathbb{E}[y]=\mathbb{E}[Ax+b+\varepsilon]=A\mu+b,\]

<p>and</p>

\[var[y]=var[Ax+b]+var[\varepsilon]=A\Sigma A^T+L^{-1}.\]

<p>Therefore $y\sim\mathcal{N}(A\mu+b, A\Sigma A^T+L^{-1})$. For the distribution of $x|y$, we can refer to the conclusion of section 4.1. Introducing a union variable $z=(x,y)^T$, we obtain</p>

\[z\sim\mathcal{N}\left(\begin{pmatrix}\mu\\A\mu+b\end{pmatrix},\begin{pmatrix}\Sigma&amp;Cov(x,y)\\Cov(y,x)&amp;A\Sigma A^T+L^{-1}\end{pmatrix}\right).\]

<p>By the property of covariance, we have $Cov(y,x)=Cov(x,y)^T$. Further,</p>

\[\begin{aligned}Cov(x,y)&amp;=\mathbb{E}[(x-\mu)(y-A\mu-b)^T]\\&amp;=\mathbb{E}[(x-\mu)(Ax+b+\varepsilon-A\mu-b)^T]\\&amp;=\mathbb{E}[(x-\mu)(Ax-A\mu)^T]+\mathbb{E}[(x-\mu)\varepsilon^T]\\&amp;=\mathbb{E}[(x-\mu)(x-\mu)^T]A^T+0\\&amp;=var(x)A^T\\&amp;=\Sigma A^T\end{aligned}.\]

<p>With the known union distribution $z=(x,y)^T$ and $P(y)$, we can derive $P(x|y)$ by the method mentioned in section 4.1.</p>

<h1 id="5-conclusion">5. Conclusion</h1>

<p>In this post, we talk a lot about Gaussian distribution as it plays an important role in machine learning. Also, we are trying to get familar with those mathematical things.</p>

</div>

 <!--https://blog.webjeda.com/jekyll-related-posts/-->
<div class="related">
  <h2>Related posts</h2>
  
  
  

  

    
    

    

    
      <div>
      <h5><a href="/2020/12/05/inference-ml09/">Machine Learning - 09 Exact Inference of Graphical Models <span class="label label-default">machine-learning</span> </a></h5>
      </div>
      
      
    

  

    
    

    

    
      <div>
      <h5><a href="/2020/11/29/probabilistic_graphical_models-ml08/">Machine Learning - 08 Probabilistic Graphical Models <span class="label label-default">machine-learning</span> </a></h5>
      </div>
      
      
    

  

    
    

    

    
      <div>
      <h5><a href="/2020/11/12/exponential_family-ml07/">Machine Learning - 07 Exponential Family <span class="label label-default">machine-learning</span> </a></h5>
      </div>
      
      
        
</div>


<!--

<div class="related">
  <h2>Related posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2020/12/05/inference-ml09/">
            Machine Learning - 09 Exact Inference of Graphical Models
            <small>05 Dec 2020</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2020/11/29/probabilistic_graphical_models-ml08/">
            Machine Learning - 08 Probabilistic Graphical Models
            <small>29 Nov 2020</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2020/11/12/exponential_family-ml07/">
            Machine Learning - 07 Exponential Family
            <small>12 Nov 2020</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

-->

<!-- Comments -->
<!--

-->
<!-- Comments Form -->
<!--
  <form method="POST" action="">
    <input name="options[redirect]" type="hidden" value="https://example.com">
    <input name="options[slug]" type="hidden" value="intro-ml01">
      <label>*Name</label>
      <input name="fields[name]" type="text">
      <label>E-mail</label>
      <input name="fields[email]" type="email"><br>
      <label>Message:</label>
      <textarea style="width:100%" name="fields[message]" rows="12"></textarea>
      <button type="submit">Submit comment</button>
      <small>Comments will appear after moderation.</small>
  </form>
-->
      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script src='/public/js/script.js'></script>
  </body>
</html>