I"…4<p><em>The notes are based on the <a href="https://github.com/shuhuai007/Machine-Learning-Session">session</a> and the <a href="https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter8.pdf">material</a>. For the fundamental of linear algebra, one can always refer to <a href="http://math.mit.edu/~gs/linearalgebra/">Introduction to Linear Algebra</a> and <a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">The Matrix Cookbook</a> for more details. Many thanks to these great works.</em></p>

<h1 id="0-introduction">0. Introduction</h1>

<p>An exponential family is a family of distributions which share some properties in common. The real message of this note is the simplicity and elegance of the exponential family. Once the ideas are mastered, it is often easier to work within the general exponential family framework than with specific instances.</p>

<h1 id="1-exponential-family">1. Exponential Family</h1>

<p>Given one real-vector parameter <script type="math/tex">\mathbf{\theta}=[\theta_1,\theta_2,\dots,\theta_d]^T</script>, we define an <em>exponential family</em> of probability distributions as those distributions whose density have the following general form:</p>

<script type="math/tex; mode=display">f_X(x\vert\eta)=h(x)\exp\left(\eta^T T(x)-A(\eta)\right).</script>

<p><strong>Canonical parameter</strong>: $\eta$ is called <em>canonical</em>, or <em>natural parameter</em> (function), which can be viewed as a transformation of $\mathcal{\theta}$. The set of values of $\eta$ is always convex.</p>

<p><strong>Sufficient statistic</strong>: Given a data set sampled from <script type="math/tex">f_X(x\vert\eta)</script>, the sufficient statistic $T(x)$ is a function of the data that holds all information the data set provides with regard to the unknown parameter $\mathbf{\theta}$.</p>

<p><strong>Log-partition function</strong>: $A(\eta)$ is the <em>log-partition function</em> to normalize <script type="math/tex">f_X(x\vert \eta)</script> to be a probability distribution,</p>

<script type="math/tex; mode=display">A(\eta)=\log\left(\int_{X}h(x)\exp(\eta^T T(x))\text{d}x\right).</script>

<p>In the following sections, we will discuss them detailedly.</p>

<h1 id="2-sufficient-statistic">2. Sufficient Statistic</h1>

<p>Consider the problem of estimating the unknown parameters by <em>maximum likelihood estimation</em> (MLE) in exponential family cases. Specifically, for an <em>i.i.d.</em> data set <script type="math/tex">\mathcal{D}=\{x_1,x_2,\dots,x_N\}</script>, we have the log likelihood</p>

<script type="math/tex; mode=display">\mathcal{L}(\eta\vert\mathcal{D})=\log\left(\prod_{i=1}^Nh(x_i)\right)+\eta^T\left(\sum_{i=1}^NT(x_i)\right)-NA(\eta).</script>

<p>By <em>MLE</em>, we have the estimation $\hat\eta$ when its gradient with respect to $\eta$ is zero:</p>

<script type="math/tex; mode=display">\mathcal{L}â€™(\eta\vert\mathcal{D})=\sum_{i=1}^NT(x_i)-NAâ€™(\eta)=0.</script>

<p>Solving the equation, we have</p>

<script type="math/tex; mode=display">Aâ€™(\hat\eta)=\frac{1}{N}\sum_{i=1}^NT(x_i),</script>

<p>which is the general formula of MLE for the parameters in the exponential family. Further, notice that our formula involves the data only via the sufficient statistic $T(x_i)$. This gives the operational meaning to <em>sufficiency</em>â€”for the purpose of estimating parameters we retain only the sufficient statistic.</p>

<h1 id="3-log-partition-function">3. Log-partition Function</h1>

<p>As we mentioned in section 1, $A(\eta)$ can be viewed as a normalization factor. In fact, $A(\eta)$ is not a degree of freedom in the specification of an exponential family density; it is determined once $T(x)$ and $h(x)$ are determined. The relation between $A(\eta)$ and $T(x)$ can be further characterized by</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}Aâ€™(\eta)&=\frac{\text{d}\log\left(\int_{X}h(x)\exp(\eta^T T(x))\text{d}x\right)}{\text{d}\eta}\\&=\frac{\int_{X}h(x)\exp(\eta^T T(x))\cdot  T(x)\text{d}x}{\int_{X}h(x)\exp(\eta^T T(x))\text{d}x}\\&=\frac{\int_{X}h(x)\exp(\eta^T T(x))\cdot T(x)\text{d}x}{\exp({A(\mathbf{\theta})})}\\&=\int_{X}\underbrace{h(x)\exp(\eta^T T(x)-A(\eta))}_{f_X(x\vert \eta)}\cdot T(x)\text{d}x\\&=\mathbb{E}_{f_X(x\vert\eta)}[T(x)].\end{aligned} %]]></script>

<p>Further, we have</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}Aâ€™â€™(\eta)&=\int_{X}f_X(x\vert \eta)\cdot(T(x)-Aâ€™(\eta)) T(x)\text{d}x\\&=\int_{X}f_X(x\vert \eta)\cdot(T(x))^2\text{d}x-Aâ€™(\eta)\int_{X}f_X(x\vert \mathbf{\eta})\cdot T(x)\text{d}x\\&=\mathbb{E}_{f_X(x\vert\eta)}[(T(x))^2]-\left(\mathbb{E}_{f_X(x\vert\eta)}[T(x)]\right)^2\\&=var[T(x)],\end{aligned} %]]></script>

<p>which also shows that $A(\eta)$ is convex as $var[T(x)]\ge 0$.</p>

<h1 id="4-maximum-entropy">4. Maximum Entropy</h1>

<p>The entropy of $P$ with distribution $p(x)$ supported on $X$ is</p>

<script type="math/tex; mode=display">H(P)=\mathbb{E}_{P}[-\log p(x)].</script>

<p>The <em>maximum entropy</em> principle is that: given some constraints (prior information) about the distribution $P$, we consider all probability distributions satisfying said constraints such that the constraints are being utilized as <em>objective</em> as possible, <em>i.e.,</em> be as uncertain as possible.</p>

<p>For example, consider the case where the very constraint is $\sum_Xp(x)=1$, which formulates</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\max&\quad H(P)\\\text{s.t.}&\quad \sum_{X}p(x)=1.\end{aligned} %]]></script>

<p>By the definition we have</p>

<script type="math/tex; mode=display">H(P)=-\sum_{i=1}^{\vert X\vert}p(x_i)\log p(x_i).</script>

<p>Then the <em>Lagrangian</em> for the optimization problem is</p>

<script type="math/tex; mode=display">L(P,\lambda)=\sum_{i=1}^{\vert X\vert}p(x_i)\log p(x_i)+\lambda\left(1-\sum_{i=1}^{\vert X\vert}p(x_i)\right).</script>

<p>Setting the first derivation of the Lagrangian to be zero yields</p>

<script type="math/tex; mode=display">\frac{\partial L}{\partial p(x_i)}=0\implies \hat{p}(x_i)=\exp(\lambda-1),</script>

<p>which gives that</p>

<script type="math/tex; mode=display">\hat{p}(x_1)=\hat{p}(x_2)=\dots=\hat{p}(x_{\vert X\vert})=\frac{1}{\vert X\vert},</script>

<p><em>i.e.,</em> the distribution with maximum entropy is <em>uniform distribution</em>.</p>

<p>We now consider a general case where $p(x)$ is continuous with a general constraint $\mathbb{E}_P[\Phi(x)]=\alpha$, where $\Phi(x)=[\phi_1(x),\phi_2(x),\dots,\phi_d(x)]\in\mathbb{R}^d$ and $\alpha=[\alpha_1,\alpha_2,\dots,\alpha_d]\in\mathbb{R}^d$, which formulates</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\max&\quad H(P)\\\text{s.t.}&\quad \mathbb{E}_P[\Phi(x)]=\alpha\\\implies\min&\quad \int_Xp(x)\log p(x)\text{d}x\\\text{s.t.}&\quad \int_X p(x)\phi_i(x)\text{d}x=\alpha_i,\ i=1,2,\dots, d,\\&\quad \int_X p(x)\text{d}x=1.\end{aligned} %]]></script>

<p>Similarly, we obtain the Lagrangian as</p>

<script type="math/tex; mode=display">L(P,\theta,\lambda)=\int_X p(x)\log p(x)\text{d}x+\sum_{i=1}^d\theta_i\left(\alpha_i-\int_X p(x)\phi_i(x)\text{d}x\right)+\lambda\left(\int_X p(x)\text{d}x-1\right).</script>

<p>By treating the density $P=[p(x)]_{x\in X}$ as a finite vector such that $\int_X p(x)\text{d}x$ is similar to $\sum_X p(x)$, we have</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\frac{\partial L}{\partial p(x)}&=\frac{\partial }{\partial p(x)}\left(\sum_X p(x)\log p(x)-\sum_{i=1}^d\theta_i\sum_X p(x)\phi_i(x)+\lambda\sum_X p(x)\right)\\&=1+\log p(x)-\sum_{i=1}^d\theta_i\phi_i(x)+\lambda\\&=1+\log p(x)-\theta^T\Phi(x)+\lambda.\end{aligned} %]]></script>

<p>Setting the derivation to be zero for all $x$, we have</p>

<script type="math/tex; mode=display">p(x)=\exp\left\{\theta^T\Phi(x)-(\lambda+1)\right\},</script>

<p>which is in the exponential family form with</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\eta&=\theta,\\T(x)&=\Phi(x),\\A(\eta)&=\lambda+1,\\h(x)&=1.\end{aligned} %]]></script>

<h1 id="5-gaussian-distribution">5. Gaussian Distribution</h1>

<p>In this section, we consider an example, Gaussian distribution, which is of the exponential family and exemplifies the properties we mentioned above.</p>

<p>We first rewritten the PDF of one-dimension Gaussian distribution to show it is in the exponential family .</p>

<p><em>Proof:</em> Given unknown parameter <script type="math/tex">\mathbf{\theta}=[\mu,\sigma^2]</script>, the Gaussian density can be written as follows,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}f_X(x\vert \mathbf{\theta})&=\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{-\frac{1}{2\sigma^2}(x-\mu)^2\right\}\\&=\frac{1}{\sqrt{2\pi}}\exp\left\{\frac{\mu}{\sigma^2}x-\frac{1}{2\sigma^2}x^2-\frac{1}{2\sigma^2}\mu^2-\log\sigma\right\}\\&=\frac{1}{\sqrt{2\pi}}\exp\left\{\begin{bmatrix}\frac{\mu}{\sigma^2}&-\frac{1}{2}\sigma^2\end{bmatrix}\begin{bmatrix}x\\x^2\end{bmatrix}-\left(\frac{\mu^2}{2\sigma^2}+\log\sigma\right)\right\},\end{aligned} %]]></script>

<p>which is in the exponential family form with</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\eta&=\begin{bmatrix}\frac{\mu}{\sigma^2}&-\frac{1}{2\sigma^2}\end{bmatrix}^T,\\T(x)&=\begin{bmatrix}x&x^2\end{bmatrix}^T,\\A(\eta)&=\frac{\mu^2}{2\sigma^2}+\log\sigma=-\frac{\eta_1^2}{4\eta_2}-\frac{1}{2}\log(-2\eta_2),\\h(x)&=\frac{1}{\sqrt{2\pi}}.\end{aligned} %]]></script>

<script type="math/tex; mode=display">\tag*{$\blacksquare$}</script>

<p>Then we verify the relation between the sufficient statistic and MLE method.</p>

<p><em>Proof:</em> Given a data set <script type="math/tex">\mathcal{D}=\{x_1,x_2,\dots,x_N\}</script>, as we mentioned in section 2, we can derive the parameters via the sufficient statistic as follows,</p>

<script type="math/tex; mode=display">\begin{cases}Aâ€™(\eta)=\frac{1}{N}\sum_{i=1}^NT(x)\implies\begin{cases}Aâ€™(\hat\eta_1)=\frac{1}{N}\sum_{i=1}^N x_i\\Aâ€™(\hat\eta_2)=\frac{1}{N}\sum_{i=1}^Nx_i^2\end{cases}\\A(\eta)=-\frac{\eta_1^2}{4\eta_2}-\frac{1}{2}\log(-2\eta_2)\implies\begin{cases}Aâ€™(\hat\eta_1)=-\frac{\eta_1}{2\eta_2}=\hat\mu\\Aâ€™(\hat\eta_2)=\frac{\eta_1^2}{4\eta_2^2}-\frac{1}{2\eta_2}=\hat\sigma^2+\hat\mu^2\end{cases}\end{cases}</script>

<p>Solving the equations we have</p>

<script type="math/tex; mode=display">\hat\mu=\frac{1}{N}\sum_{i=1}^Nx_i,\quad \hat\sigma=\frac{1}{N}\sum_{i=1}^Nx_i^2-\hat\mu^2,</script>

<p>which is consistent with the result in the <a href="https://2ez4ai.github.io/2020/09/28/intro-ml01/">post</a>. $\tag*{$\blacksquare$}$</p>

<p>Now we show that</p>

<script type="math/tex; mode=display">Aâ€™â€™(\hat\eta)=var[T(x)].</script>

<p><em>Proof</em>: Firstly, we have</p>

<script type="math/tex; mode=display">\begin{cases}Aâ€™â€™(\hat\eta_1)=-\frac{1}{2\eta_2}=\sigma^2\\Aâ€™â€™(\hat\eta_2)=-\frac{\eta_1^2}{2\eta_2^3}+\frac{1}{2\eta_2^2}=4\sigma^2\mu^2+2\sigma^4\end{cases}</script>

<p>For <script type="math/tex">% <![CDATA[
T(x)=\begin{bmatrix}x&x^2\end{bmatrix}^T %]]></script>, we have</p>

<script type="math/tex; mode=display">var[x]=\sigma^2, \text{ as }x\sim\mathcal{N}(\mu,\sigma^2),</script>

<p>and</p>

<script type="math/tex; mode=display">var[x^2]=\mathbb{E}[x^4]-\left(\mathbb{E}[x^2]\right)^2.</script>

<p>For $\mathbb{E}[x^2]$, it follows that</p>

<script type="math/tex; mode=display">\mathbb{E}[x^2]=var[x]+(\mathbb{E}[x])^2=\sigma^2+\mu^2.</script>

<p>For $\mathbb{E}[x^4]$, to compute it we leverage <em>moment generating functions</em> which follows that</p>

<script type="math/tex; mode=display">M_X(t)=e^{\mu t+\frac{1}{2}\sigma^2t^2},\quad \mathbb{E}[x^4]=M^{(4)}_X(0).</script>

<p>After a laborious computing, we have</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}var[x^2]&=\mathbb{E}[x^4]-\left(\mathbb{E}[x^2]\right)^2\\&=3\sigma^4+6\sigma^2\mu^2+\mu^4-\sigma^4-2\sigma^2-\mu^4\\&=4\sigma^2\mu^2+2\sigma^4.\end{aligned} %]]></script>

<p>Therefore, we have</p>

<script type="math/tex; mode=display">Aâ€™â€™(\hat\eta)=\begin{bmatrix}var[x]\\var[x^2]\end{bmatrix}.\tag*{$\blacksquare$}</script>

<p>Finally, we show that $X\sim\mathcal{N}(\mu,\sigma^2)$ is the distribution that maximizes the entropy over all distributions $P$ satisfying</p>

<script type="math/tex; mode=display">\mathbb{E}_P\left[\left(\frac{X-\mu}{\sigma}\right)^2\right]=1.</script>

<p><em>Proof:</em> Consider the expression we formulated in section 4,</p>

<script type="math/tex; mode=display">p(x)=\exp\left\{\theta^T\Phi(x)-(\lambda+1)\right\},</script>

<p>which maximizes the entropy while satisfying $\mathbb{E}_P[\Phi(x)]=\alpha$. Now letting</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\alpha&=1,\\\Phi(x)&=\frac{(x-\mu)^2}{\sigma^2},\\\theta&=-\frac{1}{2},\\\exp\{-\lambda-1\}&=\frac{1}{\sqrt{2\pi}\sigma}.\end{aligned} %]]></script>

<p>Therefore we have</p>

<script type="math/tex; mode=display">p(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{-\frac{1}{2\sigma^2}(x-\mu)^2\right\}.\tag*{$\blacksquare$}</script>

<h1 id="6-conclusion">6. Conclusion</h1>

<p>In this post, we briefly introduced the basic form of the exponential family. Then we discussed its properties from three perspectives: sufficient statistic, log-partition function and maximum entropy. Moreover, with one-dimension Gaussian distribution, we exemplified the properties.</p>

:ET