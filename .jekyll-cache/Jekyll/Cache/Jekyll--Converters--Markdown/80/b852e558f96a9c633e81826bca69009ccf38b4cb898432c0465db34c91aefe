I"O<p><em>This is a brief summary of paper <a href="https://arxiv.org/pdf/1702.08165.pdf">Reinforcement Learning with Deep Energy-Based Policies</a> for my personal interest.</em></p>

<ul id="markdown-toc">
  <li><a href="#0-introduction" id="markdown-toc-0-introduction">0. Introduction</a></li>
  <li><a href="#1-motivation" id="markdown-toc-1-motivation">1. Motivation</a></li>
  <li><a href="#2-soft-definition" id="markdown-toc-2-soft-definition">2. Soft Definition</a>    <ul>
      <li><a href="#21-objective-function" id="markdown-toc-21-objective-function">2.1. Objective Function</a></li>
      <li><a href="#22-soft-function" id="markdown-toc-22-soft-function">2.2. Soft Function</a></li>
    </ul>
  </li>
  <li><a href="#3-theorem-analyses" id="markdown-toc-3-theorem-analyses">3. Theorem Analyses</a>    <ul>
      <li><a href="#31-policy-improvement" id="markdown-toc-31-policy-improvement">3.1. Policy Improvement</a></li>
      <li><a href="#32-policy-iteration" id="markdown-toc-32-policy-iteration">3.2. Policy Iteration</a></li>
      <li><a href="#33-soft-bellman-equation" id="markdown-toc-33-soft-bellman-equation">3.3. Soft Bellman Equation</a></li>
      <li><a href="#34-soft-value-iteration" id="markdown-toc-34-soft-value-iteration">3.4. Soft Value Iteration</a></li>
    </ul>
  </li>
  <li><a href="#4-algorithm" id="markdown-toc-4-algorithm">4. Algorithm</a></li>
  <li><a href="#4-conclusion" id="markdown-toc-4-conclusion">4. Conclusion</a></li>
  <li><a href="#5-references" id="markdown-toc-5-references">5. References</a></li>
</ul>

<h1 id="0-introduction">0. Introduction</h1>

<p>After publishing the paper <a href="https://arxiv.org/pdf/1702.08165.pdf">soft Q learning</a> in Jul 2017, the author proposed the influential algorithm <a href="https://arxiv.org/pdf/1801.01290.pdf">SAC</a> in the next year. While SAC has received tremendous publicity, the discussion, if any, about this precedent work generally falls into the rut of <a href="https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/">the excellent blog</a>. Therefore I decided to summarize some of thoughts and problems I had during the reading. Though this post is mainly for my personal interest, any advice will be appreciated.</p>

<h1 id="1-motivation">1. Motivation</h1>

<p>In reinforcement learning (RL) problem, given an agent and an environment, the objective of the agent is to learn a policy that maximizes the rewards the agent can gain. However, faced with an unknown environment, there is a tradeoff between exploitation and exploration. Before maximum entropy RL, the exploration is generally ensured by external mechanisms, such as $\varepsilon-$greedy in DQN and adding exploratory noise to the actions in DDPG. Un like those heuristic and inefficient methods, maximum entropy encourages the agent to conduct exploration by itself based on both reward and entropy. Specifically, in maximum entropy RL, the optimal policy is redefined as</p>

\[\pi^\ast_\text{MaxEnt}=\arg\max_\pi\mathbb{E}_{\pi}\left[r(s_t,a_t)+\mathcal{H}(\pi(\cdot\vert s_t))\right],\tag{1}\]

<p>which adds a regularization term to the standard definition. Many paper adds a parameter $\alpha$ to entropy, which we will ignore in the following discussion as it does not affect the related conclusion. Based on this redefinition, the motivation of this paper can be summarized as follows:</p>

<ul>
  <li>
    <p>Generalize it to continuous cases:</p>

    <p>Before this work, entropy maximization was mainly utilized in discrete cases. Theoretical analyses were needed for applying into continuous cases.</p>
  </li>
  <li>
    <p>Take trajectory-wise entropy into consideration:</p>

    <p>The term $\mathcal{H}(\pi(\cdot\vert s_t))$ only considers the entropy of the policy at state $s_t$. Traditional methods tend to act greedily based on the entropy at the next state. In this work, the author considers the long term entropy reward instead that of the next state.</p>
  </li>
  <li>
    <p>Policy formulation:</p>

    <p>Even though we have an objective function given the definition, it still needs a probabilistic definition from which we can make sampling. Instead of using conditional Gaussian distribution like many other works, the author borrows the idea of Boltzmann distribution.</p>
  </li>
</ul>

<h1 id="2-soft-definition">2. Soft Definition</h1>

<p>We now discuss the related definition in this paper.</p>

<h2 id="21-objective-function">2.1. Objective Function</h2>

<p>The optimal policy in this paper is defined as</p>

\[\pi^\ast=\arg\max_\pi\sum_t\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}\left[r(s_t,a_t)+\mathcal{H}(\pi(\cdot\vert s_t))\right],\tag{2}\]

<p>which differs from the one defined in $(1)$ as it aims to reach states where they may have high entropy in the future. Specifically, a detailed version is given by</p>

\[\pi^\ast=\arg\max_\pi\sum_t\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}\left[\sum_{l=t}^\infty \gamma^{l-t}\mathbb{E}_{(s_l,a_l)\sim\rho_\pi}\left[r(s_l,a_l)+\mathcal{H}(\pi(\cdot\vert s_l))\right]\bigg\vert s_t,a_t\right],\tag{3}\]

<p>where the first expectation \(\sum_t\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}\) is over all the pairs \((s_t, a_t)\) at any time step, and the second expectation \(\sum_{l=t}^\infty \gamma^{l-t}\mathbb{E}_{(s_l,a_l)\sim\rho_\pi}\) is over all the trajectories originating from $(s_t,a_t)$.</p>

<blockquote>
  <p>In the original paper, the detailed version is actually given by</p>

\[\pi^\ast_\text{MaxEnt}=\arg\max_\pi\sum_t\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}\left[\sum_{l=t}^\infty \gamma^{l-t}\mathbb{E}_{(s_l,a_l)}\left[r(s_t,a_t)+\mathcal{H}(\pi(\cdot\vert s_t))\right]\bigg\vert s_t,a_t\right].\]

  <p>Problem: The subscripts (shown in red below) in the second expectation does confuse me a lot. Why it is over $t$ rather than $l$?</p>

\[r(s_{\color{red}t},a_{\color{red}t})+\mathcal{H}(\pi(\cdot\vert s_{\color{red}t}))?\]
</blockquote>

<h2 id="22-soft-function">2.2. Soft Function</h2>

<p>The corresponding $Q$-function in this paper is defined as</p>

\[Q_\text{soft}^\pi(s_t,a_t)\triangleq r(s_t,a_t)+\sum_{l=t+1}^\infty \gamma^{l-t}\mathbb{E}_{(s_{l},a_{l})\sim\rho_\pi}\left[r(s_{l},a_{l})+\mathcal{H}(\pi(\cdot\vert s_{l}))\right].\tag{4}\]

<p>Notice that the entropy at state $s_t$ was omitted in the definition.</p>

<p>The corresponding value function is given by</p>

\[V_\text{soft}^\pi(s_t)\triangleq \log \int_{\mathcal{A}}\exp\left(Q_\text{soft}^\pi(s_t,a)\right)da,\tag{5}\]

<p>which is actually in the form of log sum exponential that approximates maximum.</p>

<p>Given the two definitions, the soft policy is then given by</p>

\[\pi(a_t\vert s_t)=\exp\left(Q^\pi_\text{soft}(s_t,a_t)-V^\pi_\text{soft}(s_t)\right).\tag{6}\]

<p>As $V^\pi_\text{soft}$ only depends on $Q^\pi_\text{soft}$, the soft policy is actually the Boltzmann distribution based on the value of $Q^\pi_\text{soft}$. The comparison is shown in Figure 1. It can be found that Boltzmann distribution assigns a reasonable likelihood for all actions (rather the optimal one), which encourages a diverse exploration.</p>

<figure>
    <div style="display:flex">
            <figure>
<img src="http://bair.berkeley.edu/static/blog/softq/figure_3a_unimodal-policy.png" />
                <figcaption><center>(a)</center></figcaption>
            </figure>
            <figure>
<img src="http://bair.berkeley.edu/static/blog/softq/figure_3b_multimodal_policy.png" />
                <figcaption><center>(b)</center></figcaption>
            </figure>
    </div>
</figure>
<center>
<p style="font-size:80%;">
Figure 1. Policies based on the value of Q function. (a) Unimodal policy. (b) Multimodal policy.
  </p>
</center>
<p>With the those definitions, we have proposed the solution for the problems mentioned in the motivation: continuous function for continuous states and actions space; the trajectory-wise optimization defined by the objective function; and Boltzmann distribution to represent the optimal policy. To ensure things work, we need theoretical analyses and feasible update rules.</p>

<h1 id="3-theorem-analyses">3. Theorem Analyses</h1>

<p>We now discuss the related theorem guarantee. The first theorem shows us the optimality:</p>

<p><strong>Theorem 1</strong>: The optimal policy for equation (3) is given by</p>

\[\pi^\ast(a_t\vert s_t)=\exp\left(Q^\ast_\text{soft}(s_t,a_t)-V^\ast_\text{soft}(s_t)\right).\]

<p>The proof sketch follows two steps: policy improvement and policy iteration.</p>

<h2 id="31-policy-improvement">3.1. Policy Improvement</h2>

<p>We first show that given any policy, we can improve it by ‘softmizing’ it. Specifically,</p>

\[\forall\pi,\text{let }\tilde\pi(\cdot\vert s)\propto\exp\left(Q^\pi_\text{soft}(s,\cdot)\right),\text{then }Q^\pi_\text{soft}(s_t,a_t)\le Q^{\tilde\pi}_\text{soft}(s_t,a_t).\]

<p>To show that, we rewrite the second part of $Q$ function (defined in (4)) as</p>

\[\begin{aligned}&amp;\sum_{l=t+1}^\infty \gamma^{l-t}\mathbb{E}_{(s_{l},a_{l})\sim\rho_\pi}\left[r(s_{l},a_{l})+\mathcal{H}(\pi(\cdot\vert s_{l}))\right]\\=&amp;\mathbb{E}_{(s_{t+1},a_{t+1})\sim\rho_\pi}[[\gamma\mathcal{H}(\pi(\cdot\vert s_{l}))+\gamma r(s_{t+1},a_{t+1})\\&amp;+\sum_{l=t+2}^\infty \gamma^{l-t}\mathbb{E}_{(s_{l},a_{l})\sim\rho_\pi}[r(s_{l},a_{l})+\mathcal{H}(\pi(\cdot\vert s_{l}))]]\\=&amp;\mathbb{E}_{(s_{t+1},a_{t+1})\sim\rho_\pi}[\gamma\mathcal{H}(\pi(\cdot\vert s_{l}))+\gamma (r(s_{t+1},a_{t+1})\\&amp;+ \sum_{l=t+2}^\infty \gamma^{l-t-1}\mathbb{E}_{(s_{l},a_{l})\sim\rho_\pi}[r(s_{l},a_{l})+\mathcal{H}(\pi(\cdot\vert s_{l}))])]\\=&amp;\mathbb{E}_{(s_{t+1},a_{t+1})\sim\rho_\pi}[\gamma\mathcal{H}(\pi(\cdot\vert s_{t+1}))+\gamma Q^\pi_\text{soft}(s_{t+1},a_{t+1})].\end{aligned}\]

<p>As the entropy term is independent of $a_{t+1}$, we then have the following equation</p>

\[Q^\pi_\text{soft}(s_t,a_t)=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}[\mathcal{H}(\pi(\cdot\vert s_{t+1}))+\mathbb{E}_{a_{t+1}\sim\pi}\left[Q^\pi_\text{soft}(s_{t+1},a_{t+1})]\right].\tag{7}\]

<p>Given the equation, we then provide an inequality:</p>

\[\mathcal{H}(\pi(\cdot\vert s_{t}))+\mathbb{E}_{a_{t}\sim\pi}\left[Q^\pi_\text{soft}(s_{t},a_{t})\right]\le \mathcal{H}(\tilde\pi(\cdot\vert s_{t}))+\mathbb{E}_{a_{t}\sim\tilde\pi}\left[Q^\pi_\text{soft}(s_{t},a_{t})\right].\tag{8}\]

<p><em>Proof of (8):</em></p>

<p>We rewrite the left hand side of the inequality</p>

\[\begin{aligned}&amp;\mathcal{H}(\pi(\cdot\vert s_{t}))+\mathbb{E}_{a_{t}\sim\pi}\left[Q^\pi_\text{soft}(s_{t},a_{t})\right]\\=&amp;\mathbb{E}_{a_t\sim \pi}\left[{\color{red}{-\log\pi(a_t\vert s_t)}}+Q^\pi_\text{soft}(s_t,a_t)\right]\\=&amp;\mathbb{E}_{a_t\sim \pi}\left[-\log\pi(a_t\vert s_t){\color{red}{+\log\tilde{\pi}(a_t\vert s_t)-\log\tilde{\pi}(a_t\vert s_t)}}+Q^\pi_\text{soft}(s_t,a_t)\right]\\=&amp;-\mathbb{E}_{a_t\sim \pi}\left[\log\pi(a_t\vert s_t)-\log\tilde{\pi}(a_t\vert s_t)\right]+\mathbb{E}_{a_t\sim \pi}\left[Q^\pi_\text{soft}(s_t,a_t)-\log\tilde{\pi}(a_t\vert s_t)\right]\\=&amp;-D_\text{KL}(\pi\vert\vert\tilde{\pi})+\mathbb{E}_{a_t\sim\pi}\left[Q^\pi_\text{soft}(s_t,a_t)-Q^\pi_\text{soft}(s_t,a_t)+\log\int_\mathcal{A}\exp(Q^\pi_\text{soft}(s_t,a'))da'\right]\\=&amp;-D_\text{KL}(\pi\vert\vert\tilde{\pi})+\log\int_\mathcal{A}\exp(Q^\pi_\text{soft}(s_t,a'))da’.\end{aligned}\]

<p>For the right hand side of the inequality, we have</p>

\[\begin{aligned}&amp;\mathcal{H}(\tilde\pi(\cdot\vert s_{t}))+\mathbb{E}_{a_{t}\sim\tilde\pi}\left[Q^\pi_\text{soft}(s_{t},a_{t})\right]\\=&amp;\mathbb{E}_{a_t\sim \tilde\pi}\left[{\color{red}{-\log\tilde\pi(a_t\vert s_t)}}+Q^\pi_\text{soft}(s_t,a_t)\right]\\=&amp;\mathbb{E}_{a_t\sim \tilde\pi}\left[-\log\frac{\exp\left(Q^\pi_\text{soft}(s_t,a_t)\right)}{\int_\mathcal{A}\exp\left(Q^\pi_\text{soft}(s_t,a')\right)da'}+Q^\pi_\text{soft}(s_t,a_t)\right]\\=&amp;\mathbb{E}_{a_t\sim \tilde\pi}\left[-Q^\pi_\text{soft}(s_t,a_t)+\log\int_\mathcal{A}\exp\left(Q^\pi_\text{soft}(s_t,a')\right)da'+Q^\pi_\text{soft}(s_t,a_t)\right]\\=&amp;\log\int_\mathcal{A}\exp\left(Q^\pi_\text{soft}(s_t,a')\right)da’.\end{aligned}\]

<p>Since $D_\text{KL}\ge 0$, we have</p>

\[\mathcal{H}(\pi(\cdot\vert s_{t}))+\mathbb{E}_{a_{t}\sim\pi}\left[Q^\pi_\text{soft}(s_{t},a_{t})\right]\le \mathcal{H}(\tilde\pi(\cdot\vert s_{t}))+\mathbb{E}_{a_{t}\sim\tilde\pi}\left[Q^\pi_\text{soft}(s_{t},a_{t})\right].\tag*{$\blacksquare$}\]

<p>With (7) and (8), we now ready to show policy improvement. The idea is simple: we use the inequality (8) to contract the right hand side of equality (7) to complete the proof.</p>

<p><em>Proof of policy improvement:</em></p>

\[\begin{aligned}Q_\text{soft}^\pi(s_t,a_t)=&amp; r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}\left[\mathcal{H}(\pi(\cdot\vert s_{t+1}))+\mathbb{E}_{a_{t+1}\sim\pi}[Q_\text{soft}^\pi(s_{t+1},a_{t+1})]\right]\\\le&amp; r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}\left[\mathcal{H}({\color{red}{\tilde{\pi}}}(\cdot\vert s_{t+1}))+ \mathbb{E}_{a_{t+1}\sim{\color{red}{\tilde{\pi}}}}[Q^\pi_\text{soft}(s_{t+1},a_{t+1})]\right]\\=&amp;r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}[\mathcal{H}({\color{red}{\tilde{\pi}}}(\cdot\vert s_{t+1}))\\&amp;+ \mathbb{E}_{a_{t+1}\sim{\color{red}{\tilde{\pi}}}}[r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+2}\sim\mathcal{P}}[\mathcal{H}(\pi(\cdot\vert s_{t+2}))+\mathbb{E}_{a_{t+2}\sim\pi}[Q_\text{soft}^\pi(s_{t+2},a_{t+2})]]]\\\le &amp;r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}[\mathcal{H}({\color{red}{\tilde{\pi}}}(\cdot\vert s_{t+1}))\\&amp;+ \mathbb{E}_{a_{t+1}\sim{\color{red}{\tilde{\pi}}}}[r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+2}\sim\mathcal{P}}[\mathcal{H}({\color{red}{\tilde{\pi}}}(\cdot\vert s_{t+2}))+\mathbb{E}_{a_{t+2}\sim{\color{red}{\tilde{\pi}}}}[Q_\text{soft}^\pi(s_{t+2},a_{t+2})]]]\\ \vdots \\ \le &amp; r(s_t,a_t)+\sum_{l=t+1}^\infty \mathbb{E}_{(s_l,a_l)\sim\rho_{\tilde\pi}}\left[r(s_l,a_l)+\mathcal{H}(\tilde\pi(\cdot\vert s_l))\right]\\=&amp; Q^{\tilde\pi}_\text{soft}(s_t,a_t).\end{aligned}\]

<p>Therefore we complete the proof. \(\tag*{$\blacksquare$}\)</p>

<h2 id="32-policy-iteration">3.2. Policy Iteration</h2>

<p>With <em>policy improvement</em> theorem, we can improve any arbitrary policy. Therefore the policy can be naturally updated by</p>

\[\pi_{i+1}(\cdot \vert s_t)\propto \exp\left(Q^{\pi_i}_\text{soft}(s_t,\cdot)\right).\]

<p>Since any policy can be improved in this way, the optimal policy must satisfy this form, and the proof of <em>Theorem 1</em> is completed. \(\tag*{$\blacksquare$}\)</p>

<h2 id="33-soft-bellman-equation">3.3. Soft Bellman Equation</h2>

<p><strong>Theorem 2.</strong> The soft $Q$ function defined in (4) satisfies the soft Bellman equation</p>

\[Q^\ast_\text{soft}(s_t,a_t)=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}\left[V^\ast_\text{soft}(s_{t+1})\right].\]

<p><em>Proof of Theorem 2</em>:</p>

<p>The proof is pretty straightforward. Notice that</p>

\[\begin{aligned}&amp;\mathcal{H}(\pi(\cdot\vert s_{t+1}))+\mathbb{E}_{a_{t+1}\sim\pi}[Q_\text{soft}^\pi(s_{t+1},a_{t+1})]\\
=&amp;\mathbb{E}_{a_{t+1}\sim\pi}[-\log \pi(a_{t+1}\vert s_{t+1})+Q_\text{soft}^\pi(s_{t+1},a_{t+1})]\\
=&amp;\mathbb{E}_{a_{t+1}\sim\pi}[-\log \exp(Q_\text{soft}^\pi(s_{t+1},a_{t+1})-V_\text{soft}^\pi(s_{t+1}))+Q_\text{soft}^\pi(s_{t+1},a_{t+1})]\\
=&amp;\mathbb{E}_{a_{t+1}\sim\pi}[V_\text{soft}^\pi(s_{t+1})]\\
=&amp;V_\text{soft}^\pi(s_{t+1}).\end{aligned}\]

<p>Therefore the soft $Q$ function defined in (4) is equivalent to</p>

\[\begin{aligned}Q_\text{soft}^\pi(s_t,a_t)&amp;= r(s_t,a_t)+\sum_{l=t+1}^\infty \gamma^{l-t}\mathbb{E}_{(s_{l},a_{l})\sim\rho_\pi}\left[r(s_{l},a_{l})+\mathcal{H}(\pi(\cdot\vert s_{l}))\right]\\&amp;=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}\left[V^\pi_\text{soft}(s_{t+1})\right].\end{aligned}\]

\[\tag*{$\blacksquare$}\]

<h2 id="34-soft-value-iteration">3.4. Soft Value Iteration</h2>

<p>So far we have shown the optimality of soft policy (<em>Theorem 1</em>) and soft $Q$ function <em>(Theorem 2)</em>. However, we   still need a rule to learn the function. Specifically, we mainly focus on the update rule of $Q$ function as the policy and value function both are defined by $Q$ function. To this end, the author provides the following theorem.</p>

<p><strong>Theorem 3.</strong> The iteration</p>

\[Q^\pi_\text{soft}(s_t,a_t)\gets r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim\mathcal{P}}\left[V^\pi_\text{soft}(s_{t+1})\right],\]

\[V_\text{soft}^\pi(s_t)\gets \log \int_{\mathcal{A}}\exp\left(Q_\text{soft}^\pi(s_t,a)\right)da,\]

<p>converges to \(Q^\ast_\text{soft}\) and \(V^\ast_\text{soft}\), respectively.</p>

<p>The proof is quite similar to the general case in RL. For the detailed proof one can refer to the paper directly.</p>

<h1 id="4-algorithm">4. Algorithm</h1>

<p>Given the above analyses, there are two key issues in designing a practical algorithm:</p>

<ul>
  <li>The intractable integral for computing the value of $V^\pi_\text{soft}$;</li>
  <li>The intractable sampling from Boltzmann distribution.</li>
</ul>

<p>To deal with the integral issue, this paper leverages <em>importance sampling</em>, which has been widely used in many previous works. For the second issue, generally speaking, the author uses a neural network to approximate the policy, and the loss function is defined as</p>

\[J_\pi(\phi;s_t)=D_\text{KL}\left(\pi^\phi(a_t\vert s_t)\bigg\vert\bigg\vert\exp\left(Q^\theta_\text{soft}(s_t,a_t)-V^\theta_\text{soft}(s_t)\right)\right).\]

<p>The gradient of the loss function is given by <em>Stein variational gradient descent</em> (SVGD). In my view, the use of SVGD is mainly for the analysis of the resemblance between the proposed algorithm, soft Q learning (SQL), and actor-critic, as the succeeding works seem to use no SVGD anymore.</p>

<p>The author provides the implementation in <a href="https://github.com/haarnoja/softqlearning">github-softqlearning</a>. However, the latest version is faced with dependencies issue. Luckily, the older version (commits on Oct 30, 2017) works well. Other feasible implementation can be hardly found. For the performance, I tested it on Multigoal environment and the results are consistent with that of the original paper. Besides, I conducted experiments with varying values of $\alpha$, which is shown in Figure 2.</p>

<figure>
    <div style="display:flex">
            <figure>
<img src="../../../../assets/images/sql_alpha0_500.png" />
                <figcaption><center>$\alpha=0$</center></figcaption>
            </figure>
            <figure>
<img src="../../../../assets/images/sql_alpha05_500.png" />
                <figcaption><center>$\alpha=0.5$</center></figcaption>
            </figure>
            <figure>
<img src="../../../../assets/images/sql_alpha1_500.png" />
                <figcaption><center>$\alpha=1$</center></figcaption>
            </figure>
    </div>
</figure>
<center>
<p style="font-size:80%;">
Figure 2. After 500 steps, the performance of SQL in Multigoal with differnt values of $\alpha$.
  </p>
</center>

<p>Generally speaking, when $\alpha\ne 0$, SQL tends to have a high variance, which can also be viewed as the cost for exploration.</p>

<h1 id="4-conclusion">4. Conclusion</h1>

<p>It was proven that bringing in the entropy term does work for the end of better exploring. The author successfully extended the entropy RL to contiunous case. However, the high variance makes it challenging to be used for performing complicated tasks, and that may be one of the reasons why little (relatively) ink has been spilled on SQL. A wise choice could be to use SQL as an initializer rather than a trainer.</p>

<h1 id="5-references">5. References</h1>

<p><a href="https://arxiv.org/pdf/1702.08165.pdf">Reinforcement Learning with Deep Energy-Based Policies</a> - Tuomas Haarnoja et al.</p>

<p><a href="https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/">Learning Diverse Skills via Maximum Entropy Deep Reinforcement Learning</a> - BAIR</p>

<p><a href="https://julien-vitay.net/deeprl/EntropyRL.html">Deep Reinforcement Learning</a> - Julien Vitay</p>

<p><a href="https://www.slideshare.net/DongMinLee32/maximum-entropy-reinforcement-learning-stochastic-control">Maximum Entropy Reinforcement Learning (Stochastic Control)</a> - Dongmin Lee</p>

:ET