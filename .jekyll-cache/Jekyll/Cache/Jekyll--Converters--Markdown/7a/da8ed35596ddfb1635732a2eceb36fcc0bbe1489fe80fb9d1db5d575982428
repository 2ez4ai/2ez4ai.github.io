I"¤#<p><em>This is a summary of paper <a href="https://arxiv.org/pdf/1702.08165.pdf">Reinforcement Learning with Deep Energy-Based Policies</a> for my personal interest.</em></p>

<ul id="markdown-toc">
  <li><a href="#0-introduction" id="markdown-toc-0-introduction">0. Introduction</a></li>
  <li><a href="#1-motivation" id="markdown-toc-1-motivation">1. Motivation</a></li>
  <li><a href="#2-soft-policy" id="markdown-toc-2-soft-policy">2. Soft Policy</a>    <ul>
      <li><a href="#21-message" id="markdown-toc-21-message">2.1. Message</a></li>
      <li><a href="#22-sum-product" id="markdown-toc-22-sum-product">2.2. Sum-Product</a></li>
      <li><a href="#23-max-product" id="markdown-toc-23-max-product">2.3 Max-Product</a></li>
    </ul>
  </li>
  <li><a href="#3-conclusion" id="markdown-toc-3-conclusion">3. Conclusion</a></li>
</ul>

<h1 id="0-introduction">0. Introduction</h1>

<p>After publishing the paper <a href="https://arxiv.org/pdf/1702.08165.pdf">soft Q learning</a> in Jul 2017, the author proposed the influential algorithm <a href="https://arxiv.org/pdf/1801.01290.pdf">SAC</a> in the next year. While SAC has received tremendous publicity, the discussion, if any, about this precedent work was generally got into the rut of <a href="https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/">the excellent blog</a>. Therefore I decided to summarize some of thoughts and problems I had during the reading. Though this post is mainly for my personal interest, any advice will be appreciated.</p>

<h1 id="1-motivation">1. Motivation</h1>

<p>In reinforcement learning (RL) problem, given an agent and an environment, the objective of the agent is to learn a policy that maximizes the rewrads the agent can gain. However, faced with an unknown environment, there is a tradeoff between exploitation and exploration. Before maximum entropy RL, the exploration is generally ensured by external machanisms, such as $\varepsilon-$greedy in DQN and adding exploratory noise to the actions in DDPG. Un like those heuristic and inefficient methods, maximum entropy encourages the agent to conduct exploration by itself based on both reward and entropy. Specifically, in maximum entorpy RL, the optimal policy is redefined as</p>

\[\pi^\ast_\text{MaxEnt}=\arg\max_\pi\mathbb{E}_{\pi}\left[r(s_t,a_t)+\mathcal{H}(\pi(\cdot\vert s_t))\right],\tag{1}\]

<p>which adds a regularization term to the standard definition. Based on this redefinition, the motivation of this paper can be summarized as follows:</p>

<ul>
  <li>
    <p>Generalize it to continuous cases:</p>

    <p>Before this work, entropy maximization was mainly utilized in discrete cases. Theorectical analyses were needed for applying into continuous cases.</p>
  </li>
  <li>
    <p>Take trajectory-wise entropy into consideration:</p>

    <p>The term $\mathcal{H}(\pi(\cdot\vert s_t))$ only considers the entropy of the policy at state $s_t$. Traditional methods tend to act greedily based on the entropy at the next state. In this work, the author considers the long term entropy reward instead that of the next state.</p>
  </li>
  <li>
    <p>Policy formulation:</p>

    <p>Even though we have an objective function given the definition, it still needs a probabilistic definition from which we can make sampling. Instead of using conditional Gaussian distribution like many other works, the author borrows the idea of Boltzmann distribution.</p>
  </li>
</ul>

<h1 id="2-soft-policy">2. Soft Policy</h1>

<p>The optimal policy in this paper is defined as</p>

\[\pi^\ast_\text{MaxEnt}=\arg\max_\pi\sum_t\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}\left[r(s_t,a_t)+\mathcal{H}(\pi(\cdot\vert s_t))\right],\tag{2}\]

<p>which differs from the one defined in $(2)$ as it aims to reach states where they will have high entropy in the future. Specifically, a detailed version is given by</p>

\[\pi^\ast_\text{MaxEnt}=\arg\max_\pi\sum_t\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}\left[\sum_{l=t}^\infty \gamma^{l-t}\mathbb{E}_{(s_l,a_l)\sim\rho_\pi}\left[r(s_l,a_l)+\mathcal{H}(\pi(\cdot\vert s_l))\right]\bigg\vert s_t,a_t\right],\tag{3}\]

<p>where the first expectation \(\sum_t\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}\) is over all the pairs \((s_t, a_t)\) at any time step $t$, and the second expectation $\sum_{l=t}^\infty \gamma^{l-t}\mathbb{E}<em>{(s_l,a_l)\sim\rho</em>\pi}$ is over all the trajectories originating from $(s_t,a_t)$.</p>

<blockquote>
  <p>In the original paper, the detailed version is actually given by</p>

\[\pi^\ast_\text{MaxEnt}=\arg\max_\pi\sum_t\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}\left[\sum_{l=t}^\infty \gamma^{l-t}\mathbb{E}_{(s_l,a_l)}\left[r(s_t,a_t)+\mathcal{H}(\pi(\cdot\vert s_t))\right]\bigg\vert s_t,a_t\right].\]

  <p>Problem 1: The subscripts (shown in red below) in the second expectation is kind of tricky:</p>

\[r(s_{\color{red}t},a_{\color{red}t})+\mathcal{H}(\pi(\cdot\vert s_{\color{red}t})).\]
</blockquote>

<h2 id="21-message">2.1. Message</h2>

<p>For a tree graph, its maximum cliques contains only two nodes. By VE algorithm, to compute the marginal $P(x_i)$, we need to eliminate all nodes that are in the subtree of $x_i$. For node $x_j$, the elimination involves computing $\sum_{x_j}\psi_{x_j,x_k}(x_j,x_k)m_{j,k}$ where $x_k$ is the parent of $x_j$ in the tree. The term $m_{j,k}$ can be thought of a <em>message</em> that $x_j$ sends to $x_k$ about the subtree rooted at $x_j$. Similarly, the computing result can be viewed as</p>

\[m_{k,l}=\sum_{x_j}\psi_{x_j,x_k}(x_j,x_k)m_{j,k}\]

<p>that contains the information for $x_l$, the parent of $x_k$, about the subtree rooted at $x_k$. By doing so, at the end of VE, $x_i$ would receive messages from all of its immediate children and then marginalize them out to yield the final marginal.</p>

<p>Suppose that after computing $P(x_i)$, we are interested in computing $P(x_k)$ as well. If we use VE algorithm again, we can find that the computation also involves the messages $m_{j,k}$ as node $x_k$ is still the parent of node $x_j$. Moreover, such a message is exactly the same as the one used in computing $P(x_i)$ since the graph structure does not change. Therefore, it is easy to find that if we store the intermediary messages of VE, we can obtain other marginals quickly.</p>

<h2 id="22-sum-product">2.2. Sum-Product</h2>

<p>Belief propagation can be viewed as a combination of VE and <em>caching</em>. For each edge between $x_i$ and $x_j$, the messages passing on it are $m_{i,j}$ and $m_{j,i}$, which depends on the marginal we want to determine. After computing all these messages, one can compute any marginals with these messages.</p>

<p>Belief propagation:</p>

<ul>
  <li>Set a node, for example, node $x_i$, as the root;</li>
  <li>For each $x_j$ in $N(x_i)$, <em>i.e.,</em> the neighborhood of $x_i$, collect the messages sent to $x_i$:</li>
</ul>

\[m_{j,i}=\sum_{x_j}\psi_{x_i,x_j}(x_i,x_j)\psi_{x_j}(x_j)\prod_{k\in N(x_j)\setminus i}m_{k,j};\]

<ul>
  <li>For each $x_j$ in $N(x_i)$, collect the messages sent from $x_i$:</li>
</ul>

\[m_{i,j}=\sum_{x_i}\psi_{x_j,x_i}(x_j,x_i)\psi_{x_i}(x_i)\prod_{k\in N(x_i)\setminus j}m_{k,i}.\]

<p>By doing so, we can obtain all the messages with \(2\vert E\vert\) steps, where $E$ is the set of edges. Then for any marginal we have</p>

\[P(x_i)=\psi_i(x_i)\prod_{k\in N(x_i)}m_{k,i}.\]

<h2 id="23-max-product">2.3 Max-Product</h2>

<p>We now consider a problem of finding the set of values that have the largest probability so that</p>

\[\hat{\text{x}}=\arg\max_{\text{x}} P(\text{x}).\]

<p>Notice that</p>

\[\max_{\text{x}} P(\text{x})=\max_{\text{x}_1}\dots \max_{\text{x}_n}P(\text{x}).\]

<p>By <em>sum-product</em>, we have</p>

\[\begin{aligned}\max_{\text{x}} P(\text{x})&amp;=\max_{x_1}\dots \max_{x_n}\psi_i(x_i)\prod_{k\in N(x_i)}m_{k,i}\\&amp;=\max_{x_n}\max_{x_n-1{}}\psi_{x_n,x_{n-1}}(x_n,x_{n-1})\max_{x_{n-2}}\psi_{x_{n-1},x_{n-2}}(x_{n-1},x_{n-2})\\&amp;\quad\ \dots\max_{x_1}\psi_{x_2,x_1}(x_2,x_1)\psi_{x_1}(x_1).\end{aligned}\]

<p>Such a method for maximizing <em>max-product</em> is known as <em>max-product</em> algorithm.</p>

<h1 id="3-conclusion">3. Conclusion</h1>

<p>In this post, we briefly introduced two algorithms for <em>exact inference</em> in graphical models. Given a proper order of nodes, <em>variable elimination</em> algorithm is efficient. However, the finding of the proper order is an NP-hard problem. Besides, each query of marginals needs running the algorithm, during which the computation can be highly redundant. To improve computing efficiency, <em>belief propagation</em> stores the intermediate results as messages. After that, one can get any marginal by the messages. Moreover, we can also exploit those messages to determine the values of random variables with the largest probability, which is known as <em>max-product</em>.</p>

<ol>
  <li>
    <p>References</p>

    <p>Learning Diverse Skills via Maximum Entropy Deep Reinforcement Learning - BAIR https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/</p>

    <p>Deep Reinforcement Learning - Julien Vitay https://julien-vitay.net/deeprl/EntropyRL.html</p>
  </li>
</ol>

:ET