I"¼<p><em>This is a summary of paper <a href="https://arxiv.org/pdf/1702.08165.pdf">Reinforcement Learning with Deep Energy-Based Policies</a> for my personal interest.</em></p>

<ul id="markdown-toc">
  <li><a href="#0-introduction" id="markdown-toc-0-introduction">0. Introduction</a></li>
  <li><a href="#1-motivation" id="markdown-toc-1-motivation">1. Motivation</a></li>
  <li><a href="#2-belief-propagation" id="markdown-toc-2-belief-propagation">2. Belief Propagation</a>    <ul>
      <li><a href="#21-message" id="markdown-toc-21-message">2.1. Message</a></li>
      <li><a href="#22-sum-product" id="markdown-toc-22-sum-product">2.2. Sum-Product</a></li>
      <li><a href="#23-max-product" id="markdown-toc-23-max-product">2.3 Max-Product</a></li>
    </ul>
  </li>
  <li><a href="#3-conclusion" id="markdown-toc-3-conclusion">3. Conclusion</a></li>
</ul>

<h1 id="0-introduction">0. Introduction</h1>

<p>After publishing the paper <a href="https://arxiv.org/pdf/1702.08165.pdf">soft Q learning</a> in Jul 2017, the author proposed the influential algorithm <a href="https://arxiv.org/pdf/1801.01290.pdf">SAC</a> in the next year. While SAC has received tremendous publicity, the discussion, if any, about this precedent work was generally got into the rut of <a href="https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/">the excellent blog</a>. Therefore I decided to summarize some of thoughts and problems I had during the reading. Though this post is mainly for my personal interest, any advice will be appreciated.</p>

<h1 id="1-motivation">1. Motivation</h1>

<p>In reinforcement learning (RL) problem, given an agent and an environment, the objective of the agent is to learn a policy that maximizes the rewrads the agent can gain. However, faced with an unknown environment, there is a tradeoff between exploitation and exploration. Before maximum entropy RL, the exploration is generally ensured by external machanisms, such as $\varepsilon-$greedy in DQN and adding exploratory noise to the actions in DDPG. Un like those heuristic and inefficient methods, maximum entropy encourages the agent to conduct exploration by itself based on both reward and entropy. Specifically, the optimal policy in a traditional reinforcement learning is given by</p>

\[\pi^\ast_\text{std}=\arg\max_\pi\sum_t\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}\left[r(s_t,a_t)\right].\]

<p>In the maximum entorpy RL, the optimal policy is redefined as</p>

\[\pi^\ast_\text{MaxEnt}=\arg\max_\pi\sum_t\mathbb{E}_{(s_t,a_t)\sim\rho_\pi}\left[r(s_t,a_t)+\mathcal{H}(\pi(\cdot\vert s_t))\right],\]

<p>which adds a regularization term to the standard definition. This paper is the first one to</p>

<h1 id="2-belief-propagation">2. Belief Propagation</h1>

<p>For convenience, we consider undirected graphs with tree structure, where the optimal variable elimination ordering for node $x_i$ is the post-order iteration of the subtree rooted at $x_i$. The relationship between any two directly connected nodes is decided by which node the tree is rooted at and how far the two nodes are away from the root: the close one is the parent of the farther one.</p>

<h2 id="21-message">2.1. Message</h2>

<p>For a tree graph, its maximum cliques contains only two nodes. By VE algorithm, to compute the marginal $P(x_i)$, we need to eliminate all nodes that are in the subtree of $x_i$. For node $x_j$, the elimination involves computing $\sum_{x_j}\psi_{x_j,x_k}(x_j,x_k)m_{j,k}$ where $x_k$ is the parent of $x_j$ in the tree. The term $m_{j,k}$ can be thought of a <em>message</em> that $x_j$ sends to $x_k$ about the subtree rooted at $x_j$. Similarly, the computing result can be viewed as</p>

\[m_{k,l}=\sum_{x_j}\psi_{x_j,x_k}(x_j,x_k)m_{j,k}\]

<p>that contains the information for $x_l$, the parent of $x_k$, about the subtree rooted at $x_k$. By doing so, at the end of VE, $x_i$ would receive messages from all of its immediate children and then marginalize them out to yield the final marginal.</p>

<p>Suppose that after computing $P(x_i)$, we are interested in computing $P(x_k)$ as well. If we use VE algorithm again, we can find that the computation also involves the messages $m_{j,k}$ as node $x_k$ is still the parent of node $x_j$. Moreover, such a message is exactly the same as the one used in computing $P(x_i)$ since the graph structure does not change. Therefore, it is easy to find that if we store the intermediary messages of VE, we can obtain other marginals quickly.</p>

<h2 id="22-sum-product">2.2. Sum-Product</h2>

<p>Belief propagation can be viewed as a combination of VE and <em>caching</em>. For each edge between $x_i$ and $x_j$, the messages passing on it are $m_{i,j}$ and $m_{j,i}$, which depends on the marginal we want to determine. After computing all these messages, one can compute any marginals with these messages.</p>

<p>Belief propagation:</p>

<ul>
  <li>Set a node, for example, node $x_i$, as the root;</li>
  <li>For each $x_j$ in $N(x_i)$, <em>i.e.,</em> the neighborhood of $x_i$, collect the messages sent to $x_i$:</li>
</ul>

\[m_{j,i}=\sum_{x_j}\psi_{x_i,x_j}(x_i,x_j)\psi_{x_j}(x_j)\prod_{k\in N(x_j)\setminus i}m_{k,j};\]

<ul>
  <li>For each $x_j$ in $N(x_i)$, collect the messages sent from $x_i$:</li>
</ul>

\[m_{i,j}=\sum_{x_i}\psi_{x_j,x_i}(x_j,x_i)\psi_{x_i}(x_i)\prod_{k\in N(x_i)\setminus j}m_{k,i}.\]

<p>By doing so, we can obtain all the messages with \(2\vert E\vert\) steps, where $E$ is the set of edges. Then for any marginal we have</p>

\[P(x_i)=\psi_i(x_i)\prod_{k\in N(x_i)}m_{k,i}.\]

<h2 id="23-max-product">2.3 Max-Product</h2>

<p>We now consider a problem of finding the set of values that have the largest probability so that</p>

\[\hat{\text{x}}=\arg\max_{\text{x}} P(\text{x}).\]

<p>Notice that</p>

\[\max_{\text{x}} P(\text{x})=\max_{\text{x}_1}\dots \max_{\text{x}_n}P(\text{x}).\]

<p>By <em>sum-product</em>, we have</p>

\[\begin{aligned}\max_{\text{x}} P(\text{x})&amp;=\max_{x_1}\dots \max_{x_n}\psi_i(x_i)\prod_{k\in N(x_i)}m_{k,i}\\&amp;=\max_{x_n}\max_{x_n-1{}}\psi_{x_n,x_{n-1}}(x_n,x_{n-1})\max_{x_{n-2}}\psi_{x_{n-1},x_{n-2}}(x_{n-1},x_{n-2})\\&amp;\quad\ \dots\max_{x_1}\psi_{x_2,x_1}(x_2,x_1)\psi_{x_1}(x_1).\end{aligned}\]

<p>Such a method for maximizing <em>max-product</em> is known as <em>max-product</em> algorithm.</p>

<h1 id="3-conclusion">3. Conclusion</h1>

<p>In this post, we briefly introduced two algorithms for <em>exact inference</em> in graphical models. Given a proper order of nodes, <em>variable elimination</em> algorithm is efficient. However, the finding of the proper order is an NP-hard problem. Besides, each query of marginals needs running the algorithm, during which the computation can be highly redundant. To improve computing efficiency, <em>belief propagation</em> stores the intermediate results as messages. After that, one can get any marginal by the messages. Moreover, we can also exploit those messages to determine the values of random variables with the largest probability, which is known as <em>max-product</em>.</p>

<ol>
  <li>
    <p>References</p>

    <p>Learning Diverse Skills via Maximum Entropy Deep Reinforcement Learning - BAIR https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/</p>

    <p>Deep Reinforcement Learning - Julien Vitay https://julien-vitay.net/deeprl/EntropyRL.html</p>
  </li>
</ol>

:ET