I"úX<p><em>The notes are based on the <a href="https://github.com/shuhuai007/Machine-Learning-Session">session</a>. For the fundamental of linear algebra, one can always refer to <a href="http://math.mit.edu/~gs/linearalgebra/">Introduction to Linear Algebra</a> and <a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">The Matrix Cookbook</a> for more details. Many thanks to these great works.</em></p>

<ul id="markdown-toc">
  <li><a href="#0-introduction" id="markdown-toc-0-introduction">0. Introduction</a></li>
  <li><a href="#1-perceptron" id="markdown-toc-1-perceptron">1. Perceptron</a></li>
  <li><a href="#2-linear-discriminant-analysis" id="markdown-toc-2-linear-discriminant-analysis">2. Linear Discriminant Analysis</a></li>
  <li><a href="#3-discriminant-classifiers" id="markdown-toc-3-discriminant-classifiers">3. Discriminant Classifiers</a>    <ul>
      <li><a href="#31-logistic-regression" id="markdown-toc-31-logistic-regression">3.1. Logistic regression</a></li>
    </ul>
  </li>
  <li><a href="#4-generative-classifiers" id="markdown-toc-4-generative-classifiers">4. Generative Classifiers</a>    <ul>
      <li><a href="#41-naive-bayes-classifier" id="markdown-toc-41-naive-bayes-classifier">4.1. Naive Bayes classifier</a></li>
      <li><a href="#42-gaussian-discriminant-analysis" id="markdown-toc-42-gaussian-discriminant-analysis">4.2. Gaussian discriminant analysis</a></li>
    </ul>
  </li>
  <li><a href="#5-conclusion" id="markdown-toc-5-conclusion">5. Conclusion</a></li>
</ul>
<h1 id="0-introduction">0. Introduction</h1>

<p><em>The following introduction is derived from the <a href="http://pages.stat.wisc.edu/~wahba/stat860public/pdf1/liu.zhang.wu.lum.11.pdf">paper</a>.</em></p>

<p>As a supervised learning technique, the goal of classification is to construct a classification rule based on a training set where both data and class labels are given. Once obtained, the classification rule can then be used for class prediction of new objects whose covariates are available.</p>

<p>Among various classification methods, there are two main groups: <em>soft</em> and <em>hard</em> classification. In particular, a soft classification rule generally estimates the class <em>conditional probabilities</em> explicitly and then makes the class prediction <em>based on the estimated probability</em>. Depending on whether calculating the conditional probability directly or approximating it by a model, there are <em>generative classifiers</em> and <em>discriminant classifiers</em> among the <em>soft</em> methods. In contrast, hard classification bypasses the requirement of class probability estimation and directly estimates the <em>classification boundary</em>.</p>

<p>Typical soft classifiers include some traditional distribution-based likelihood approaches such as logistic regression. On the other hand, some margin-based approaches such as perceptron and the SVM, generally distributional assumption-free, belong to the class of hard classification methods.</p>

<p>We assume the data set is linearly separable in the following subsections.</p>

<h1 id="1-perceptron">1. Perceptron</h1>

<p>Perceptron is a <em>hard</em> method for <em>binary classification</em>. Suppose we have i.i.d. data <script type="math/tex">\mathcal{D}=\{(x_1,y_1), (x_2,y_2),\dots,(x_N,y_N)\},X=\{x_1,x_2,\dots,x_N\}, Y=\{y_1,y_2,\dots,y_N\}</script> where $x_i\in\mathbb{R}^{d\times 1}$ can be viewed as the feature and <script type="math/tex">y_i\in\{-1,1\}</script> is the corresponding label. In particular, we denote <script type="math/tex">X_{c1}=\{x_i\vert y_i=+1\}</script> and <script type="math/tex">X_{c2}=\{x_i\vert y_i=-1\}</script> as the set of class $c_1$ and class $c_2$, respectively. Moreover, let $N_1=| X_{c1}|$ and $N_2=|X_{c2}|$, where $N_1+N_2=N$. The model of perceptron follows</p>

<script type="math/tex; mode=display">f(w)=\text{sign}(w^Tx),</script>

<p>where $w\in\mathbb{R}^{d\times 1}$ and $\text{sign}(\cdot)$ is the sign function. Perceptron is actually an error-driven method. Specifically, for data $(x_i,y_i)$, the correctness of perceptron can be described as</p>

<script type="math/tex; mode=display">% <![CDATA[
y_iw^Tx_i\ge0\iff\begin{cases}w^Tx_i\ge0,\ f(w)=1,&\text{if } y_i=+1\\w^Tx_i<0,\ f(w)=-1, &\text{if }y_i=-1\end{cases} %]]></script>

<p>Define <script type="math/tex">% <![CDATA[
\tilde D=\{(x,y)\vert y_iw^T x_i<0, i=1,\dots,N\} %]]></script> be the set of data that was classified incorrectly. Then the loss function of the model can be defined as the size of $\tilde D$:</p>

<script type="math/tex; mode=display">% <![CDATA[
\mathcal{L}(w)=\sum_{i=1}^NI(y_iw^Tx_i<0), %]]></script>

<p>where $I(\cdot)$ is the indicator function. Though such a loss function is intuitive, it is uncontinuous and can be hard to be optimized. From the standpoint of the model, to make $y_iw^Tx_i\ge0$ is equivalent to make $y_iw^Tx_i$ as larger as possible, thus we can transform the loss function into</p>

<script type="math/tex; mode=display">\mathcal{L}(w)=\sum_{i=1}^N-y_iw^Tx_i,</script>

<p>which can be minimized by various optimization methods such as <em>stochastic gradient descent</em>.</p>

<h1 id="2-linear-discriminant-analysis">2. Linear Discriminant Analysis</h1>

<p>Now we introduce <em>linear discriminant analysis</em> (LDA), which is a method for <em>binary classification</em>. Note that in some materials, LDA is defined as a dimensionality reduction technique. Further, we introduce LDA method  in this note from a hard classification perspective. The soft perspective can be also found in other materials. The notations for data in this subsection are the same as that in subsection 1. We further define the mean</p>

<script type="math/tex; mode=display">\bar x_{c1}=\frac{1}{N_1}\sum_{x\in X_{c1}}x,\quad \bar x_{c2}=\frac{1}{N_2}\sum_{x\in X_{c2}}x,</script>

<p>and the variance</p>

<script type="math/tex; mode=display">S_{c1}=\frac{1}{N_1}\sum_{x\in X_{c1}}(x-\bar x_{c1})(x-\bar x_{c1})^T,\quad S_{c2}=\frac{1}{N_2}\sum_{x\in X_{c2}}(x-\bar x_{c2})(x-\bar x_{c2})^T.</script>

<blockquote>
  <p>The idea of LDA is proposed by Ronald Fisher in 1988: maximize the distance between the mean of each class and minimize the spreading within the class itself.</p>

</blockquote>

<p>In LDA, we consider the â€˜<em>projection</em>â€™ of $x$:</p>

<script type="math/tex; mode=display">z=w^Tx,</script>

<p>where $w\in\mathbb{R}^{d\times 1}$ is a unit vector to be learned. Specifically, the scalar $z$ is the length of the projection of $x$ on $w$, thus we can view such $z$ as the projection of $x$ into a <em>one dimensional subspace</em>. Note that the definition here is different from the definition of projection in <em>Introduction to Linear Algebra</em>.</p>

<p>Then we have the following definitions about the <em>mean</em>,</p>

<script type="math/tex; mode=display">\bar z =\frac{1}{N}\sum_{i=1}^Nw^Tx_i,\quad \bar z_1=\frac{1}{N_1}\sum_{x\in X_{c1}}w^Tx,\quad \bar z_2=\frac{1}{N_2}\sum_{x\in X_{c2}}w^Tx.</script>

<p>Similarly, we have the definitions related to the variance as</p>

<script type="math/tex; mode=display">S=\frac{1}{N}\sum_{i=1}^{N}(w^Tx_i-\bar z)^2,\quad S_1=\frac{1}{N_1}\sum_{x\in X_{c1}}(w^Tx-\bar z_1)^2,\quad S_2=\frac{1}{N_2}\sum_{x\in X_{c2}}(w^Tx-\bar z_2)^2.</script>

<p>Then we use the mean to define the distance between the two class and the variance to represent the spreading within the class itself. LDA is then to find the unit vector $\hat w$ that maximizes</p>

<script type="math/tex; mode=display">\mathcal{J}(w)=\frac{(\bar z_1-\bar z_2)^2}{S_1+S_2}.</script>

<p>For the numerator, it follows that</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}(\bar z_1-\bar z_2)^2&=\left(\frac{1}{N_1}\sum_{x\in X_{c1}}w^Tx_i-\frac{1}{N_2}\sum_{x\in X_{c2}}w^Tx_i\right)^2\\&=w^T\left(\bar x_{c1}-\bar x_{c_2}\right)\left(\bar x_{c1}-\bar x_{c_2}\right)^Tw\end{aligned}. %]]></script>

<p>For the denominator, it follows that</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}S_1+S_2&=\frac{1}{N_1}\sum_{x\in X_{c1}}(w^Tx-\bar z_1)^2+\frac{1}{N_2}\sum_{x\in X_{c2}}(w^Tx-\bar z_2)^2\\&=w^T\left[\frac{1}{N_1}\sum_{x\in X_{c1}}(x-\bar x_{c1})(x-\bar x_{c1})^T\right]w+w^T\left[\frac{1}{N_2}\sum_{x\in X_{c2}}(x-\bar x_{c2})(x-\bar x_{c2})^T\right]w\\&=w^TS_{c1}w+w^TS_{c2}w\\&=w^T(S_{c1}+S_{c2})w\end{aligned}. %]]></script>

<p>Therefore, we have</p>

<script type="math/tex; mode=display">\mathcal{J}(w)=\frac{w^TS_bw}{w^TS_ww},</script>

<p>where $S_b=\left(\bar x_{c1}-\bar x_{c_2}\right)\left(\bar x_{c1}-\bar x_{c_2}\right)^T$ represents the distance <em>between-class</em>, $S_w=S_{c1}+S_{c2}$ represents the spreading <em>within-class</em>. Such transformation is actually for computing derivation. $J(w)$ can be maximized by taking the derivative w.r.t $w$ and setting it to be $0$. Specifically,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\frac{\partial \mathcal{J}(w)}{\partial w}&=\frac{\left(\frac{\partial}{\partial w}w^TS_b w\right)w^TS_ww-w^TS_bw\left(\frac{\partial}{\partial w}w^TS_w w\right)}{(w^TS_ww)^2}\\&=\frac{(2S_bw)w^TS_ww-w^TS_bw(2S_ww)}{(w^TS_ww)^2}\end{aligned}. %]]></script>

<p>Setting it to be 0 is equivalent to</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}(2S_bw)w^TS_ww-w^TS_bw(2S_ww)&=0\\ (w^TS_bw)S_ww&=S_bw(w^TS_ww)\\S_w w&=\frac{w^TS_ww}{w^TS_bw}S_bw\end{aligned}. %]]></script>

<p>As $w\in\mathbb{R}^{d\times 1}$ and $S_w,S_b\in\mathbb{R}^{d\times d}$, the term $(w^TS_ww)/(w^TS_bw)\in\mathbb{R}$. For convenience, we denote it as $\lambda$. Then we have an equivalent <em>generalized eigenvalue problem</em></p>

<script type="math/tex; mode=display">S_ww=\lambda S_bw.</script>

<p>If one of $S_b$ and $S_w$ has full rank, the generalized eigenvalue problem can be converted into a standard eigenvalue problem. However, to solve the problem entails complex computation. We now assume $S_w^{-1}$ exists. Recall that $w$ is a unit vector. Thus what we need to care is only the direction of $w$:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\hat w&\propto \lambda S_w^{-1}S_bw\\&\propto \lambda S_w^{-1}\left(\bar x_{c1}-\bar x_{c_2}\right)\left(\bar x_{c1}-\bar x_{c_2}\right)^Tw\\&\propto\lambda_1S_w^{-1}\left(\bar x_{c1}-\bar x_{c_2}\right)\\&\propto S_w^{-1}\left(\bar x_{c1}-\bar x_{c_2}\right)\end{aligned}, %]]></script>

<p>where $\lambda_1=\lambda \left(\bar x_{c1}-\bar x_{c_2}\right)^Tw$ is a scalar as $\left(\bar x_{c1}-\bar x_{c_2}\right)^Tw\in\mathbb{R}$.</p>

<h1 id="3-discriminant-classifiers">3. Discriminant Classifiers</h1>

<p>Discriminant classifiers focus on the classification problem directly. Specifically, discriminant classifiers  model the posterior $P(Y\vert X)$, then makes the class prediction based on the estimated probability.</p>

<h2 id="31-logistic-regression">3.1. Logistic regression</h2>

<p>Logistic regression inputs the result of a <em>linear regression</em> to a <em>sigmoid function</em> to make classification. A sigmoid function is</p>

<script type="math/tex; mode=display">\sigma(z)=\frac{1}{1+e^{-z}}</script>

<p>which maps $z\in(-\infty,+\infty)$ into a probability $[0,1]$. Therefore we can model the posterior probability as</p>

<script type="math/tex; mode=display">P(y=1|x;w)=\sigma(w^Tx)=\frac{1}{1+e^{-w^Tx}},</script>

<p>where $w$ is the parameter to be learned. As for $P(y=-1\vert x;w)$, it can be obtained by $1-P(y=1\vert x;w)$ since we are considering a binary classification problem. However, for a supervised learning technique, it would be convenient to consider both two labels into one function. To this end, we set the label value to be <script type="math/tex">y_i\in\{0,1\}</script> . Moreover, we denote $P(y_i=1\vert x_i;w)$ and $P(y_i=0\vert x_i;w)$ as $p_{i\cdot 1}$ and $p_{i\cdot 0}$, respectively. Then logistic regression is to find $\hat w$ that maximizes</p>

<script type="math/tex; mode=display">\mathcal{J}(w)=P(Y|X;w)=\prod_{i=1}^N p_{i\cdot 1}^{y_i}p_{i\cdot 0}^{1-y_i}.</script>

<p>Given the dataset $\mathcal{D}$, such maximization problem can be solved by <em>maximum likelihood estimation</em>:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\hat w&=\arg\max_w \log P(Y\vert X;w)\\&=\arg\max_w \log\prod_{i=1}^N p_{i\cdot 1}^{y_i}p_{i\cdot 0}^{1-y_i}\\&=\arg\max_w \sum_{i=1}^N(y_i\log p_{i\cdot 1}+(1-y_i)\log p_{i\cdot 0})\\&=\arg\max_w \sum_{i=1}^N[y_i\log \sigma(w^Tx_i)+(1-y_i)\log (1-\sigma(w^Tx_i))]\end{aligned}. %]]></script>

<blockquote>
  <p>The term <script type="math/tex">\sum_{i=1}^N(y_i\log \sigma(w^Tx_i)+(1-y_i)\log (1-\sigma(w^Tx_i)))</script> is actually the negative of <em>cross entropy</em> over $P(Y)$ and $\sigma(w^TX)$.</p>
</blockquote>

<p>To solve the above problem, one can refer to SGD method.</p>

<h1 id="4-generative-classifiers">4. Generative Classifiers</h1>

<p>For a binary classification problem, we actually have no need to know the specfic value of <script type="math/tex">P(y=1\vert x)</script> and <script type="math/tex">P(y=0\vert x)</script>. What matters is whether <script type="math/tex">P(y=1\vert x)>P(y=0\vert x)</script> or not. Unlike discriminant methods which model and compute the posterior probability directly, in <em>generative classifiers</em>, we compare the posterior probability in an indirect way. Specifically, by <em>Bayesâ€™s theorem</em>, we have</p>

<script type="math/tex; mode=display">P(Y|X)=\frac{P(X|Y)P(Y)}{P(X)}\propto P(X|Y)P(Y).</script>

<p>Therefore, to compare the posterior probability is to compare the union probability. The classification predicted by generative classifiers is</p>

<script type="math/tex; mode=display">\hat y=\arg\max_{y\in\{0,1\}}P(y\vert x)=\arg\max_{y\in\{0,1\}}P(x\vert y)P(y).</script>

<p>In generative classifier methods, a key problem is how to model the likelihood <script type="math/tex">P(x\vert y)</script> and the prior $P(y)$.</p>

<h2 id="41-naive-bayes-classifier">4.1. Naive Bayes classifier</h2>

<p>Naive Bayes classifier is the simplest generative classifier. For a binary classification problem, suppose the feature of $x_i$ is composed of $(x_{i1},x_{i2},\dots,x_{id})$. Then naive Bayes classifier assumes not only the independence among the data but also that <em>every pair of the feature is independent</em>, <em>i.e.,</em></p>

<script type="math/tex; mode=display">x_{im}\vert y_i\perp x_{in}\vert y_i,m,n=1,2,\dots,d \text{ and }m\ne n.</script>

<p>Then the likelihood becomes</p>

<script type="math/tex; mode=display">P(x_i\vert y_i)=\prod_{j=1}^dP(x_{ij}\vert y_i).</script>

<p>Further, it models the prior and each feature as,</p>

<script type="math/tex; mode=display">y_i\sim\text{Bern}(\phi),\quad x_{ij}\vert y_i\sim\mathcal{N}(\mu_{j},\sigma_j^2),</script>

<p>where $\phi,\mu_j$, and $\sigma_{j}$ are parameters that can be learned by MLE method. Note that such model is just a common case. The key idea of naive Bayes classifier is its independence assumption. Specifically, naive Bayes classifier is not a single method but a family of methods. By assuming the independence, it can be extremely fast compared with other classification methods.</p>

<h2 id="42-gaussian-discriminant-analysis">4.2. Gaussian discriminant analysis</h2>

<p>As a generative method, <em>Gaussian discriminant analysis</em> (GDA) models the prior and the likelihood as follows,</p>

<script type="math/tex; mode=display">y\sim\text{Bern}(\phi),\quad x\vert y=0\sim\mathcal{N}(\mu_1,\Sigma),\quad x\vert y=1\sim\mathcal{N}(\mu_2,\Sigma),</script>

<p>where <script type="math/tex">\phi,\mu_1,\mu_2</script>, and <script type="math/tex">\Sigma</script> are parameters to be learned. We define $w=(\phi, \mu_1,\mu_2,\Sigma)$. Then GDA is to find $\hat w$ that maximizes</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\mathcal{J}(w)&=\log \prod_{i=1}^N P(x_i\vert y_i;\mu_1,\mu_2,\Sigma)P(y_i;\phi)\\&=\sum_{i=1}^N\left(\log P(x_i\vert y_i;\mu_1,\mu_2,\Sigma)+\log P(y_i;\phi)\right)\end{aligned}. %]]></script>

<p>Similar to the case in section 3.1, we represent the likelihood and the prior as</p>

<script type="math/tex; mode=display">P(x_i\vert y_i;\mu_1,\mu_2,\Sigma)=\rho^{y_i}(\mu_1,\Sigma)\rho^{1-y_i}(\mu_2,\Sigma),\quad P(y_i;\phi)=\phi^{y_i}(1-\phi)^{1-y_i},</script>

<p>where $\rho(\mu_1,\Sigma)$ and $\rho(\mu_2,\Sigma)$ are the PDF of <script type="math/tex">x\vert y=0</script> and <script type="math/tex">x\vert y=1</script>, respectively. Then it follows that</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\mathcal{J}(w)&=\sum_{i=1}^N\left(\log\rho^{y_i}(\mu_1,\Sigma)\rho^{1-y_i}(\mu_2,\Sigma)+\log\phi^{y_i}(1-\phi)^{1-y_i}\right)\\&=\sum_{i=1}^N\left(y_i\log \rho(\mu_1,\Sigma)+(1-y_i)\log\rho(\mu_2,\Sigma)+y_i\log\phi+(1-y_i)\log(1-\phi)\right)\end{aligned}. %]]></script>

<p>Then to find <script type="math/tex">\hat w</script> that maximizes $\mathcal{J}(w)$ is equivalent to set the derivation of $\mathcal{J}(w)$ w.r.t $w$ to be zero.</p>

<p>For $\hat \phi$:</p>

<script type="math/tex; mode=display">\begin{aligned}\frac{\partial \mathcal{J}(w)}{\partial\phi}=\sum_{i=1}^N\left(\frac{y_i}{\phi}-\frac{1-y_i}{1-\phi}\right)\end{aligned}.</script>

<p>Solving <script type="math/tex">\partial \mathcal{J}(w)/\partial\phi=0</script>, we have</p>

<script type="math/tex; mode=display">\hat \phi=\frac{1}{N}\sum_{i=1}^N y_i=\frac{N_1}{N}.</script>

<p>For $\hat\mu_1$ (or <script type="math/tex">\hat\mu_2</script> likewise):</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\frac{\partial\mathcal{J}(w)}{\partial\mu_1}&=\frac{\partial\left(\sum_{i=1}^N y_i\log \rho(\mu_1,\Sigma)\right)}{\partial\mu_1}\\&=\frac{\partial\left(\sum_{i=1}^Ny_i\log\left(\frac{1}{(2\pi)^{d/2}\vert\Sigma\vert^{1/2}}\exp\left(-\frac{1}{2}(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)\right)\right)\right)}{\partial\mu_1}\\&=\frac{\partial\left(-\frac{1}{2}\sum_{i=1}^Ny_i(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)\right)}{\partial\mu_1}\\&=\frac{\partial\left(-\frac{1}{2}\sum_{i=1}^Ny_i(x_i^T\Sigma^{-1}x_i-x_i^T\Sigma^{-1}\mu_1-\mu_1^T\Sigma^{-1}x_i+\mu_1^T\Sigma^{-1}\mu_1)\right)}{\partial\mu_1}\\&=\sum_{i=1}^Ny_i\left(\Sigma^{-1}\mu_1-\Sigma^{-1}x_i\right)\end{aligned}. %]]></script>

<blockquote>
  <p><a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">The Matrix Cookbook</a>:</p>

  <script type="math/tex; mode=display">\frac{\partial x^Ta}{\partial x}=\frac{\partial a^Tx}{\partial x}=a.</script>

  <script type="math/tex; mode=display">\frac{\partial x^TBx}{\partial x}=(B+B^T)x.</script>
</blockquote>

<p>Solving <script type="math/tex">\partial \mathcal{J}(w)/\partial\mu_1=0</script>, we have</p>

<script type="math/tex; mode=display">\hat \mu_1=\frac{\sum_{i=1}^N y_ix_i}{\sum_{i=1}^N y_i}=\bar x_{c1}.</script>

<p>Similarly, we have</p>

<script type="math/tex; mode=display">\hat\mu_2=\bar x_{c2}.</script>

<p>For $\hat\Sigma$, we first consider the following transformation</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\sum_{i=1}^N\left(y_i\log \rho(\mu_1,\Sigma)+(1-y_i)\log\rho(\mu_2,\Sigma)\right)&=\sum_{x\in X_{c1}}\log\rho(\mu_1,\Sigma)+\sum_{x\in X_{c2}}\log\rho(\mu_2,\Sigma)\end{aligned}. %]]></script>

<p>As $\Sigma$ is shared by both <script type="math/tex">\rho(\mu_1,\Sigma)</script> and <script type="math/tex">\rho(\mu_2,\Sigma)</script>, we consider the expansion of $\rho(\mu_1,\Sigma)$ for example,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\sum_{x\in X_{c1}}\log\rho(\mu_1,\Sigma)&=\sum_{x\in X_{c1}}\log\left(\frac{1}{(2\pi)^{d/2}\vert\Sigma\vert^{1/2}}\exp\left(-\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\right)\right)\\&=\underbrace{\sum_{x\in X_{c1}}-\frac{d}{2}\log2\pi}_{\text{constant }\lambda_1}-\sum_{x\in X_{c1}}\left(\frac{1}{2}\log\vert \Sigma\vert+\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\right)\\&=\lambda_1-\frac{N_1}{2}\log\vert\Sigma\vert-\frac{1}{2}\sum_{x\in X_{c1}}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\end{aligned}. %]]></script>

<p>For the third term, notice that $(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\in\mathbb{R}$, thus</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\sum_{x\in X_{c1}}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)&=\text{tr}\left(\sum_{x\in X_{c1}}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\right)\\&=\text{tr}\left(\sum_{x\in X_{c1}}(x-\mu_1)(x-\mu_1)^T\Sigma^{-1}\right)\\&=N_1\text{tr}\left(S_{c1}\Sigma^{-1}\right)\end{aligned}. %]]></script>

<blockquote>
  <p><a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">The Matrix Cookbook</a>:</p>

  <script type="math/tex; mode=display">\text{tr}(ABC)=\text{tr}(CAB)=\text{tr}(BCA).</script>

  <script type="math/tex; mode=display">\frac{\partial\text{tr}(AB)}{\partial A}=B^T.</script>

  <script type="math/tex; mode=display">\frac{\partial \vert A\vert}{\partial A}=\vert A\vert A^{-1}.</script>
</blockquote>

<p>Hence,</p>

<script type="math/tex; mode=display">\sum_{x\in X_{c1}}\log\rho(\mu_1,\Sigma)=\lambda_1-\frac{N_1}{2}\log\vert\Sigma\vert-\frac{N_1}{2}\text{tr}\left(S_{c1}\Sigma^{-1}\right).</script>

<p>Similarly,</p>

<script type="math/tex; mode=display">\sum_{x\in X_{c2}}\log\rho(\mu_2,\Sigma)=\lambda_2-\frac{N_2}{2}\log\vert\Sigma\vert-\frac{N_2}{2}\text{tr}\left(S_{c2}\Sigma^{-1}\right).</script>

<p>Then it follows that</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\frac{\partial\mathcal{J}(w)}{\partial\Sigma}&=\frac{\partial \left(\sum_{i=1}^N\left(y_i\log \rho+(1-y_i)\log\rho\right)\right)}{\partial\Sigma}\\&=\frac{\partial \left(\sum_{x\in X_{c1}}\log\rho+\sum_{x\in X_{c2}}\log\rho\right)}{\partial\Sigma}\\&=-\frac{1}{2}\frac{\partial \left(N\log\vert\Sigma\vert+N_1\text{tr}\left(S_{c1}\Sigma^{-1}\right)+N_2\text{tr}\left(S_{c2}\Sigma^{-1}\right)\right)}{\partial\Sigma}\\&=-\frac{1}{2}\left(N\Sigma^{-1}-N_1S_{c1}\Sigma^{-2}-N_2S_{c2}\Sigma^{-2}\right)\end{aligned}. %]]></script>

<p>By setting <script type="math/tex">\partial \mathcal{J}(w)/\partial\Sigma=0</script>, we finally arrive at</p>

<script type="math/tex; mode=display">\hat\Sigma=\frac{N_1S_{c1}+N_2S_{c2}}{N}.</script>

<h1 id="5-conclusion">5. Conclusion</h1>

<p>In this post, we introduced five linear classifiers. Among these models, $\mathcal{L}(w)$ is to be minimized, while $\mathcal{J}(w)$ is to be maximized. We omitted the prediction part of a classification problem. What we focused is actually how to model these data, especially in those generative cases.</p>

<p>This post is obviously a long story. Moreover, there are many other things stoped my writing occasionally these days. It definitely has some logical problems, let alone typos, to be fixed. Anyway, I made it. Hope next time I can do better.</p>

:ET