I"h!<p><em>The notes are based on the <a href="https://github.com/shuhuai007/Machine-Learning-Session">session</a>. For the fundamental of linear algebra, one can always refer to <a href="http://math.mit.edu/~gs/linearalgebra/">Introduction to Linear Algebra</a> and <a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">The Matrix Cookbook</a> for more details. Many thanks to these great works.</em></p>

<ul id="markdown-toc">
  <li><a href="#0-introduction" id="markdown-toc-0-introduction">0. Introduction</a></li>
  <li><a href="#1-kernel-method" id="markdown-toc-1-kernel-method">1. Kernel method</a></li>
  <li><a href="#2-kernel-function" id="markdown-toc-2-kernel-function">2. Kernel function</a></li>
  <li><a href="#3-conclusion" id="markdown-toc-3-conclusion">3. Conclusion</a></li>
</ul>
<h1 id="0-introduction">0. Introduction</h1>

<p>Kernel methods are a class of algorithms for pattern analysis. The name of kernel methods comes from the use of <em>kernel function</em>, which enable operations in a high-dimensional and implicit space. Specifically, by <a href="https://en.wikipedia.org/wiki/Cover%27s_theorem">Cover’s theorem</a>, given a set of training data that is not <em>linearly separable</em>, one can with high probability transform it into a training set that is linearly separable by projecting it into a higher-dimensional space via some <em>non-linear transformation</em>. With the help of kernel function, the operation, <em>i.e.,</em> inner product, it involves after transforming can be often computationally cheaper than the explicit computation. Such an approach is called the <em>kernel trick</em>. In this post, we will focus on the application of kernel method to SVM.</p>

<h1 id="1-kernel-method">1. Kernel method</h1>

<p>Define the data set as <script type="math/tex">\mathcal{D}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}, X=\{x_1,x_2,\dots,x_N\}</script> and <script type="math/tex">Y=\{y_1,y_2,\dots,y_N\}</script> where $x_i\in\mathbb{R}^{d\times 1}$ and <script type="math/tex">y_i\in\{-1,1\}</script>. We further assume that the data set is non-linearly separable. Kernel method supposes that there is a non-linear transformation $\phi(x):\mathbb{R}^{d\times 1}\to\mathbb{R}^{p\times 1},d&lt;p,$  such that <script type="math/tex">\mathcal{D}_p=\{(\phi(x_1),y_1),(\phi(x_2),y_2),\dots,(\phi(x_N),y_N)\}</script> are linearly separable. For such a linearly separable data set, recalling the problem we formulated in section 1.3 of <a href="https://2ez4ai.github.io/2020/10/28/support_vector_machine-ml05/">SVM</a>, we have the duality problem</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\min_\lambda&\quad \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^N\left(\lambda_i\lambda_jy_iy_j\phi^T(x_i)\phi(x_j)\right)-\sum_{i=1}^N\lambda_i\\\text{s.t.}&\quad \lambda_i\ge0,i=1,2,\dots,N\\&\quad \sum_{i=1}^N\lambda_iy_i=0.\end{aligned} %]]></script>

<p>However, after transforming, the inner product $\phi(x_i)^T\phi(x_j)=\langle\phi(x_i),\phi(x_j)\rangle$ could be hard to obtain (consider the case that $\phi(\cdot)$ has infinite dimensions), which requires the aid of <em>kernel function</em>.</p>

<h1 id="2-kernel-function">2. Kernel function</h1>

<p>A kernel function is defined as $K:\mathbb{R}^{d\times 1}\times\mathbb{R}^{d\times 1}\to\mathbb{R}$. Specifically, for non-linear transformation $\phi(\cdot)\in\mathcal{H}\text{ (Hilbert space) }:\mathbb{R}^{d\times 1}\to\mathbb{R}^{p\times 1}$ and any $x_i,x_j\in\mathbb{R}^{d\times 1}$, we call $K(x_i,x_j)=\langle\phi(x_i),\phi(x_j)\rangle$ a kernel function. Such a kernel function is regarded <em>positive definite</em>, which satisfies</p>

<script type="math/tex; mode=display">K(x_i,x_j)=K(x_j,x_i),</script>

<p>and for <script type="math/tex">x_{1},x_{2},\dots,x_{N}\in\mathbb{R}^{d\times 1},</script></p>

<script type="math/tex; mode=display">\mathcal{K}=[K(x_{i},x_{j})]_{N\times N}\text{ is a positive semi-definite (PSD) matrix},</script>

<p>where <script type="math/tex">\mathcal{K}</script> is called <em>Gram matrix</em> of $K$ over set <script type="math/tex">\{x_{1},x_{2},\dots,x_{N}\}</script>. When the explicit expression of $\phi(\cdot)$ is hard to be determined, it quite often to show the positive definiteness of a kernel function via its corresponding Gram matrix.</p>

<p><strong>(Properties)</strong> We now show two <em>properties</em> of kernel functions.</p>

<ul>
  <li>Let $K$ be kernel function such that <script type="math/tex">K:\mathbb{R}^{d\times 1}\times\mathbb{R}^{d\times 1}\to\mathbb{R}</script>, then we define its Gram matrix $\mathcal{K}\in\mathbb{R}^{N\times N}$ over <script type="math/tex">\{x_1,x_2,\dots,x_N\}</script> where $x_i\in\mathbb{R}^{d\times 1}$. Considering the mapping function $\phi(\cdot):\mathbb{R}^{d\times 1}\to\mathbb{R}^{p\times 1}$, we have</li>
</ul>

<script type="math/tex; mode=display">K(x_i,x_j)=\langle\phi(x_i),\phi(x_j)\rangle, \phi(\cdot)\in\mathcal{H}\implies \begin{cases}K(x_i,x_j)=K(x_j,x_i)\\ \mathcal{K}\text{ is a PSD matrix}\end{cases}.</script>

<p><em>Proof</em>:</p>

<p>By the definition of $K(x_i,x_j)$, we have</p>

<script type="math/tex; mode=display">K(x_i,x_j)=\langle \phi(x_i),\phi(x_j)\rangle,\quad K(x_j,x_i)=\langle \phi(x_j),\phi(x_i)\rangle.</script>

<p>By the symmetry of inner product, we have <script type="math/tex">\langle \phi(x_i),\phi(x_j)\rangle=\langle \phi(x_j),\phi(x_i)\rangle</script>. It then follows that</p>

<script type="math/tex; mode=display">K(x_i,x_j)=K(x_j,x_i).</script>

<p>Therefore the Gramian matrix $\mathcal{K}=[K(x_{i},x_{j})]_{N\times N}$ is symmetric real matrix. Now we show that <script type="math/tex">\forall\alpha\in\mathbb{R}^{R\times 1}, \alpha^T\mathcal{K}\alpha\ge 0.</script> The notation is given by</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\alpha^T\mathcal{K}\alpha=(\alpha_1,\alpha_2,\dots,\alpha_N)\begin{bmatrix}K_{11}&K_{12}&\dots&K_{1N}\\K_{21}&K_{22}&\dots&K_{2N}\\\vdots&\vdots&\ddots&\vdots\\K_{N1}&K_{N2}&\dots&K_{NN}\end{bmatrix}\begin{pmatrix}\alpha_1\\\alpha_2\\\vdots\\\alpha_N\end{pmatrix}\end{aligned}, %]]></script>

<p>where $K_{ij}=K(x_{ri},x_{rj})$. We then have</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\alpha^T\mathcal{K}\alpha&=\sum_{i=1}^R\sum_{j=1}^R \alpha_i\alpha_jK_{ij}\\&=\sum_{i=1}^R\sum_{j=1}^R \alpha_i\alpha_j\phi^T(x_{ri})\phi(x_{rj})\\&=\sum_{i=1}^R \alpha_i\phi^T(x_{ri})\sum_{j=1}^R\alpha_j\phi(x_{rj})\\&=\left(\sum_{i=1}^R \alpha_i\phi(x_{ri})\right)^T\left(\sum_{j=1}^R\alpha_j\phi(x_{rj})\right)\\&=\left\langle\left(\sum_{i=1}^R \alpha_i\phi(x_{ri})\right), \left(\sum_{j=1}^R \alpha_i\phi(x_{rj})\right)\right\rangle\\&=\left\vert\left\vert\sum_{i=1}^R \alpha_i\phi(x_{ri})\right\vert\right\vert^2,\end{aligned} %]]></script>

<p>therefore, $\alpha^T\mathcal{K}\alpha\ge 0$ and $\mathcal{K}$ is a PSD matrix.$\tag*{$\blacksquare$}$</p>

<ul>
  <li>Let $\mathcal{K}\in\mathbb{R}^{d\times d}$ be a symmetric PSD matrix, then for <script type="math/tex">\{x_1,x_2,\dots,x_N\}</script> where $x_i\in\mathbb{R}^{d\times 1}$, we have kernel function $K(x_i,x_j)=x_i^T\mathcal{K}x_j$.</li>
</ul>

<p><em>Proof</em>:</p>

<p>Consider the <em>diagonalisation</em> of $\mathcal{K}=Q^T\Lambda Q$ by an orthogonal matrix $Q$, where $\Lambda$ is a diagnoal matrix containing the non-negative eigenvalues of $\mathcal{K}$. Let $\sqrt{\Lambda}$ be the diagonal matrix with the square roots of the eigenvalues and set $A=\sqrt{\Lambda}Q$.  Then for <script type="math/tex">\{x_1,x_2,\dots,x_N\}</script> where $x_i\in\mathbb{R}^{d\times 1}$, we have</p>

<script type="math/tex; mode=display">x_i^T\mathcal{K}x_j=x_i^TQ^T\Lambda Qx_j=x_i^TA^TA x_j=\langle A x_i,Ax_j\rangle.</script>

<p>Therefore we have kernel function $K(x_i,x_j)=x_i^T\mathcal{K}x_j=\langle Ax_i,Ax_j\rangle$ with linear transformation $\phi(\cdot)=A\cdot. \tag*{$\blacksquare$}$</p>

<h1 id="3-conclusion">3. Conclusion</h1>

<p>In this post, we introduced <em>kernel method</em> for classification problem. Given <em>Cover’s theorem</em>, we can project non-linear data into high-dimensional space and obtain linearly separable data. To simplify the computation incurred by the duality problem, we can leverage <em>kernel function</em> to avoid the computing labor.</p>

<p>This is definitely not a good introduction to kernel methods. For more details of kernel method, I would recommend <a href="https://people.eecs.berkeley.edu/~jordan/kernels/0521813972c03_p47-84.pdf">Kernel methods: an overview</a>.</p>
:ET