I"¥;<p><em>The notes are based on the <a href="https://github.com/shuhuai007/Machine-Learning-Session">session</a>. For the fundamental of linear algebra, one can always refer to <a href="http://math.mit.edu/~gs/linearalgebra/">Introduction to Linear Algebra</a> and <a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">The Matrix Cookbook</a> for more details. Many thanks to these great works.</em></p>

<h1 id="0-introduction">0. Introduction</h1>

<p>Support vector machine (SVM) is a supervised learning method for classification and regression analysis. It is one of the most robust prediction method. Here we mainly consider its applications in classification. Specifically, for the data of $d$-dimensional, we want to know whether we can separate classes with a $(d-1)$-dimensional <em>hyperplane</em>. In particular, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class. According to whether the dataset is linearly separable or not, there are <em>hard-margin</em> SVM, <em>soft-margin</em> SVM and <em>kernel</em> SVM.</p>

<h1 id="1-hard-margin-svm">1. Hard-margin SVM</h1>

<p>Hard-margin SVM works only when data is completely linearly separable without any errors.</p>

<div align="center"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/300px-SVM_margin.png" width="350" />
</div>
<h2 id="11-problem-formulation">1.1. Problem formulation</h2>

<p>Suppose we have data set <script type="math/tex">\mathcal{D}=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}</script> where $x_i\in\mathbb{R}^{d\times 1}$ is the data feature and $y_i\in{-1,1}$ is the corresponding class label. A case where $d=2$ is shown in the figure above. The hyperplane that separates the data is defined as</p>

<script type="math/tex; mode=display">w^Tx-b=0,</script>

<p>where $w\in\mathbb{R}^{d\times 1}$ and $b\in\mathbb{R}$ are parameters to be learned. Then like what we arrived in <a href="https://2ez4ai.github.io/2020/10/14/linear_classification-ml03/">perceptron</a>, a correct classifier should ensure that</p>

<script type="math/tex; mode=display">y_i(w^Tx_i-b)>0,\forall i=1,2,\dots,N.</script>

<p>We further define the <em>margin</em> as the parallel lines that has the minimum distance from the data to the hyperplane. In <em>hard-margin</em> SVM, we need to find the <em>maximum-margin</em> hyperplane that maximizes the distance, which can be described by</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\max_{w,b}\min_{i}&\quad \frac{\vert w^Tx_i-b\vert}{\vert\vert w\vert\vert}\\\text{s.t.}&\quad y_i(w^Tx_i-b)>0,\forall i=1,2,\dots,N\end{aligned}. %]]></script>

<p>The problem further is equivalent to</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{alignat*}{3}\max_{w,b}\min_{i}&\quad \frac{y_i(w^Tx_i-b)}{\vert\vert w\vert\vert}\implies\max_{w,b}\frac{1}{\vert\vert w\vert\vert}\min_i&\quad y_i(w^Tx_i-b)\\\text{s.t.}&\quad y_i(w^Tx_i-b)>0,\forall i=1,2,\dots,N\end{alignat*}. %]]></script>

<p>For the constraint $y_i(w^Tx_i-b)&gt;0, \forall i=1,2,\dots,N$, there exists a positive parameter $r&gt;0$ such that</p>

<script type="math/tex; mode=display">\min_i y_i(w^Tx_i-b)=r.</script>

<p>As there are many $w,b$ available for the separation as long as they have the same directions. We add a new constraint that $r=1$. Then it follows that</p>

<script type="math/tex; mode=display">\min_i y_i(w^Tx_i-b)=y_i((w_{\text{old}}^T/r)x_i-b_{\text{old}}/r)=1.</script>

<p>The problem then is transformed into</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{alignat*}{3}&&\max_{w,b}\frac{1}{\vert\vert w\vert\vert}\min_i y_i(w^Tx_i-b)&\implies\min_{w,b}\quad \frac{1}{2}w^Tw\\&&\text{s.t.}\quad y_i(w^Tx_i-b)>0&\implies\text{s.t.}\quad y_i(w^Tx_i-b)\ge 1,i=1,2,\dots,N\end{alignat*}, %]]></script>

<p>which is a linearly constrained <em>quadratic optimization</em> (QP) problem.</p>

<h2 id="12-lagrange-duality">1.2. Lagrange duality</h2>

<p>The following content is about <a href="https://web.stanford.edu/~boyd/cvxbook/">convex optimization</a>. In section 1, we have the <em>primal problem</em></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\min_{w,b}&\quad \frac{1}{2}w^Tw\\\text{s.t.}&\quad y_i(w^Tx_i-b)\ge 1,i=1,2,\dots,N\end{aligned}. %]]></script>

<p>The <em>Lagrangian</em> for the problem is a function defined as</p>

<script type="math/tex; mode=display">L(w,b,\lambda)=\frac{1}{2}w^Tw+\sum_{i=1}^N\lambda_i\left(1-y_i\left(w^Tx_i-b\right)\right),</script>

<p>where $\lambda_i\ge 0$ is the <em>Lagrange multiplier</em> associated with the constraints. Consider the problem of $\max_\lambda L(w,b,\lambda)$,</p>

<script type="math/tex; mode=display">% <![CDATA[
\max_{\lambda\ge 0} L(w,b,\lambda)=\begin{cases}\frac{1}{2}w^Tw+\infty&\text{if }\exists i\in\{1,2,\dots,N\}\text{ s.t. }1-y_i(w^Tx_i-b)>0,\\\frac{1}{2}w^Tw+0&\text{otherwise.}\end{cases} %]]></script>

<p>The problem makes sense (non infinity) only when the original constraint is satisfied. In that case, the primal problem is equivalent to</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\min_{w,b}\max_\lambda&\quad L(w,b,\lambda)\\\text{s.t.}&\quad \lambda_i\ge 0,i=1,2,\dots,N\end{aligned}. %]]></script>

<p>Then we define the <em>Lagrange dual function</em> for the primal problem as</p>

<script type="math/tex; mode=display">g(\lambda)=\min_{w,b}L(w,b,\lambda).</script>

<blockquote>
  <p>Actually, the correct definition of the <em>Lagrange dual function</em> should be</p>

  <script type="math/tex; mode=display">g(\lambda)=\inf_{w,b}L(w,b,\lambda).</script>

  <p>Here we assume the minimum exists and the infimum is the minimum for understanding.</p>
</blockquote>

<p>The <em>Lagrange dual problem</em> of the original problem is then defined as</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\max_{\lambda}&\quad g(\lambda)\\\text{s.t.}&\quad \lambda_i\ge 0,i=1,2,\dots,N\end{aligned}. %]]></script>

<p>The dual problem is introduced for its convexity. Specifically, notice that the infimum (minimum in this case) of $g(\lambda)$ is unconstrained as opposed to the original constrained minimization problem. Further, $g(\lambda)$ is concave with respect to $\lambda$ regardless of the original problem.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Primal Problem</th>
      <th style="text-align: center">Lagrange Dual Problem</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><script type="math/tex">% <![CDATA[
\begin{aligned}\min_{w,b}\max_\lambda&\quad L(w,b,\lambda)\\\text{s.t.}&\quad \lambda_i\ge 0,i=1,2,\dots,N\end{aligned} %]]></script></td>
      <td style="text-align: center"><script type="math/tex">% <![CDATA[
\begin{aligned}\max_{\lambda}\min_{w,b}&\quad L(w,b,\lambda)\\\text{s.t.}&\quad \lambda_i\ge 0,i=1,2,\dots,N\end{aligned} %]]></script></td>
    </tr>
  </tbody>
</table>

<p>A natural problem is whether the two problems are equivalent. Obviously, the equivalence is obtained if and only if</p>

<script type="math/tex; mode=display">\min_{w,b}\max_{\lambda} L(w,b,\lambda)=\max_{\lambda}\min_{w,b} L(w,b,\lambda).</script>

<p>If the equation holds, we say the <em>strong duality</em> holds. It can also be shown that the <em>weak duality</em> always holds as</p>

<script type="math/tex; mode=display">\min_{w,b}\max_{\lambda} L(w,b,\lambda)\ge\max_{\lambda}\min_{w,b} L(w,b,\lambda).</script>

<p><em>Proof:</em></p>

<p>Obviously, we have</p>

<script type="math/tex; mode=display">\max_\lambda L(w,b,\lambda)\ge L(w,b,\lambda)\ge\min_{w,b}L(w,b,\lambda).</script>

<p>Define $F(w,b)=\max_\lambda L(w,b,\lambda)$ and $G(\lambda)=\min_{w,b}L(w,b,\lambda)$. According the above inequality, it follows that</p>

<script type="math/tex; mode=display">F(w,b)\ge G(\lambda)\implies\min_{w,b}F(w,b)\ge\max_\lambda G(\lambda).</script>

<p>Therefore we have</p>

<script type="math/tex; mode=display">\min_{w,b}\max_{\lambda} L(w,b,\lambda)\ge\max_{\lambda}\min_{w,b} L(w,b,\lambda).\tag*{$\blacksquare$}</script>

<p>Solving the dual problem in fact is used to find nontrivial lower bounds for difficult original problems. In our case, the strong duality holds for the linearly constrained QP problem. Thus to solve the primal problem is to solve the dual problem.</p>

<h2 id="13-karushkuhntucker-conditions">1.3. Karushâ€“Kuhnâ€“Tucker conditions</h2>

<p>For the primal problem and its dual problem, if the strong duality holds, then <em>Karushâ€“Kuhnâ€“Tucker (KKT) conditions</em> are satisfied as</p>

<ul>
  <li>
    <p>(a). Primal Feasibility:</p>

    <script type="math/tex; mode=display">y_i(w^Tx_i-b)\ge1,i=1,2,\dots,N</script>
  </li>
  <li>
    <p>(b). Dual Feasibility:</p>

    <script type="math/tex; mode=display">\lambda_i\ge 0,i=1,2,\dots,N</script>
  </li>
  <li>
    <p>(c). Complementary Slackness:</p>

    <script type="math/tex; mode=display">\hat\lambda_i\left(1-y_i\left(\hat{w}^Tx_i-\hat b\right)\right)=0,i=1,2,\dots,N</script>
  </li>
  <li>
    <p>(d). Zero gradient of Lagrangian with respect to $w,b$:</p>

    <script type="math/tex; mode=display">\frac{\partial L}{\partial b}=0,\quad \frac{\partial L}{\partial w}=0.</script>
  </li>
</ul>

<p>The conditions (a) and (b) are the original constraints. As for condition (c), recall that we define $y_i(w^Tx_i-b)=1$ for the data that is exactly <script type="math/tex">1/\vert\vert w\vert\vert</script> away from the hyperplane $w^Tx-b=0$, <em>i.e.,</em> on the margin of the hyperplane. For those which are not on the margin, to satisfy KKT conditions, it must follow that</p>

<script type="math/tex; mode=display">\hat\lambda_k=0,k\in\{i\vert y_i(w^Tx_i-b)>1,i=1,2,\dots,N\}.</script>

<p>The condition (d) is for the dual problem. Specifically, we consider the unconstrained problem $\min_{w,b}L(w,b,\lambda)$ in the dual problem. For the differentiable function $L(w,b,\lambda)$ , by <em>Fermatâ€™s theorem</em>, the extremum exists when condition (d) is satisfied, $i.e.,$</p>

<script type="math/tex; mode=display">\frac{\partial L}{\partial b}=\sum_{i=1}^N\lambda_iy_i=0,\\\frac{\partial L}{\partial w}=w-\sum_{i=1}^N\lambda_iy_ix_i=0.</script>

<p>Solving the equations we have</p>

<script type="math/tex; mode=display">\sum_{i=1}^N\lambda_iy_i=0,\forall b,\quad \hat w=\sum_{i=1}^N\lambda_iy_ix_i.</script>

<p>Plugging them into $L(w,b,\lambda)$, we can transform the problem into</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\max_{\lambda\ge0}&\quad \min_{w,b}L(w,b,\lambda)\\\implies\max_{\lambda\ge0}&\quad \frac{1}{2}\sum_{i=1}^{N}\left(\lambda_iy_ix_i^T\right)\sum_{i=1}^{N}\left(\lambda_iy_ix_i\right)+\sum_{i=1}^N\lambda_i-\sum_{i=1}^N\lambda_iy_i\left(\sum_{j=1}^N\lambda_jy_jx_j^T\right)x_i\\\text{s.t.}&\quad \sum_{i=1}^N\lambda_iy_i=0\\\implies\max_{\lambda\ge0}&\quad -\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^N\left(\lambda_i\lambda_jy_iy_jx_i^Tx_j\right)+\sum_{i=1}^N\lambda_i\\\text{s.t.}&\quad \sum_{i=1}^N\lambda_iy_i=0\\\implies \min_\lambda&\quad \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^N\left(\lambda_i\lambda_jy_iy_jx_i^Tx_j\right)-\sum_{i=1}^N\lambda_i\\\text{s.t.}&\quad \lambda_i\ge0,i=1,2,\dots,N\\&\quad \sum_{i=1}^N\lambda_iy_i=0\end{aligned} %]]></script>

<p>The optimal $\hat\lambda$ can be obtained by <em>sequential minimal optimization</em> (SMO) algorithm. Here we assume we already have the optimal value. Notice we have $\sum_{i=1}^N\hat\lambda_iy_i=0$, which means there exists at least one $\hat\lambda_k\ne0$ otherwise $\hat w=0$. According our analysis, $\hat\lambda_k\ne 0$ only when</p>

<script type="math/tex; mode=display">y_k(w^Tx_k-b)-1=0.</script>

<p>Therefore we have the solution</p>

<script type="math/tex; mode=display">\hat w=\sum_{i=1}^N\hat\lambda_iy_ix_i,</script>

<script type="math/tex; mode=display">\hat b=\sum_{i=1}^N\hat\lambda_iy_ix_i^Tx_k-y_k.</script>

<p>Accordingly, the hyperplane is the linear combination of the data on the margin with corresponding $\hat\lambda_k&gt;0$. We call those data <em>support vectors</em> from where the name SVM comes.</p>

<h2 id="2-soft-margin-svm">2. Soft-margin SVM</h2>

<p>In practice, there are noise and outliers among the data, which makes the data nonlinearly separable. In that case, hard-margin fails to work. Now we introduce <em>soft-margin SVM</em> which extends SVM to the nonlinearly separable data. Recall that in hard-margin SVM, we have the constraint</p>

<script type="math/tex; mode=display">y_i(w^Tx_i-b)\ge 1,i=1,2,\dots,N</script>

<p>which confines the model to the linearly separable case. To extent the model to general cases, we introduce <em>loss function</em>, which can be defined as</p>

<ul>
  <li>
    <p>The number of wrongly classifying:</p>

    <script type="math/tex; mode=display">% <![CDATA[
\text{loss}=\sum_{i=1}^N I(y_i(w^Tx_i-b)<1), %]]></script>

    <p>where $I(\cdot)$ is the indicator function. However, such loss function is not differentiable.</p>
  </li>
  <li>
    <p>The sum of the distances between the hyperplane and the outliers:</p>

    <script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\text{loss}_i&=\begin{cases}0&y_i(w^Tx_i-b)\ge 1\\1-y_i(w^Tx_i-b)&y_i(w^Tx_i-b)<1\text{ (wrongly classified)}\end{cases}\\&=\max\{0, 1-y_i(w^Tx_i-b)\}.\\\text{loss}&=\sum_{i=1}^N\text{loss}_i,\end{aligned} %]]></script>

    <p>which is called <strong>hinge loss</strong>.</p>
  </li>
</ul>

<p>However, the $\max$ in the hinge loss is not differentiable neither. We now adapt the original constraint as</p>

<script type="math/tex; mode=display">y_i(w^Tx_i-b)\ge 1-\xi_i,i=1,2,\dots,N</script>

<p>where $\xi_i\ge0$ and $\sum_{i=1}^N\xi_i\le$ constant are called <em>slack variables</em>. The slack variables is introduced to allow for some points to be on the wrong side of the margin. Specifically, for the points that are on the wrong side, it will break the original constraint $y_i(w^Tx_i-b)\ge 1$ as</p>

<script type="math/tex; mode=display">% <![CDATA[
y_i(w^Tx_i-b)=\xi< 1. %]]></script>

<p>With slack variables, such classification is allowed as long as</p>

<script type="math/tex; mode=display">\xi\ge 1-\xi_i\implies\xi_i\ge1-\xi.</script>

<p>Moreover, we do not want the $\xi_i$ to be too large to distinguish points correctly. Thus we have the new formulation</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}\min_{w,b}&\quad \frac{1}{2}w^Tw+C\sum_{i=1}^N\xi_i\\\text{s.t.}&\quad y_i(w^Tx_i-b)\ge 1-\xi_i\\&\quad \xi_i\ge 0\\&\quad i=1,2,\dots,N\end{aligned}, %]]></script>

<p>where $C$ is the <em>cost</em> parameter that determines to what extent we allow for outliers. To solve the problem one can refer to the hard-margin case as they are actually similar.</p>

<h1 id="4-conclusion">4. Conclusion</h1>

<p>In this post, we first introduced hard-margin SVM for linearly separable data. By introducing a loss function and slack variables, soft-margin SVM allows for noise and outliers so that it can handle non linear case. The two models can both be solved by <em>convex optimization</em> methods. For convex optimization, we briefly reviewed <em>Lagrange duality</em>, <em>Slaterâ€™s condition</em> and <em>KKT conditions</em>.</p>
:ET